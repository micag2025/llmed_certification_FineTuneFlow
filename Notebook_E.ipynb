{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92694493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Notebook_E ‚Äî Evaluation Dashboard (3 Model Comparison)\n",
    "# Models:\n",
    "#   - Baseline BART (no training)\n",
    "#   - BART + LoRA fine-tuned\n",
    "#   - BART merged (LoRA merged into base)\n",
    "#\n",
    "# Computes:\n",
    "#   ROUGE-1 / ROUGE-2 / ROUGE-L\n",
    "#   BLEU\n",
    "#   BERTScore-F1\n",
    "#\n",
    "# Saves:\n",
    "#   per-model CSVs\n",
    "#   summary comparison table\n",
    "#   charts for each metric\n",
    "# =====================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Paths (EDIT THESE IF NEEDED)\n",
    "# -----------------------------------------------------\n",
    "BASELINE_CSV = \"/content/llmed_certification_FineTuneFlow/metrics/baseline_predictions.csv\"\n",
    "LORA_CSV     = \"/content/llmed_certification_FineTuneFlow/metrics/validation_predictions.csv\"\n",
    "MERGED_CSV   = \"/content/llmed_certification_FineTuneFlow/metrics/validation_predictions_merged.csv\"\n",
    "\n",
    "OUTPUT_DIR = \"/content/llmed_certification_FineTuneFlow/metrics/dashboard_outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Saving results to: {OUTPUT_DIR}\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Load prediction files\n",
    "# -----------------------------------------------------\n",
    "print(\"\\nüì• Loading prediction CSV files...\")\n",
    "\n",
    "df_base  = pd.read_csv(BASELINE_CSV)\n",
    "df_lora  = pd.read_csv(LORA_CSV)\n",
    "df_merge = pd.read_csv(MERGED_CSV)\n",
    "\n",
    "print(f\"Baseline: {len(df_base)} samples\")\n",
    "print(f\"LoRA:     {len(df_lora)} samples\")\n",
    "print(f\"Merged:   {len(df_merge)} samples\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Metric computation tools\n",
    "# -----------------------------------------------------\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(\n",
    "    [\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "def compute_metrics(df):\n",
    "    \"\"\"Compute ROUGE, BLEU, BERTScore and return df with metrics + summary dict.\"\"\"\n",
    "    # Ensure summary columns are strings and handle potential NaNs\n",
    "    df[\"model_summary\"] = df[\"model_summary\"].astype(str).fillna(\"\")\n",
    "    df[\"human_summary\"] = df[\"human_summary\"].astype(str).fillna(\"\")\n",
    "\n",
    "    rouge1_list, rouge2_list, rougeL_list = [], [], []\n",
    "\n",
    "    print(\"üîç Computing ROUGE...\")\n",
    "\n",
    "    for pred, ref in zip(df[\"model_summary\"], df[\"human_summary\"]):\n",
    "        scores = scorer.score(ref, pred)\n",
    "        rouge1_list.append(scores[\"rouge1\"].fmeasure)\n",
    "        rouge2_list.append(scores[\"rouge2\"].fmeasure)\n",
    "        rougeL_list.append(scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "    df[\"rouge1\"] = rouge1_list\n",
    "    df[\"rouge2\"] = rouge2_list\n",
    "    df[\"rougeL\"] = rougeL_list\n",
    "\n",
    "    print(\"üîç Computing BERTScore...\")\n",
    "    _, _, F1 = bert_score(\n",
    "        df[\"model_summary\"].tolist(),\n",
    "        df[\"human_summary\"].tolist(),\n",
    "        lang=\"en\",\n",
    "        rescale_with_baseline=True\n",
    "    )\n",
    "    df[\"bert_f1\"] = F1.numpy()\n",
    "\n",
    "    print(\"üîç Computing BLEU...\")\n",
    "    references = [[ref.split()] for ref in df[\"human_summary\"]]\n",
    "    candidates = [pred.split() for pred in df[\"model_summary\"]]\n",
    "    bleu = corpus_bleu(references, candidates)\n",
    "\n",
    "    summary = {\n",
    "        \"rouge1\": df[\"rouge1\"].mean(),\n",
    "        \"rouge2\": df[\"rouge2\"].mean(),\n",
    "        \"rougeL\": df[\"rougeL\"].mean(),\n",
    "        \"bert_f1\": df[\"bert_f1\"].mean(),\n",
    "        \"bleu\": bleu\n",
    "    }\n",
    "\n",
    "    return df, summary\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Compute Metrics for All Models\n",
    "# -----------------------------------------------------\n",
    "print(\"\\n=== üßÆ Baseline BART ===\")\n",
    "df_base, s_base = compute_metrics(df_base)\n",
    "\n",
    "print(\"\\n=== üßÆ BART + LoRA ===\")\n",
    "df_lora, s_lora = compute_metrics(df_lora)\n",
    "\n",
    "print(\"\\n=== üßÆ BART Merged ===\")\n",
    "df_merge, s_merge = compute_metrics(df_merge)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Build Summary Table\n",
    "# -----------------------------------------------------\n",
    "s_base[\"model\"] = \"Baseline-BART\"\n",
    "s_lora[\"model\"] = \"LoRA\"\n",
    "s_merge[\"model\"] = \"Merged\"\n",
    "\n",
    "summary_df = pd.DataFrame([s_base, s_lora, s_merge])\n",
    "\n",
    "print(\"\\nüìä Summary Comparison:\")\n",
    "print(summary_df)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Save Detailed Metrics\n",
    "# -----------------------------------------------------\n",
    "df_base.to_csv(f\"{OUTPUT_DIR}/baseline_metrics.csv\", index=False)\n",
    "df_lora.to_csv(f\"{OUTPUT_DIR}/lora_metrics.csv\", index=False)\n",
    "df_merge.to_csv(f\"{OUTPUT_DIR}/merged_metrics.csv\", index=False)\n",
    "summary_df.to_csv(f\"{OUTPUT_DIR}/summary_metrics.csv\", index=False)\n",
    "\n",
    "print(\"\\nüíæ Saved all CSV outputs.\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Visualization ‚Äî Matplotlib ONLY\n",
    "# -----------------------------------------------------\n",
    "metrics = [\"rouge1\", \"rouge2\", \"rougeL\", \"bert_f1\", \"bleu\"]\n",
    "\n",
    "print(\"\\nüìà Creating charts...\")\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(summary_df[\"model\"], summary_df[metric])\n",
    "    plt.title(f\"Comparison of {metric.upper()}\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    chart_path = f\"{OUTPUT_DIR}/{metric}_comparison.png\"\n",
    "    plt.savefig(chart_path)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Chart saved: {chart_path}\")\n",
    "\n",
    "print(\"\\nüéâ Notebook_E complete!\")\n",
    "print(f\"All evaluation outputs stored in:\\n{OUTPUT_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
