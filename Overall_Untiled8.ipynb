{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "zeRW-gFYc3kC",
   "metadata": {
    "id": "zeRW-gFYc3kC"
   },
   "source": [
    "# ðŸ“š Comprehensive Fine-Tuning Pipeline: BART-LoRA for HighlightSum\n",
    "\n",
    "## Overview\n",
    "This notebook implements a complete **end-to-end fine-tuning pipeline** for abstractive summarization using **BART-Large** with **LoRA adapters**. It includes:\n",
    "- âœ… Dataset loading & inspection\n",
    "- âœ… Multi-model benchmarking\n",
    "- âœ… Fine-tuning with PEFT\n",
    "- âœ… Comprehensive evaluation (ROUGE, BERTScore, BLEU)\n",
    "- âœ… Error analysis & visualization\n",
    "- âœ… Model merging & inference\n",
    "\n",
    "**Target Task**: Highlight-based abstractive summarization on HighlightSum dataset\n",
    "**Hardware**: GPU (T4, L4, or better recommended)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fca538",
   "metadata": {
    "id": "02fca538"
   },
   "source": [
    "# Dataset Inspection: HighlightSum\n",
    "\n",
    "This section loads and explores the **HighlightSum** dataset to understand its structure and contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ub3rxVBOdA3x",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13383f34163f42c186829e9d74c2ca32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94934e56cc6d42b5b62a7e0361bb11b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv:   0%|          | 0.00/27.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79201b0d3738418e8420f640788d49af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203a9e6b35a34e798646aff61733e534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d08f1fd5d2e44cc8e05759b702fbac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/27401 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2861240673487589247e17c451349f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1360 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35388f2aca744109ab2e0a23b3368f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2347 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Dataset Overview:\n",
      "  Train splits: 27,401 samples\n",
      "  Val splits: 1,360 samples\n",
      "  Test splits: 2,347 samples\n",
      "\n",
      "ðŸ”‘ Keys: ['id', 'dialogue', 'summary']\n",
      "\n",
      "ðŸ“˜ First training example:\n",
      "\n",
      "ðŸ”¸ DIALOGUE (32390 chars):\n",
      "Speaker A: Cool. Do you wanna give me the little cable thing? Yeah. Cool. Ah, that's why it won't meet. Okay, cool. Yep, cool. Okay, functional requirements. Alright, yeah. It's working. Cool, okay. So what I have, wh where I've got my information from is a survey where the usability lab um observed...\n",
      "\n",
      "ðŸ”¹ SUMMARY (1299 chars):\n",
      "The project manager opens the meeting by stating that they will address functional design and then going over the agenda. The industrial designer gives his presentation, explaining how remote controls function and giving personal preference to a clear, simple design that upgrades the technology as well as incorporates the latest features in chip design. The interface specialist gives her presentation next, addressing the main purpose of a remote control. She pinpoints the main functions of on/off, channel-switching, numbers for choosing particular channels, and volume; and also suggests adding a menu button to change settings such as brightness on the screen. She gives preference to a remote that is small, easy to use, and follows some conventions. The group briefly discusses the possibility of using an LCD screen if cost allows it, since it is fancy and fashionable. The marketing expert presents, giving statistical information from a survey of 100 subjects. She prefers a remote that is sleek, stylish, sophisticated, cool, beautiful, functional, solar-powered, has long battery life, and has a locator. They discuss the target group, deciding it should be 15-35 year olds. After they talk about features they might include, the project manager closes the meeting by allocating tasks.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"knkarthick/highlightsum\")\n",
    "\n",
    "# View first sample structure\n",
    "sample = dataset[\"train\"][0]\n",
    "print(\"ðŸ“Š Dataset Overview:\")\n",
    "print(f\"  Train splits: {len(dataset['train']):,} samples\")\n",
    "print(f\"  Val splits: {len(dataset['validation']):,} samples\")\n",
    "print(f\"  Test splits: {len(dataset['test']):,} samples\")\n",
    "print(f\"\\nðŸ”‘ Keys: {list(sample.keys())}\")\n",
    "\n",
    "# Inspect sample details\n",
    "print(\"\\nðŸ“˜ First training example:\")\n",
    "print(f\"\\nðŸ”¸ DIALOGUE ({len(sample['dialogue'])} chars):\\n{sample['dialogue'][:300]}...\")\n",
    "print(f\"\\nðŸ”¹ SUMMARY ({len(sample['summary'])} chars):\\n{sample['summary']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9840e9d2",
   "metadata": {
    "id": "9840e9d2"
   },
   "source": [
    "# Large Model Safety Benchmarking (with Preprocessing)\n",
    "\n",
    "This section outlines the preprocessing steps required before running benchmarking or fineâ€‘tuning large models. Since only **2,000 samples** will be used for fineâ€‘tuning, efficient batching and GPU-aware preprocessing are essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4f29c8",
   "metadata": {
    "id": "9b4f29c8"
   },
   "source": [
    "### Why Preprocessing Matters\n",
    "\n",
    "Preprocessing ensures that:\n",
    "- Inputs are normalized and consistently formatted.\n",
    "- Tokenization is performed efficiently before training.\n",
    "- Dataset splits are clearly defined.\n",
    "\n",
    "This improves both reproducibility and training performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb26e3be",
   "metadata": {
    "id": "fb26e3be"
   },
   "source": [
    "_Basic preprocessing_ has been performed, including tokenization of dialogues with appropriate padding and truncation, batch preparation for seq2seq models, and selection of a subset from the HighlightSUM train split for benchmarking. In details, the performed basic preprocessing is charachterised by:  \n",
    "> _Tokenization_:\n",
    "  - All text inputs (dialogue) are tokenized using the model-specific tokenizer.  \n",
    "  - For causal models, if pad_token was missing, it was set to eos_token to allow batching/padding.  \n",
    "  - Seq2seq and causal models both use truncation and padding (max_length=1024) to ensure consistent tensor shapes.  \n",
    "> Dataset splitting:  \n",
    "  - Selected a subset of samples (N_SAMPLES) from the HighlightSUM train split for benchmarking.\n",
    "  - If desired, you could extend this to full train/validation/test splits for proper evaluation.\n",
    "> Batching (seq2seq models):  \n",
    "  - Inputs are batched to reduce memory usage on GPU, which is part of preprocessing before model inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clbC2K3cjNdd",
   "metadata": {
    "id": "clbC2K3cjNdd"
   },
   "source": [
    "improved version Notebook C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eB8UU1ZMfURS",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "áŠ‚ Using device: cuda\n",
      "Train samples: 27401\n",
      "Validation samples: 1360\n",
      "Test samples (used): 200\n",
      "Columns: ['id', 'dialogue', 'summary']\n",
      "\n",
      "áŠ‚ Running Model Benchmarks...\n",
      "\n",
      "á½¨0 Benchmarking BART-large (facebook/bart-large-cnn) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2060890673804807aacfceb5d1ff50d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4abbcc789954fcf978084234184de62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ecca9ed262f46c490362620ad978055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0261f457cbad435499e842d738b935f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3a76761ea94921b95cb15284bb89af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497aa3311ea14aa880cdd8d78769c07e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "BART-large (seq2seq): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:42<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "á½¨0 Benchmarking T5-large (t5-large) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9eaa6e2f54849de91d09e7304f0d81b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247bd28af4924f029f147883b02983f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7bab1b471064d54ba99b2fcfcf9cdb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b217aedddbc94115aee0658e1296a82b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d712ae8b675f42688917b573bf46594c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T5-large (seq2seq): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [04:24<00:00,  5.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "á½¨0 Benchmarking Phi-3-Mini (microsoft/Phi-3-mini-4k-instruct) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0250632d424969af64891d260662eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2908deb6a9b4ece999471677c7a031f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2e94d91fee4df08f9452f2ec0ca32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea954a9248124bedbcdcda8156a2e3d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be030e5b7a34b8c8310670dbcd07b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f552eceb6cb349e8804dce09b01931be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6f0a61870b49f7a1e1856e5ffa420d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e41b58810bb04ab08bd16383d891032b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5039c96ff5f4815af4c1fd1cbed8107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5998f8577447c69b5af9b57631ab8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8045aacced66486882fed6cd716b771b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508e35cf2bc3496facafc5b02258e125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phi-3-Mini (causal): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [16:28<00:00,  9.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "á½¨0 Benchmarking LLaMA-1B (meta-llama/Llama-3.2-1B-Instruct) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f5c4d52c6d4852930bd8048d5ced0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef09b97dfd26497d80995ec2776a47b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c532b327db74771bb28fb58a401d647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf259f2a9364fdb91471301c25e8dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd4241736b24cc1903e266d50894307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846eaca69a5f47a391d28079f6efee3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLaMA-1B (causal):  14%|â–ˆâ–        | 14/100 [01:19<08:10,  5.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  15%|â–ˆâ–Œ        | 15/100 [01:21<06:37,  4.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  16%|â–ˆâ–Œ        | 16/100 [01:23<05:13,  3.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  17%|â–ˆâ–‹        | 17/100 [01:26<04:52,  3.52s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  18%|â–ˆâ–Š        | 18/100 [01:27<04:04,  2.98s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  19%|â–ˆâ–‰        | 19/100 [01:33<05:15,  3.89s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  20%|â–ˆâ–ˆ        | 20/100 [01:36<04:39,  3.49s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  21%|â–ˆâ–ˆ        | 21/100 [01:42<05:26,  4.14s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  22%|â–ˆâ–ˆâ–       | 22/100 [01:44<04:49,  3.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  23%|â–ˆâ–ˆâ–Ž       | 23/100 [01:47<04:28,  3.49s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  24%|â–ˆâ–ˆâ–       | 24/100 [01:51<04:22,  3.46s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  25%|â–ˆâ–ˆâ–Œ       | 25/100 [01:54<04:18,  3.45s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  26%|â–ˆâ–ˆâ–Œ       | 26/100 [02:04<06:32,  5.30s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  27%|â–ˆâ–ˆâ–‹       | 27/100 [02:09<06:27,  5.31s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  28%|â–ˆâ–ˆâ–Š       | 28/100 [02:18<07:40,  6.39s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  29%|â–ˆâ–ˆâ–‰       | 29/100 [02:21<06:14,  5.27s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [02:26<06:20,  5.43s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [02:29<05:16,  4.59s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [02:31<04:12,  3.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [02:33<03:44,  3.34s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [02:36<03:30,  3.19s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [02:41<04:12,  3.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [02:44<03:41,  3.46s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [02:50<04:26,  4.23s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [02:53<04:06,  3.98s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [02:55<03:13,  3.17s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [02:57<02:57,  2.95s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [02:59<02:35,  2.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [03:05<03:22,  3.49s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [03:17<05:59,  6.30s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [03:19<04:38,  4.98s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [03:28<05:40,  6.20s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [03:30<04:26,  4.93s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [03:33<03:40,  4.17s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [03:34<02:55,  3.37s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [03:36<02:28,  2.92s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [03:42<03:06,  3.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [03:44<02:47,  3.42s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [03:51<03:24,  4.26s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [03:56<03:40,  4.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [03:58<02:55,  3.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [04:02<02:55,  3.90s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [04:08<03:14,  4.42s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [04:14<03:32,  4.94s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [04:16<02:47,  3.99s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [04:18<02:26,  3.57s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [04:21<02:09,  3.23s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [04:27<02:39,  4.08s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [04:29<02:12,  3.49s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [04:31<01:55,  3.12s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [04:33<01:33,  2.59s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [04:39<02:06,  3.61s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [04:44<02:20,  4.13s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [04:45<01:45,  3.20s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [04:51<02:07,  4.00s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [04:54<01:57,  3.80s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [04:56<01:35,  3.18s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [04:59<01:33,  3.22s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [05:03<01:37,  3.49s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [05:09<01:50,  4.08s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [05:11<01:29,  3.44s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [05:14<01:24,  3.37s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [05:18<01:23,  3.50s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [05:21<01:21,  3.52s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [05:24<01:10,  3.22s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [05:30<01:25,  4.05s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [05:33<01:18,  3.90s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [05:36<01:05,  3.43s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [05:42<01:16,  4.24s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [05:43<00:58,  3.46s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [05:49<01:05,  4.09s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [05:51<00:54,  3.60s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [05:54<00:45,  3.25s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [05:55<00:34,  2.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [05:57<00:30,  2.52s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [06:01<00:31,  2.89s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [06:03<00:26,  2.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [06:05<00:22,  2.50s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [06:07<00:18,  2.32s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [06:13<00:23,  3.31s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [06:19<00:24,  4.07s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [06:24<00:22,  4.46s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [06:25<00:13,  3.50s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [06:28<00:09,  3.29s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [06:30<00:05,  2.83s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [06:32<00:02,  2.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [06:35<00:00,  3.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "á½¨0 Benchmarking LLaMA-3B (meta-llama/Llama-3.2-3B-Instruct) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6793663faa484db1c3f514e55f05a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "791b96bed34d4ad6905995cc0b524a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20eb73a30ad94149861b2d7b963ad404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0cc31bdb8149cb8f981a0266880bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702fe6212c7a40ad87d7b34308803cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6402964cd32a4ae1a38abf4a4a1e1aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16dc83d8c1af41f3bd65431749c4dac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4857e75b049e452daeaedd0c1d199f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa0ba88f0604fd7b84da422bb62ab72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b014d28d803d41919c31abc19012ac77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLaMA-3B (causal):  14%|â–ˆâ–        | 14/100 [02:42<16:47, 11.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  15%|â–ˆâ–Œ        | 15/100 [02:52<15:37, 11.03s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  16%|â–ˆâ–Œ        | 16/100 [02:55<12:13,  8.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  17%|â–ˆâ–‹        | 17/100 [03:03<11:26,  8.28s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  18%|â–ˆâ–Š        | 18/100 [03:07<09:33,  6.99s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  19%|â–ˆâ–‰        | 19/100 [03:11<08:30,  6.31s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  20%|â–ˆâ–ˆ        | 20/100 [03:14<07:06,  5.33s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  21%|â–ˆâ–ˆ        | 21/100 [03:24<08:49,  6.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  22%|â–ˆâ–ˆâ–       | 22/100 [03:32<09:01,  6.94s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  23%|â–ˆâ–ˆâ–Ž       | 23/100 [03:35<07:32,  5.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  24%|â–ˆâ–ˆâ–       | 24/100 [03:40<07:05,  5.59s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  25%|â–ˆâ–ˆâ–Œ       | 25/100 [03:50<08:40,  6.94s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  26%|â–ˆâ–ˆâ–Œ       | 26/100 [03:59<09:20,  7.58s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  27%|â–ˆâ–ˆâ–‹       | 27/100 [04:08<09:51,  8.10s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  28%|â–ˆâ–ˆâ–Š       | 28/100 [04:18<10:07,  8.44s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  29%|â–ˆâ–ˆâ–‰       | 29/100 [04:21<08:19,  7.03s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [04:31<08:59,  7.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [04:40<09:20,  8.13s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [04:49<09:30,  8.39s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [04:53<07:48,  7.00s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [04:56<06:31,  5.93s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [05:05<07:32,  6.96s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [05:15<08:10,  7.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [05:17<06:24,  6.10s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [05:22<05:49,  5.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [05:26<05:17,  5.21s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [05:30<04:48,  4.80s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [05:39<06:03,  6.16s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [05:48<06:41,  6.91s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [05:57<07:12,  7.58s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [05:59<05:30,  5.91s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [06:08<06:18,  6.89s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [06:13<05:32,  6.16s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [06:15<04:32,  5.13s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [06:24<05:20,  6.16s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [06:32<05:39,  6.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [06:34<04:24,  5.29s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [06:43<05:12,  6.37s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [06:52<05:48,  7.27s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [07:01<06:04,  7.76s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [07:05<04:59,  6.51s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [07:14<05:27,  7.29s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [07:18<04:40,  6.38s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [07:21<03:49,  5.33s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [07:30<04:35,  6.55s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [07:40<05:03,  7.41s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [07:48<05:12,  7.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [07:58<05:24,  8.31s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [08:00<04:05,  6.46s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [08:09<04:32,  7.35s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [08:14<03:52,  6.47s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [08:23<04:17,  7.35s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [08:32<04:28,  7.91s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [08:42<04:34,  8.32s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [08:51<04:31,  8.48s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [09:00<04:32,  8.78s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [09:10<04:29,  8.98s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [09:14<03:42,  7.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [09:19<03:12,  6.86s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [09:23<02:38,  5.86s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [09:27<02:22,  5.47s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [09:30<01:57,  4.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [09:39<02:22,  5.92s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [09:48<02:38,  6.90s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [09:57<02:48,  7.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [10:07<02:50,  8.12s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [10:15<02:46,  8.32s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [10:19<02:13,  7.02s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [10:26<02:06,  7.01s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [10:31<01:46,  6.27s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [10:40<01:53,  7.12s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [10:49<01:56,  7.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [10:59<01:55,  8.23s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [11:08<01:50,  8.50s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [11:11<01:22,  6.90s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [11:13<00:58,  5.36s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [11:22<01:05,  6.60s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [11:32<01:07,  7.45s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [11:35<00:50,  6.37s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [11:45<00:51,  7.35s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [11:54<00:46,  7.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [11:57<00:32,  6.52s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [12:06<00:28,  7.25s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [12:11<00:19,  6.57s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [12:15<00:11,  5.52s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [12:24<00:06,  6.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [12:29<00:00,  7.50s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df_sorted\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"model_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"meta-llama/Llama-3.2-1B-Instruct\",\n          \"t5-large\",\n          \"meta-llama/Llama-3.2-3B-Instruct\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rouge1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.197540545985217,\n        \"min\": 10.977281761408955,\n        \"max\": 28.635874255728172,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          28.635874255728172,\n          10.977281761408955,\n          23.771792578237726\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rouge2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.101704601687299,\n        \"min\": 1.9440093037647932,\n        \"max\": 9.618124519432426,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          9.618124519432426,\n          1.9440093037647932,\n          8.222793347601673\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rougeL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.881426094408254,\n        \"min\": 9.636944068641258,\n        \"max\": 21.205387493718362,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          21.205387493718362,\n          9.636944068641258,\n          17.30620340596481\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 362.45749717977753,\n        \"min\": 101.63183641433716,\n        \"max\": 987.636198759079,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          393.9294948577881,\n          263.02784180641174,\n          748.2234883308411\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"throughput\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7201431081377649,\n        \"min\": 0.2025037156913559,\n        \"max\": 1.9678872984704436,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.5077050655275298,\n          0.7603757785732805,\n          0.26729981498731864\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"efficiency\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07975915798730271,\n        \"min\": 0.014485775990486366,\n        \"max\": 0.20724446852009892,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.05383041323517471,\n          0.03663849424630127,\n          0.023129724842736486\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.866576615173311,\n        \"min\": -0.9599622636932412,\n        \"max\": 1.230693974550218,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.4632295061715081,\n          -0.9599622636932412,\n          -0.16234214229540284\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df_sorted"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-75aa3efe-a08e-4ea6-98a1-e3c0420e5643\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>time</th>\n",
       "      <th>throughput</th>\n",
       "      <th>efficiency</th>\n",
       "      <th>composite_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BART-large</th>\n",
       "      <td>facebook/bart-large-cnn</td>\n",
       "      <td>28.105383</td>\n",
       "      <td>9.183429</td>\n",
       "      <td>21.062636</td>\n",
       "      <td>101.631836</td>\n",
       "      <td>1.967887</td>\n",
       "      <td>0.207244</td>\n",
       "      <td>1.230694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLaMA-1B</th>\n",
       "      <td>meta-llama/Llama-3.2-1B-Instruct</td>\n",
       "      <td>28.635874</td>\n",
       "      <td>9.618125</td>\n",
       "      <td>21.205387</td>\n",
       "      <td>393.929495</td>\n",
       "      <td>0.507705</td>\n",
       "      <td>0.053830</td>\n",
       "      <td>0.463230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLaMA-3B</th>\n",
       "      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n",
       "      <td>23.771793</td>\n",
       "      <td>8.222793</td>\n",
       "      <td>17.306203</td>\n",
       "      <td>748.223488</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.023130</td>\n",
       "      <td>-0.162342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phi-3-Mini</th>\n",
       "      <td>microsoft/Phi-3-mini-4k-instruct</td>\n",
       "      <td>20.550442</td>\n",
       "      <td>7.028457</td>\n",
       "      <td>14.306677</td>\n",
       "      <td>987.636199</td>\n",
       "      <td>0.202504</td>\n",
       "      <td>0.014486</td>\n",
       "      <td>-0.571619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T5-large</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>10.977282</td>\n",
       "      <td>1.944009</td>\n",
       "      <td>9.636944</td>\n",
       "      <td>263.027842</td>\n",
       "      <td>0.760376</td>\n",
       "      <td>0.036638</td>\n",
       "      <td>-0.959962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-75aa3efe-a08e-4ea6-98a1-e3c0420e5643')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-75aa3efe-a08e-4ea6-98a1-e3c0420e5643 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-75aa3efe-a08e-4ea6-98a1-e3c0420e5643');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-d61dda4a-15f3-43fc-bb00-d3b46f58a058\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d61dda4a-15f3-43fc-bb00-d3b46f58a058')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-d61dda4a-15f3-43fc-bb00-d3b46f58a058 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_cd1e7008-0645-4447-a6fe-d16d7c384779\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_sorted')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_cd1e7008-0645-4447-a6fe-d16d7c384779 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df_sorted');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                    model_id     rouge1    rouge2     rougeL  \\\n",
       "BART-large           facebook/bart-large-cnn  28.105383  9.183429  21.062636   \n",
       "LLaMA-1B    meta-llama/Llama-3.2-1B-Instruct  28.635874  9.618125  21.205387   \n",
       "LLaMA-3B    meta-llama/Llama-3.2-3B-Instruct  23.771793  8.222793  17.306203   \n",
       "Phi-3-Mini  microsoft/Phi-3-mini-4k-instruct  20.550442  7.028457  14.306677   \n",
       "T5-large                            t5-large  10.977282  1.944009   9.636944   \n",
       "\n",
       "                  time  throughput  efficiency  composite_score  \n",
       "BART-large  101.631836    1.967887    0.207244         1.230694  \n",
       "LLaMA-1B    393.929495    0.507705    0.053830         0.463230  \n",
       "LLaMA-3B    748.223488    0.267300    0.023130        -0.162342  \n",
       "Phi-3-Mini  987.636199    0.202504    0.014486        -0.571619  \n",
       "T5-large    263.027842    0.760376    0.036638        -0.959962  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved outputs to /content/llmed_certification_FineTuneFlow/outputs/benchmarks/notebook_C\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# á¡ Notebook C â€” Large Model Safe Benchmarking (Refactored)\n",
    "# =====================================================\n",
    "# Usage: paste into Google Colab and run.\n",
    "# Notes:\n",
    "#  - Designed to be Colab-friendly (offload support, small sample defaults).\n",
    "#  - Batches causal models for better throughput.\n",
    "#  - Seeds for reproducibility.\n",
    "#  - Standardized metric normalization (z-score) for composite scoring.\n",
    "# =====================================================\n",
    "\n",
    "# -------------------------\n",
    "# Install dependencies (run once)\n",
    "# -------------------------\n",
    "!pip install -q datasets transformers accelerate sentencepiece rouge-score pandas tqdm matplotlib seaborn\n",
    "\n",
    "# -------------------------\n",
    "# Imports\n",
    "# -------------------------\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# -------------------------\n",
    "# Configuration (tweak here)\n",
    "# -------------------------\n",
    "DEBUG = False                 # set True to enable CUDA_LAUNCH_BLOCKING for debugging\n",
    "USE_OFFLOAD = True            # set True to allow model offloading (helps on Colab)\n",
    "OFFLOAD_FOLDER = \"offload\"    # folder for offloaded tensors\n",
    "N_SAMPLES = 200               # number of test samples to evaluate (use small number for quick runs)\n",
    "SEQ2SEQ_BATCH_SIZE = 4        # batch size for seq2seq (e.g., BART, T5)\n",
    "CAUSAL_BATCH_SIZE = 2         # batch size for causal (e.g., LLaMA-like) -- keep small for memory\n",
    "#MAX_INPUT_TOKENS = 1024       # tokenizer max length for inputs\n",
    "MAX_INPUT_TOKENS = 768\n",
    "#MAX_NEW_TOKENS = 120          # tokens to generate\n",
    "MAX_NEW_TOKENS = 192\n",
    "#SAVE_DIR = \"outputs/benchmarks/notebook_C\"\n",
    "SAVE_DIR = \"/content/llmed_certification_FineTuneFlow/outputs/benchmarks/notebook_C\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Optional debug env\n",
    "# -------------------------\n",
    "if DEBUG:\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# -------------------------\n",
    "# Device\n",
    "# -------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"áŠ‚ Using device: {device}\")\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# -------------------------\n",
    "# Load dataset (small subset by default)\n",
    "# -------------------------\n",
    "dataset = load_dataset(\"knkarthick/highlightsum\")\n",
    "train_data = dataset[\"train\"]\n",
    "val_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"].select(range(min(N_SAMPLES, len(dataset[\"test\"]))))\n",
    "print(\"Train samples:\", len(train_data))\n",
    "print(\"Validation samples:\", len(val_data))\n",
    "print(\"Test samples (used):\", len(test_data))\n",
    "print(\"Columns:\", test_data.column_names)\n",
    "\n",
    "# -------------------------\n",
    "# ROUGE scorer\n",
    "# -------------------------\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"], use_stemmer=True)\n",
    "\n",
    "def compute_rouge(preds, refs):\n",
    "    \"\"\"\n",
    "    Compute average ROUGE f-measures (percent).\n",
    "    Uses rouge_scorer where signature is: score(target/reference, prediction)\n",
    "    \"\"\"\n",
    "    assert len(preds) == len(refs), \"Predictions and references must match length\"\n",
    "    agg = {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        # target = reference, prediction = model output\n",
    "        scores = scorer.score(ref, pred)\n",
    "        for k in agg:\n",
    "            agg[k] += scores[k].fmeasure\n",
    "    n = len(preds) if len(preds) > 0 else 1\n",
    "    return {k: (v / n) * 100.0 for k, v in agg.items()}\n",
    "\n",
    "# -------------------------\n",
    "# Utilities\n",
    "# -------------------------\n",
    "def batchify(lst, batch_size):\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i:i+batch_size]\n",
    "\n",
    "def safe_tokenizer_pad(tokenizer):\n",
    "    if tokenizer.pad_token is None:\n",
    "        # ensure a pad token exists for batching\n",
    "        if tokenizer.eos_token is not None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        else:\n",
    "            tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "    return tokenizer\n",
    "\n",
    "# -------------------------\n",
    "# Summarization inference functions\n",
    "# -------------------------\n",
    "def summarize_seq2seq_batch(model, tokenizer, texts, max_input_tokens=MAX_INPUT_TOKENS, max_new_tokens=MAX_NEW_TOKENS):\n",
    "    \"\"\"\n",
    "    Perform batched seq2seq inference. Returns (pred_texts, elapsed_time_seconds).\n",
    "    Measures only the generation time (tokenization done outside timer).\n",
    "    \"\"\"\n",
    "    # Tokenize (on CPU/GPU depending on tokenizer implementation)\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        max_length=max_input_tokens,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    start = time.time()\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    latency = time.time() - start\n",
    "    preds = [tokenizer.decode(o, skip_special_tokens=True).strip() for o in output_ids]\n",
    "    return preds, latency\n",
    "\n",
    "def summarize_causal_batch(model, tokenizer, texts, system_prompt=\"Summarize this text concisely.\", max_input_tokens=MAX_INPUT_TOKENS, max_new_tokens=MAX_NEW_TOKENS):\n",
    "    \"\"\"\n",
    "    Perform batched inference for causal/chat-style models. Accepts a list of raw texts.\n",
    "    We create a simple prompt; for more advanced chat tokenizers, a tokenizer-specific template may be needed.\n",
    "    Returns (pred_texts, elapsed_time_seconds).\n",
    "    \"\"\"\n",
    "    # Build prompts: simple system + user text; if tokenizer has chat helpers you can extend this.\n",
    "    prompts = []\n",
    "    for t in texts:\n",
    "        # Use a modest prompt that works with most causal models\n",
    "        p = f\"{system_prompt}\\n\\nText:\\n{t}\\n\\nSummary:\"\n",
    "        prompts.append(p)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        max_length=max_input_tokens,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    start = time.time()\n",
    "    out_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.pad_token_id)\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    latency = time.time() - start\n",
    "    preds = [tokenizer.decode(o, skip_special_tokens=True).strip() for o in out_ids]\n",
    "    # Post-process: remove the prompt prefix if tokenizer doesn't handle it (best-effort)\n",
    "    # We'll do a simple split by \"Summary:\" and take the tail if present\n",
    "    cleaned = []\n",
    "    for prompt, pred in zip(prompts, preds):\n",
    "        if \"Summary:\" in pred:\n",
    "            cleaned.append(pred.split(\"Summary:\",1)[-1].strip())\n",
    "        else:\n",
    "            # fallback: try to remove repeating prompt portion\n",
    "            cleaned.append(pred.replace(prompt, \"\").strip()[:1000])\n",
    "    return cleaned, latency\n",
    "\n",
    "# -------------------------\n",
    "# Model benchmark core\n",
    "# -------------------------\n",
    "def load_model_safe(model_id, use_offload=USE_OFFLOAD, torch_dtype=torch.float16):\n",
    "    \"\"\"\n",
    "    Load a model with device_map='auto'. If offload enabled, set offload_folder.\n",
    "    Returns the loaded model and a flag is_seq2seq.\n",
    "    \"\"\"\n",
    "    # attempt to detect seq2seq by config when possible\n",
    "    # We try Seq2Seq loader first only if model config indicates encoder-decoder\n",
    "    kwargs = {\"device_map\": \"auto\", \"torch_dtype\": torch_dtype}\n",
    "    if use_offload:\n",
    "        os.makedirs(OFFLOAD_FOLDER, exist_ok=True)\n",
    "        kwargs[\"offload_folder\"] = OFFLOAD_FOLDER\n",
    "\n",
    "    # We'll try to load the model with generic loaders. Errors are caught by caller.\n",
    "    # Load as seq2seq if model config indicates encoder-decoder (preferred detection).\n",
    "    model_try = None\n",
    "    # We need to inspect remote config -> using AutoModelForSeq2SeqLM may fail on causal models,\n",
    "    # so we try seq2seq first and fallback to causal.\n",
    "    try:\n",
    "        model_try = AutoModelForSeq2SeqLM.from_pretrained(model_id, **kwargs)\n",
    "        is_seq2seq = getattr(model_try.config, \"is_encoder_decoder\", True)\n",
    "        return model_try, is_seq2seq\n",
    "    except Exception:\n",
    "        # fallback to causal\n",
    "        model_try = AutoModelForCausalLM.from_pretrained(model_id, **kwargs)\n",
    "        is_seq2seq = getattr(model_try.config, \"is_encoder_decoder\", False)\n",
    "        return model_try, is_seq2seq\n",
    "\n",
    "def benchmark_models_safe(models_dict, n_samples=N_SAMPLES, seq2seq_bs=SEQ2SEQ_BATCH_SIZE, causal_bs=CAUSAL_BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Benchmark multiple models safely.\n",
    "    Returns a dict of results per model.\n",
    "    Each model result contains: rouge1, rouge2, rougeL, time, throughput, efficiency\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    refs = test_data[\"summary\"][:n_samples]\n",
    "\n",
    "    for name, model_id in models_dict.items():\n",
    "        print(f\"\\ná½¨0 Benchmarking {name} ({model_id}) ...\")\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "            tokenizer = safe_tokenizer_pad(tokenizer)\n",
    "\n",
    "            # Load model\n",
    "            model, is_seq2seq = load_model_safe(model_id)\n",
    "            model.eval()\n",
    "            # ensure model uses tokenizer length if we added tokens\n",
    "            try:\n",
    "                model.resize_token_embeddings(len(tokenizer))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # Move model to correct device is handled by device_map=\"auto\" loader.\n",
    "            # But in CPU-only environment we move to CPU device explicitly.\n",
    "            if not torch.cuda.is_available():\n",
    "                model.to(device)\n",
    "\n",
    "            preds = []\n",
    "            total_time = 0.0\n",
    "\n",
    "            if is_seq2seq:\n",
    "                # Batch processing for seq2seq models\n",
    "                indices = list(range(0, n_samples))\n",
    "                for i in tqdm(range(0, n_samples, seq2seq_bs), desc=f\"{name} (seq2seq)\"):\n",
    "                    batch_texts = test_data[\"dialogue\"][i:i+seq2seq_bs]\n",
    "                    batch_preds, latency = summarize_seq2seq_batch(model, tokenizer, batch_texts)\n",
    "                    preds.extend(batch_preds)\n",
    "                    total_time += latency\n",
    "            else:\n",
    "                # Batch processing for causal/chat models\n",
    "                for i in tqdm(range(0, n_samples, causal_bs), desc=f\"{name} (causal)\"):\n",
    "                    batch_texts = test_data[\"dialogue\"][i:i+causal_bs]\n",
    "                    batch_preds, latency = summarize_causal_batch(model, tokenizer, batch_texts)\n",
    "                    preds.extend(batch_preds)\n",
    "                    total_time += latency\n",
    "\n",
    "            # ensure correct length\n",
    "            preds = preds[:n_samples]\n",
    "            refs_used = refs[:len(preds)]\n",
    "\n",
    "            # Compute metrics\n",
    "            scores = compute_rouge(preds, refs_used)\n",
    "            total_time = max(total_time, 1e-8)  # avoid div by zero\n",
    "            throughput = len(preds) / total_time\n",
    "            efficiency = scores[\"rougeL\"] / total_time\n",
    "\n",
    "            results[name] = {\n",
    "                \"model_id\": model_id,\n",
    "                \"rouge1\": scores[\"rouge1\"],\n",
    "                \"rouge2\": scores[\"rouge2\"],\n",
    "                \"rougeL\": scores[\"rougeL\"],\n",
    "                \"time\": total_time,\n",
    "                \"throughput\": throughput,\n",
    "                \"efficiency\": efficiency\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error with {name}: {e}\")\n",
    "            results[name] = {\n",
    "                \"model_id\": model_id,\n",
    "                \"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0,\n",
    "                \"time\": float(\"inf\"), \"throughput\": 0.0, \"efficiency\": 0.0\n",
    "            }\n",
    "\n",
    "        # Free GPU memory after each model if available\n",
    "        try:\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return results\n",
    "\n",
    "# -------------------------\n",
    "# Models to benchmark (example sets)\n",
    "# -------------------------\n",
    "MODELS_A = {\n",
    "    \"BART-large\": \"facebook/bart-large-cnn\",\n",
    "    \"T5-large\": \"t5-large\",\n",
    "}\n",
    "\n",
    "MODELS_B = {\n",
    "    # Replace with models accessible in your environment; some may not be downloadable in limited Colab.\n",
    "    \"Phi-3-Mini\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    \"LLaMA-1B\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"LLaMA-3B\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "}\n",
    "\n",
    "# =====================================================\n",
    "# RUN BENCHMARK (A then B)\n",
    "# =====================================================\n",
    "print(\"\\náŠ‚ Running Model Benchmarks...\")\n",
    "results_A = benchmark_models_safe(MODELS_A, n_samples=min(N_SAMPLES, len(test_data)))\n",
    "results_B = benchmark_models_safe(MODELS_B, n_samples=min(N_SAMPLES, len(test_data)))\n",
    "results_all = {**results_A, **results_B}\n",
    "\n",
    "# =====================================================\n",
    "# Build DataFrame and compute composite score\n",
    "# =====================================================\n",
    "df = pd.DataFrame.from_dict(results_all, orient=\"index\")\n",
    "# Ensure numeric types\n",
    "for col in [\"rouge1\",\"rouge2\",\"rougeL\",\"time\",\"throughput\",\"efficiency\"]:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "# Standardize (z-score) selected metrics to make composite score robust to scale\n",
    "metrics_for_norm = [\"rougeL\", \"efficiency\", \"throughput\"]\n",
    "df_norm = df.copy()\n",
    "for col in metrics_for_norm:\n",
    "    if df[col].std() > 0:\n",
    "        df_norm[col] = (df[col] - df[col].mean()) / df[col].std()\n",
    "    else:\n",
    "        df_norm[col] = 0.0\n",
    "\n",
    "# Composite: weights (tunable)\n",
    "df[\"composite_score\"] = (\n",
    "    0.60 * df_norm[\"rougeL\"]\n",
    "    + 0.25 * df_norm[\"efficiency\"]\n",
    "    + 0.15 * df_norm[\"throughput\"]\n",
    ")\n",
    "\n",
    "df_sorted = df.sort_values(\"composite_score\", ascending=False)\n",
    "display(df_sorted)\n",
    "\n",
    "# =====================================================\n",
    "# Save results\n",
    "# =====================================================\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "csv_path = os.path.join(SAVE_DIR, \"final_ranking.csv\")\n",
    "html_path = os.path.join(SAVE_DIR, \"final_ranking.html\")\n",
    "json_path = os.path.join(SAVE_DIR, \"final_ranking.json\")\n",
    "\n",
    "df_sorted.to_csv(csv_path)\n",
    "df_sorted.to_html(html_path)\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(df_sorted.to_dict(orient=\"index\"), f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved outputs to {SAVE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38789b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clone the GitHub in Colab\n",
    "from getpass import getpass\n",
    "token = getpass()\n",
    "\n",
    "# Construct the URL for git clone, typically to the root of the repository\n",
    "# Assuming the repository is 'tttry' on GitHub for user 'micag2025'\n",
    "git_url = f\"https://micag2025:{token}@github.com/micag2025/llmed_certification_FineTuneFlow.git\"\n",
    "\n",
    "# Install git (if not already installed)\n",
    "!apt-get install -y git\n",
    "\n",
    "# Clone the repository\n",
    "!git clone \"$git_url\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eIgInpOrj70i",
   "metadata": {
    "id": "eIgInpOrj70i"
   },
   "source": [
    "Improved version Notebook D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rGHxZTSakB_x",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hLoaded final ranking:\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"print(\\\"Files:\\\\n \\\", \\\"\\\\n \\\"\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"model_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"meta-llama/Llama-3.2-1B-Instruct\",\n          \"t5-large\",\n          \"meta-llama/Llama-3.2-3B-Instruct\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rouge1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.197540545985215,\n        \"min\": 10.977281761408957,\n        \"max\": 28.63587425572817,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          28.63587425572817,\n          10.977281761408957,\n          23.77179257823773\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rouge2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.101704601687299,\n        \"min\": 1.9440093037647928,\n        \"max\": 9.618124519432426,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          9.618124519432426,\n          1.9440093037647928,\n          8.222793347601673\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rougeL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.881426094408252,\n        \"min\": 9.636944068641258,\n        \"max\": 21.20538749371836,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          21.20538749371836,\n          9.636944068641258,\n          17.30620340596481\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 362.45749717977753,\n        \"min\": 101.63183641433716,\n        \"max\": 987.636198759079,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          393.9294948577881,\n          263.02784180641174,\n          748.2234883308411\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"throughput\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7201431081377652,\n        \"min\": 0.2025037156913559,\n        \"max\": 1.967887298470444,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.5077050655275298,\n          0.7603757785732805,\n          0.2672998149873186\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"efficiency\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07975915798730274,\n        \"min\": 0.0144857759904863,\n        \"max\": 0.2072444685200989,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0538304132351747,\n          0.0366384942463012,\n          0.0231297248427364\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.866576615173311,\n        \"min\": -0.9599622636932412,\n        \"max\": 1.230693974550218,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.4632295061715081,\n          -0.9599622636932412,\n          -0.1623421422954028\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-51727ccb-ca86-4f62-9dc7-d20f4ba4c043\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>time</th>\n",
       "      <th>throughput</th>\n",
       "      <th>efficiency</th>\n",
       "      <th>composite_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BART-large</th>\n",
       "      <td>facebook/bart-large-cnn</td>\n",
       "      <td>28.105383</td>\n",
       "      <td>9.183429</td>\n",
       "      <td>21.062636</td>\n",
       "      <td>101.631836</td>\n",
       "      <td>1.967887</td>\n",
       "      <td>0.207244</td>\n",
       "      <td>1.230694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLaMA-1B</th>\n",
       "      <td>meta-llama/Llama-3.2-1B-Instruct</td>\n",
       "      <td>28.635874</td>\n",
       "      <td>9.618125</td>\n",
       "      <td>21.205387</td>\n",
       "      <td>393.929495</td>\n",
       "      <td>0.507705</td>\n",
       "      <td>0.053830</td>\n",
       "      <td>0.463230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLaMA-3B</th>\n",
       "      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n",
       "      <td>23.771793</td>\n",
       "      <td>8.222793</td>\n",
       "      <td>17.306203</td>\n",
       "      <td>748.223488</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.023130</td>\n",
       "      <td>-0.162342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phi-3-Mini</th>\n",
       "      <td>microsoft/Phi-3-mini-4k-instruct</td>\n",
       "      <td>20.550442</td>\n",
       "      <td>7.028457</td>\n",
       "      <td>14.306677</td>\n",
       "      <td>987.636199</td>\n",
       "      <td>0.202504</td>\n",
       "      <td>0.014486</td>\n",
       "      <td>-0.571619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T5-large</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>10.977282</td>\n",
       "      <td>1.944009</td>\n",
       "      <td>9.636944</td>\n",
       "      <td>263.027842</td>\n",
       "      <td>0.760376</td>\n",
       "      <td>0.036638</td>\n",
       "      <td>-0.959962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-51727ccb-ca86-4f62-9dc7-d20f4ba4c043')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-51727ccb-ca86-4f62-9dc7-d20f4ba4c043 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-51727ccb-ca86-4f62-9dc7-d20f4ba4c043');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-8684e458-858a-40a5-94ac-ef5666b38de4\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8684e458-858a-40a5-94ac-ef5666b38de4')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-8684e458-858a-40a5-94ac-ef5666b38de4 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                    model_id     rouge1    rouge2     rougeL  \\\n",
       "BART-large           facebook/bart-large-cnn  28.105383  9.183429  21.062636   \n",
       "LLaMA-1B    meta-llama/Llama-3.2-1B-Instruct  28.635874  9.618125  21.205387   \n",
       "LLaMA-3B    meta-llama/Llama-3.2-3B-Instruct  23.771793  8.222793  17.306203   \n",
       "Phi-3-Mini  microsoft/Phi-3-mini-4k-instruct  20.550442  7.028457  14.306677   \n",
       "T5-large                            t5-large  10.977282  1.944009   9.636944   \n",
       "\n",
       "                  time  throughput  efficiency  composite_score  \n",
       "BART-large  101.631836    1.967887    0.207244         1.230694  \n",
       "LLaMA-1B    393.929495    0.507705    0.053830         0.463230  \n",
       "LLaMA-3B    748.223488    0.267300    0.023130        -0.162342  \n",
       "Phi-3-Mini  987.636199    0.202504    0.014486        -0.571619  \n",
       "T5-large    263.027842    0.760376    0.036638        -0.959962  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected GPU mem (GB): 15.828320256\n",
      "\n",
      "Top selected models:\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"top_models\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"model_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"meta-llama/Llama-3.2-1B-Instruct\",\n          \"facebook/bart-large-cnn\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rouge1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3751142202454468,\n        \"min\": 28.10538263801805,\n        \"max\": 28.63587425572817,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          28.63587425572817,\n          28.10538263801805\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rouge2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.30737620510600716,\n        \"min\": 9.183428921420736,\n        \"max\": 9.618124519432426,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          9.618124519432426,\n          9.183428921420736\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rougeL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10094060409651018,\n        \"min\": 21.06263592241094,\n        \"max\": 21.20538749371836,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          21.20538749371836,\n          21.06263592241094\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 206.68565641031347,\n        \"min\": 101.63183641433716,\n        \"max\": 393.9294948577881,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          393.9294948577881,\n          101.63183641433716\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"throughput\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0325047586820497,\n        \"min\": 0.5077050655275298,\n        \"max\": 1.967887298470444,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.5077050655275298,\n          1.967887298470444\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"efficiency\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1084801188212978,\n        \"min\": 0.0538304132351747,\n        \"max\": 0.2072444685200989,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0538304132351747,\n          0.2072444685200989\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5426793299103144,\n        \"min\": 0.4632295061715081,\n        \"max\": 1.230693974550218,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.4632295061715081,\n          1.230693974550218\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"size_hint\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"1B\",\n          \"0.4B\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "top_models"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-ddd32870-aa70-41ec-929b-c6b89cb31752\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>time</th>\n",
       "      <th>throughput</th>\n",
       "      <th>efficiency</th>\n",
       "      <th>composite_score</th>\n",
       "      <th>size_hint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BART-large</th>\n",
       "      <td>facebook/bart-large-cnn</td>\n",
       "      <td>28.105383</td>\n",
       "      <td>9.183429</td>\n",
       "      <td>21.062636</td>\n",
       "      <td>101.631836</td>\n",
       "      <td>1.967887</td>\n",
       "      <td>0.207244</td>\n",
       "      <td>1.230694</td>\n",
       "      <td>0.4B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLaMA-1B</th>\n",
       "      <td>meta-llama/Llama-3.2-1B-Instruct</td>\n",
       "      <td>28.635874</td>\n",
       "      <td>9.618125</td>\n",
       "      <td>21.205387</td>\n",
       "      <td>393.929495</td>\n",
       "      <td>0.507705</td>\n",
       "      <td>0.053830</td>\n",
       "      <td>0.463230</td>\n",
       "      <td>1B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ddd32870-aa70-41ec-929b-c6b89cb31752')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-ddd32870-aa70-41ec-929b-c6b89cb31752 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-ddd32870-aa70-41ec-929b-c6b89cb31752');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-584b4f46-f0e3-46b0-af93-74a613f987b1\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-584b4f46-f0e3-46b0-af93-74a613f987b1')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-584b4f46-f0e3-46b0-af93-74a613f987b1 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_2e1c89f8-918d-4da3-a9e4-e91039c4cbbd\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('top_models')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_2e1c89f8-918d-4da3-a9e4-e91039c4cbbd button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('top_models');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                    model_id     rouge1    rouge2     rougeL  \\\n",
       "BART-large           facebook/bart-large-cnn  28.105383  9.183429  21.062636   \n",
       "LLaMA-1B    meta-llama/Llama-3.2-1B-Instruct  28.635874  9.618125  21.205387   \n",
       "\n",
       "                  time  throughput  efficiency  composite_score size_hint  \n",
       "BART-large  101.631836    1.967887    0.207244         1.230694      0.4B  \n",
       "LLaMA-1B    393.929495    0.507705    0.053830         0.463230        1B  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ” Fine-tuning plan written to /content/llmed_certification_FineTuneFlow/outputs/benchmarks/notebook_D_pro/finetune_plan_pro_20251202_123700.md\n",
      "\n",
      "ðŸ“ Outputs written to: /content/llmed_certification_FineTuneFlow/outputs/benchmarks/notebook_D_pro\n",
      "Files:\n",
      "  finetune_plan_pro_20251202_123700.md\n",
      " recommendations_pro_20251202_123700.json\n",
      " train_lora_BART-large_20251202_123700.py\n",
      " train_q_lora_LLaMA-1B_20251202_123700.py\n"
     ]
    }
   ],
   "source": [
    "# Option 2 â€” Pro-level Notebook D (model-aware templates & GPU checks)\n",
    "# More robust: model-aware prompts, GPU memory check, BitsAndBytes import, target module heuristics.\n",
    "\n",
    "!pip install -q pandas transformers bitsandbytes peft\n",
    "\n",
    "import os, json, datetime, torch\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from textwrap import dedent\n",
    "\n",
    "# -------------------------\n",
    "# Config / Paths\n",
    "# -------------------------\n",
    "FINAL_CSV = \"/content/llmed_certification_FineTuneFlow/outputs/benchmarks/notebook_C/final_ranking.csv\"\n",
    "OUT_DIR = \"/content/llmed_certification_FineTuneFlow/outputs/benchmarks/notebook_D_pro\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(FINAL_CSV):\n",
    "    raise FileNotFoundError(f\"Cannot find merged ranking CSV at {FINAL_CSV}. Run Notebook C first.\")\n",
    "\n",
    "df = pd.read_csv(FINAL_CSV, index_col=0)\n",
    "print(\"Loaded final ranking:\")\n",
    "display(df.head())\n",
    "\n",
    "# -------------------------\n",
    "# Improved size map & infer\n",
    "# -------------------------\n",
    "size_hints = {\n",
    "    \"bart-large\": \"0.4B\",\n",
    "    \"bart\": \"0.4B\",\n",
    "    \"t5-large\": \"0.8B\",\n",
    "    \"t5\": \"0.8B\",\n",
    "    \"llama-1b\": \"1B\",\n",
    "    \"llama-3b\": \"3B\",\n",
    "    \"llama\": \"3B\",\n",
    "    \"phi-3-mini\": \"4B\",\n",
    "    \"phi\": \"4B\",\n",
    "}\n",
    "\n",
    "def infer_model_size(model_name):\n",
    "    key = model_name.lower().replace(\"/\", \"-\")\n",
    "    for k, v in size_hints.items():\n",
    "        if k in key:\n",
    "            return v\n",
    "    return \"unknown\"\n",
    "\n",
    "df[\"size_hint\"] = df.index.map(infer_model_size)\n",
    "\n",
    "# -------------------------\n",
    "# Detect GPU mem (if available)\n",
    "# -------------------------\n",
    "def get_gpu_mem_gb():\n",
    "    try:\n",
    "        if not torch.cuda.is_available():\n",
    "            return None\n",
    "        prop = torch.cuda.get_device_properties(0)\n",
    "        return prop.total_memory / 1e9\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "gpu_mem = get_gpu_mem_gb()\n",
    "print(\"Detected GPU mem (GB):\", gpu_mem)\n",
    "\n",
    "# -------------------------\n",
    "# Model-aware recommendation logic\n",
    "# -------------------------\n",
    "def recommend_method(model_name, size_hint, gpu_mem_gb=None):\n",
    "    ln = model_name.lower()\n",
    "    if \"bart\" in ln or \"t5\" in ln:\n",
    "        return \"LoRA (PEFT) â€” encoderâ€“decoder friendly\"\n",
    "\n",
    "    try:\n",
    "        gb = float(size_hint.replace(\"b\",\"\").replace(\"B\",\"\").strip())\n",
    "    except:\n",
    "        return \"QLoRA (recommended) / manual check\"\n",
    "\n",
    "    # refine by gpu_mem if known\n",
    "    if gpu_mem_gb is not None:\n",
    "        if gb <= 3 and gpu_mem_gb >= 24:\n",
    "            return \"LoRA or QLoRA (both possible on >=24GB)\"\n",
    "        if gb <= 4.5 and gpu_mem_gb >= 40:\n",
    "            return \"QLoRA (4-bit) on local GPU\"\n",
    "        if gb > 6 and gpu_mem_gb < 40:\n",
    "            return \"Hosted fine-tuning / QLoRA on A100/H100\"\n",
    "\n",
    "    # fallback rules\n",
    "    if gb <= 1.5:\n",
    "        return \"LoRA or full fine-tune\"\n",
    "    if gb <= 4.5:\n",
    "        return \"QLoRA (4-bit) â€” use local or cloud with >=40GB\"\n",
    "    if gb <= 8:\n",
    "        return \"QLoRA (4-bit) â€” GPU with >=40GB recommended\"\n",
    "    return \"Hosted fine-tuning / QLoRA on A100/H100\"\n",
    "\n",
    "def hyperparams_suggestion(model_name, size_hint):\n",
    "    ln = model_name.lower()\n",
    "    try:\n",
    "        gb = float(size_hint.replace(\"b\",\"\").replace(\"B\",\"\").strip())\n",
    "    except:\n",
    "        gb = 3.0\n",
    "    # model-aware tweaks\n",
    "    if \"t5\" in ln:\n",
    "        base_lr = 2e-4\n",
    "    else:\n",
    "        base_lr = 1e-4\n",
    "\n",
    "    if gb <= 1.5:\n",
    "        return {\"epochs\": 3, \"micro_batch_size\": 8, \"lr\": base_lr}\n",
    "    if gb <= 4.5:\n",
    "        return {\"epochs\": 3, \"micro_batch_size\": 4, \"lr\": base_lr / 2}\n",
    "    if gb <= 8:\n",
    "        return {\"epochs\": 3, \"micro_batch_size\": 1, \"lr\": base_lr / 2}\n",
    "    return {\"epochs\": 2, \"micro_batch_size\": 1, \"lr\": base_lr / 5}\n",
    "\n",
    "# -------------------------\n",
    "# Top-K and recommendations\n",
    "# -------------------------\n",
    "TOP_K = 2\n",
    "top_models = df.sort_values(\"composite_score\", ascending=False).head(TOP_K)\n",
    "print(\"\\nTop selected models:\")\n",
    "display(top_models)\n",
    "\n",
    "recommendations = {}\n",
    "for model in top_models.index:\n",
    "    size_hint = infer_model_size(model)\n",
    "    method = recommend_method(model, size_hint, gpu_mem)\n",
    "    hps = hyperparams_suggestion(model, size_hint)\n",
    "    recommendations[model] = {\n",
    "        \"size_hint\": size_hint,\n",
    "        \"method\": method,\n",
    "        \"recommended_hyperparams\": hps,\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Model-aware prompt templates and target module heuristics\n",
    "# -------------------------\n",
    "def prompt_template_for_model(model_name):\n",
    "    ln = model_name.lower()\n",
    "    if \"llama\" in ln or \"meta-llama\" in ln:\n",
    "        return lambda text: f\"[INST] Summarize the conversation:\\n{text} [/INST]\"\n",
    "    if \"phi\" in ln or \"microsoft\" in ln:\n",
    "        return lambda text: f\"<|system|>Summarize the conversation.<|end|>\\\\n{text}\\\\n<|assistant|>\"\n",
    "    if \"t5\" in ln or \"flan\" in ln:\n",
    "        return lambda text: f\"summarize: {text}\"\n",
    "    return lambda text: text\n",
    "\n",
    "def guess_target_modules(model_name):\n",
    "    ln = model_name.lower()\n",
    "    # conservative defaults\n",
    "    if \"t5\" in ln or \"bart\" in ln:\n",
    "        return [\"q_proj\", \"v_proj\"]\n",
    "    if \"llama\" in ln or \"phi\" in ln:\n",
    "        return [\"q_proj\", \"v_proj\"]\n",
    "    return [\"q_proj\", \"v_proj\"]\n",
    "\n",
    "# -------------------------\n",
    "# Write plan + scripts with improved safety\n",
    "# -------------------------\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "plan_lines = [\"# Fine-tuning Plan (Auto-Generated)\", \"\", f\"Generated: {timestamp}\", \"\"]\n",
    "\n",
    "for i, (name, row) in enumerate(top_models.iterrows(), start=1):\n",
    "    rec = recommendations[name]\n",
    "    plan_lines.append(f\"### {i}. {name}\")\n",
    "    plan_lines.append(f\"- Composite score: {row.get('composite_score', 0):.4f}\")\n",
    "    plan_lines.append(f\"- ROUGE-L: {row.get('rougeL', 0):.2f}%\")\n",
    "    plan_lines.append(f\"- Inferred size: {rec['size_hint']}\")\n",
    "    plan_lines.append(f\"- Detected GPU mem (GB): {gpu_mem}\")\n",
    "    plan_lines.append(f\"- Recommended method: **{rec['method']}**\")\n",
    "    plan_lines.append(f\"- Hyperparameters: `{rec['recommended_hyperparams']}`\")\n",
    "    plan_lines.append(\"\")\n",
    "\n",
    "plan_path = Path(OUT_DIR) / f\"finetune_plan_pro_{timestamp}.md\"\n",
    "with plan_path.open(\"w\") as f:\n",
    "    f.write(\"\\n\".join(plan_lines))\n",
    "\n",
    "print(f\"\\nâœ” Fine-tuning plan written to {plan_path}\")\n",
    "\n",
    "# templates for scripts (similar to Option 1 but with BitsAndBytes import and prompt usage)\n",
    "from textwrap import indent\n",
    "\n",
    "def make_lora_script(model_id, data_path, out_dir, target_modules, hps):\n",
    "    return dedent(f\"\"\"\\\n",
    "    # Auto-generated LoRA script (encoder-decoder)\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    MODEL = \"{model_id}\"\n",
    "    DATASET_PATH = \"{data_path}\"\n",
    "    OUTPUT_DIR = \"{out_dir}\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    lora_cfg = LoraConfig(r=8, lora_alpha=32, target_modules={target_modules}, lora_dropout=0.05)\n",
    "    model = get_peft_model(model, lora_cfg)\n",
    "\n",
    "    ds = load_dataset(\"json\", data_files={{\"train\": DATASET_PATH}})[\"train\"]\n",
    "\n",
    "    def tokenize_fn(example):\n",
    "        out = tokenizer(example[\"dialogue\"], truncation=True, max_length=768)\n",
    "        labels = tokenizer(example[\"summary\"], truncation=True, max_length=192).input_ids\n",
    "        out[\"labels\"] = labels\n",
    "        return out\n",
    "\n",
    "    train_ds = ds.map(tokenize_fn, remove_columns=ds.column_names)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size={hps['micro_batch_size']},\n",
    "        num_train_epochs={hps['epochs']},\n",
    "        learning_rate={hps['lr']},\n",
    "        fp16=True,\n",
    "        save_strategy=\"no\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=train_ds)\n",
    "    trainer.train()\n",
    "    model.save_pretrained(OUTPUT_DIR)\n",
    "    \"\"\")\n",
    "\n",
    "def make_q_lora_script(model_id, data_path, out_dir, target_modules, hps):\n",
    "    return dedent(f\"\"\"\\\n",
    "    # Auto-generated QLoRA script (decoder-only)\n",
    "    import transformers\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "    from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    MODEL = \"{model_id}\"\n",
    "    DATASET_PATH = \"{data_path}\"\n",
    "    OUTPUT_DIR = \"{out_dir}\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    bnb = BitsAndBytesConfig(load_in_4bit=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL, device_map=\"auto\", quantization_config=bnb)\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    lora_cfg = LoraConfig(r=8, lora_alpha=32, target_modules={target_modules}, lora_dropout=0.05)\n",
    "    model = get_peft_model(model, lora_cfg)\n",
    "\n",
    "    ds = load_dataset(\"json\", data_files={{\"train\": DATASET_PATH}})[\"train\"]\n",
    "\n",
    "    def tokenize_fn(example):\n",
    "        prompt = example.get(\"prompt\", example[\"dialogue\"])\n",
    "        tok = tokenizer(prompt, truncation=True, max_length=768)\n",
    "        labels = tokenizer(example[\"summary\"], truncation=True, max_length=192).input_ids\n",
    "        tok[\"labels\"] = labels\n",
    "        return tok\n",
    "\n",
    "    train_ds = ds.map(tokenize_fn, remove_columns=ds.column_names)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size={hps['micro_batch_size']},\n",
    "        num_train_epochs={hps['epochs']},\n",
    "        learning_rate={hps['lr']},\n",
    "        fp16=True,\n",
    "        save_strategy=\"no\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=train_ds)\n",
    "    trainer.train()\n",
    "    model.save_pretrained(OUTPUT_DIR)\n",
    "    \"\"\")\n",
    "\n",
    "# Emit scripts\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "DATASET_PATH = \"./highlightsum_train.jsonl\"\n",
    "for model in top_models.index:\n",
    "    rec = recommendations[model]\n",
    "    target_modules = guess_target_modules(model)\n",
    "    hps = rec[\"recommended_hyperparams\"]\n",
    "    outdir = f\"./ft_outputs/{model.replace('/', '_')}_{timestamp}\"\n",
    "    if \"LoRA\" in rec[\"method\"] and (\"encoder\" in rec[\"method\"].lower() or \"encoder\" in rec[\"method\"]):\n",
    "        content = make_lora_script(model, DATASET_PATH, outdir, target_modules, hps)\n",
    "        fname = Path(OUT_DIR) / f\"train_lora_{model.replace('/', '_')}_{timestamp}.py\"\n",
    "    else:\n",
    "        content = make_q_lora_script(model, DATASET_PATH, outdir, target_modules, hps)\n",
    "        fname = Path(OUT_DIR) / f\"train_q_lora_{model.replace('/', '_')}_{timestamp}.py\"\n",
    "    with open(fname, \"w\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "# Save metadata\n",
    "with open(Path(OUT_DIR) / f\"recommendations_pro_{timestamp}.json\", \"w\") as f:\n",
    "    json.dump(recommendations, f, indent=2)\n",
    "\n",
    "print(\"\\nðŸ“ Outputs written to:\", OUT_DIR)\n",
    "print(\"Files:\\n \", \"\\n \".join(sorted(os.listdir(OUT_DIR))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J11ztaclGSb7",
   "metadata": {
    "id": "J11ztaclGSb7"
   },
   "source": [
    "improved train_bart_lora.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cAiZOENyGS5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /content/llmed_certification_FineTuneFlow/train_bart_lora.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /content/llmed_certification_FineTuneFlow/train_bart_lora.py\n",
    "# =====================================================\n",
    "# Optimized BART-LoRA Training for T4 GPU\n",
    "# Uses effective batch size 8 via gradient accumulation\n",
    "# Includes padding, length updates, and stable T4 config\n",
    "# =====================================================\n",
    "\n",
    "# !pip install -q datasets transformers peft wandb accelerate\n",
    "\n",
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "MODEL_NAME = \"facebook/bart-large-cnn\"\n",
    "OUTPUT_DIR = \"/content/llmed_certification_FineTuneFlow/ft_outputs/bart_lora_highlightsum\"\n",
    "\n",
    "#OUTPUT_DIR = \"./ft_outputs/bart_lora_highlightsum\"\n",
    "\n",
    "\n",
    "N_SAMPLES = 2000\n",
    "EPOCHS = 3\n",
    "MICRO_BATCH_SIZE = 4\n",
    "GRAD_ACC = 2                     # â†’ effective batch size = 8\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "MAX_INPUT_LENGTH = 768           # reduced from 1024 â†’ faster\n",
    "MAX_TARGET_LENGTH = 192          # increased summary length\n",
    "\n",
    "WANDB_PROJECT = \"highlightsum_bart_lora\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ðŸ”¥ Using device: {device}\")\n",
    "\n",
    "# -------------------------\n",
    "# W&B (optional)\n",
    "# -------------------------\n",
    "# Comment out the next line to disable W&B logging\n",
    "wandb.login()\n",
    "\n",
    "# -------------------------\n",
    "# Load Dataset\n",
    "# -------------------------\n",
    "dataset = load_dataset(\"knkarthick/highlightsum\")[\"train\"].select(range(N_SAMPLES))\n",
    "print(f\"Loaded {len(dataset)} samples.\")\n",
    "\n",
    "# -------------------------\n",
    "# Tokenizer\n",
    "# -------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token is None else tokenizer.pad_token\n",
    "\n",
    "# -------------------------\n",
    "# Model + LoRA\n",
    "# -------------------------\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],   # correct for BART\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.to(device)\n",
    "\n",
    "# -------------------------\n",
    "# Tokenization\n",
    "# (NOW INCLUDES STATIC PADDING â†’ faster & stable)\n",
    "# -------------------------\n",
    "def tokenize_fn(example):\n",
    "    inputs = tokenizer(\n",
    "        example[\"dialogue\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    label_ids = tokenizer(\n",
    "        example[\"summary\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        padding=\"max_length\"\n",
    "    ).input_ids\n",
    "\n",
    "    inputs[\"labels\"] = label_ids\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_fn, remove_columns=dataset.column_names)\n",
    "\n",
    "# -------------------------\n",
    "# Data Collator\n",
    "# -------------------------\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Training Arguments\n",
    "# -------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACC,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"no\",              # save ONLY final model â†’ MUCH faster\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"bart_lora_highlightsum_t4_optimized\",\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Trainer\n",
    "# -------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Train\n",
    "# -------------------------\n",
    "trainer.train()\n",
    "\n",
    "# -------------------------\n",
    "# Save final model\n",
    "# -------------------------\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Fine-tuned model saved to: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "z0i7LMg6GmPG",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-02 12:58:03.067240: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764680283.324041   21396 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764680283.388529   21396 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764680283.899929   21396 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764680283.899972   21396 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764680283.899977   21396 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764680283.899980   21396 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "ðŸ”¥ Using device: cuda\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33magostinimichelait\u001b[0m (\u001b[33magostinimichelait-ready-tensor\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "Loaded 2000 samples.\n",
      "Map: 100% 2000/2000 [00:07<00:00, 275.19 examples/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m setting up run ksx2f8dk (0.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20251202_125917-ksx2f8dk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbart_lora_highlightsum_t4_optimized\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/agostinimichelait-ready-tensor/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/agostinimichelait-ready-tensor/huggingface/runs/ksx2f8dk\u001b[0m\n",
      "{'loss': 6.3907, 'grad_norm': 3.3136115074157715, 'learning_rate': 0.00018693333333333334, 'epoch': 0.2}\n",
      "{'loss': 2.0406, 'grad_norm': 1.3898366689682007, 'learning_rate': 0.00017360000000000002, 'epoch': 0.4}\n",
      "{'loss': 1.6238, 'grad_norm': 1.8964940309524536, 'learning_rate': 0.00016026666666666667, 'epoch': 0.6}\n",
      "{'loss': 1.5143, 'grad_norm': 1.6119006872177124, 'learning_rate': 0.00014693333333333335, 'epoch': 0.8}\n",
      "{'loss': 1.4638, 'grad_norm': 5.818985462188721, 'learning_rate': 0.00013360000000000002, 'epoch': 1.0}\n",
      "{'loss': 1.4183, 'grad_norm': 0.8711148500442505, 'learning_rate': 0.00012026666666666669, 'epoch': 1.2}\n",
      "{'loss': 1.4067, 'grad_norm': 0.7760555148124695, 'learning_rate': 0.00010693333333333333, 'epoch': 1.4}\n",
      "{'loss': 1.4627, 'grad_norm': 1.1173286437988281, 'learning_rate': 9.360000000000001e-05, 'epoch': 1.6}\n",
      "{'loss': 1.4036, 'grad_norm': 0.8825370073318481, 'learning_rate': 8.026666666666666e-05, 'epoch': 1.8}\n",
      "{'loss': 1.431, 'grad_norm': 1.681577444076538, 'learning_rate': 6.693333333333334e-05, 'epoch': 2.0}\n",
      "{'loss': 1.4073, 'grad_norm': 0.944150984287262, 'learning_rate': 5.360000000000001e-05, 'epoch': 2.2}\n",
      "{'loss': 1.344, 'grad_norm': 1.1954433917999268, 'learning_rate': 4.026666666666667e-05, 'epoch': 2.4}\n",
      "{'loss': 1.3589, 'grad_norm': 0.9167070984840393, 'learning_rate': 2.6933333333333332e-05, 'epoch': 2.6}\n",
      "{'loss': 1.3672, 'grad_norm': 0.6789732575416565, 'learning_rate': 1.3600000000000002e-05, 'epoch': 2.8}\n",
      "{'loss': 1.4279, 'grad_norm': 0.7682539224624634, 'learning_rate': 2.6666666666666667e-07, 'epoch': 3.0}\n",
      "{'train_runtime': 486.4962, 'train_samples_per_second': 12.333, 'train_steps_per_second': 1.542, 'train_loss': 1.8040459493001302, 'epoch': 3.0}\n",
      "100% 750/750 [08:04<00:00,  1.55it/s]\n",
      "\n",
      "ðŸŽ‰ Fine-tuned model saved to: /content/llmed_certification_FineTuneFlow/ft_outputs/bart_lora_highlightsum\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: ðŸš€ View run \u001b[33mbart_lora_highlightsum_t4_optimized\u001b[0m at: \u001b[34m\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251202_125917-ksx2f8dk/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python train_bart_lora.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahxIdaHsJg7o",
   "metadata": {
    "id": "ahxIdaHsJg7o"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "W7y0DS-lQmee",
   "metadata": {
    "id": "W7y0DS-lQmee"
   },
   "source": [
    "- eval_bart_lora.py improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "o_KN4rUiQASg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /content/llmed_certification_FineTuneFlow/eval_bart_lora.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /content/llmed_certification_FineTuneFlow/eval_bart_lora.py\n",
    "# =====================================================\n",
    "# Fast Evaluation for BART-LoRA Highlight Summaries\n",
    "# Batched ROUGE scoring + progress bar + safer padding\n",
    "# =====================================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "MODEL_DIR = \"/content/llmed_certification_FineTuneFlow/ft_outputs/bart_lora_highlightsum\"\n",
    "OUTPUT_CSV = \"/content/llmed_certification_FineTuneFlow/metrics/validation_predictions.csv\"\n",
    "\n",
    "VALIDATION_SIZE = 200\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "MAX_INPUT_LENGTH = 768\n",
    "MAX_NEW_TOKENS = 192\n",
    "BATCH_SIZE = 8       # ðŸ”¥ batched inference = much faster\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# -------------------------\n",
    "# Load validation\n",
    "# -------------------------\n",
    "dataset = load_dataset(\"knkarthick/highlightsum\")[\"validation\"].select(range(VALIDATION_SIZE))\n",
    "print(f\"Loaded {len(dataset)} validation samples.\")\n",
    "\n",
    "dialogues = [ex[\"dialogue\"] for ex in dataset]\n",
    "human_summaries = [ex[\"summary\"] for ex in dataset]\n",
    "\n",
    "# -------------------------\n",
    "# Load model + tokenizer\n",
    "# -------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# -------------------------\n",
    "# ROUGE scorer\n",
    "# -------------------------\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "# -------------------------\n",
    "# Batched inference\n",
    "# -------------------------\n",
    "predictions = []\n",
    "\n",
    "for i in tqdm(range(0, len(dialogues), BATCH_SIZE), desc=\"Generating\"):\n",
    "    batch = dialogues[i:i + BATCH_SIZE]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        batch,\n",
    "        truncation=True,\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            num_beams=4,\n",
    "            no_repeat_ngram_size=3,\n",
    "            length_penalty=1.0,\n",
    "        )\n",
    "\n",
    "    decoded = [tokenizer.decode(o, skip_special_tokens=True) for o in outs]\n",
    "    predictions.extend(decoded)\n",
    "\n",
    "# -------------------------\n",
    "# Compute ROUGE\n",
    "# -------------------------\n",
    "rouge1_list, rouge2_list, rougeL_list = [], [], []\n",
    "\n",
    "for human, pred in zip(human_summaries, predictions):\n",
    "    scores = scorer.score(human, pred)\n",
    "    rouge1_list.append(scores[\"rouge1\"].fmeasure)\n",
    "    rouge2_list.append(scores[\"rouge2\"].fmeasure)\n",
    "    rougeL_list.append(scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "# -------------------------\n",
    "# Save CSV\n",
    "# -------------------------\n",
    "df = pd.DataFrame({\n",
    "    \"dialogue\": dialogues,\n",
    "    \"human_summary\": human_summaries,\n",
    "    \"model_summary\": predictions,\n",
    "    \"rouge1\": rouge1_list,\n",
    "    \"rouge2\": rouge2_list,\n",
    "    \"rougeL\": rougeL_list,\n",
    "})\n",
    "os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Saved evaluation results to {OUTPUT_CSV}\")\n",
    "\n",
    "# -------------------------\n",
    "# Average ROUGE\n",
    "# -------------------------\n",
    "print(\"\\nðŸ“Š Average ROUGE on validation:\")\n",
    "print(f\"ROUGE-1: {df['rouge1'].mean():.3f}\")\n",
    "print(f\"ROUGE-2: {df['rouge2'].mean():.3f}\")\n",
    "print(f\"ROUGE-L: {df['rougeL'].mean():.3f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Inspect one example\n",
    "# -------------------------\n",
    "example_id = 42\n",
    "print(\"\\nðŸ“ Dialogue:\")\n",
    "print(dialogues[example_id])\n",
    "\n",
    "print(\"\\nðŸŸ¡ Model summary:\")\n",
    "print(predictions[example_id])\n",
    "\n",
    "print(\"\\nðŸŸ¢ Human summary:\")\n",
    "print(human_summaries[example_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ooUpnSWyNq7I",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded 200 validation samples.\n",
      "2025-12-02 13:42:11.857784: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764682932.192054   32760 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764682932.283181   32760 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764682932.852064   32760 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764682932.852115   32760 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764682932.852125   32760 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764682932.852132   32760 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Generating: 100% 25/25 [03:47<00:00,  9.11s/it]\n",
      "\n",
      "âœ… Saved evaluation results to /content/llmed_certification_FineTuneFlow/metrics/validation_predictions.csv\n",
      "\n",
      "ðŸ“Š Average ROUGE on validation:\n",
      "ROUGE-1: 0.360\n",
      "ROUGE-2: 0.165\n",
      "ROUGE-L: 0.263\n",
      "\n",
      "ðŸ“ Dialogue:\n",
      "A: Hi Tom, are you busy tomorrowâ€™s afternoon?\n",
      "B: Iâ€™m pretty sure I am. Whatâ€™s up?\n",
      "A: Can you go with me to the animal shelter?.\n",
      "B: What do you want to do?\n",
      "A: I want to get a puppy for my son.\n",
      "B: That will make him so happy.\n",
      "A: Yeah, weâ€™ve discussed it many times. I think heâ€™s ready now.\n",
      "B: Thatâ€™s good. Raising a dog is a tough issue. Like having a baby ;-) \n",
      "A: I'll get him one of those little dogs.\n",
      "B: One that won't grow up too big;-)\n",
      "A: And eat too much;-))\n",
      "B: Do you know which one he would like?\n",
      "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
      "B: I bet you had to drag him away.\n",
      "A: He wanted to take it home right away ;-).\n",
      "B: I wonder what he'll name it.\n",
      "A: He said heâ€™d name it after his dead hamster â€“ Lemmy  - he's  a great Motorhead fan :-)))\n",
      "\n",
      "ðŸŸ¡ Model summary:\n",
      "A wants to get a puppy for his son. He will get one of those little dogs. He took him to the animal shelter last week. He wanted to take it home right away. He named it Lemmy after his dead hamster â€“ Lemmy â€“ he's a great Motorhead fan.\n",
      "\n",
      "ðŸŸ¢ Human summary:\n",
      "A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n"
     ]
    }
   ],
   "source": [
    "!python /content/llmed_certification_FineTuneFlow/eval_bart_lora.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rZ5s_scxTdkG",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "yViRqQMecANk",
   "metadata": {
    "id": "yViRqQMecANk"
   },
   "source": [
    "eval_metrics_bart_lora.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "sNVpDcmDb79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /content/llmed_certification_FineTuneFlow/eval_metrics_bart_lora.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /content/llmed_certification_FineTuneFlow/eval_metrics_bart_lora.py\n",
    "# =====================================================\n",
    "# Evaluation Metrics: ROUGE, BERTScore, BLEU\n",
    "# For BART-LoRA Highlight Summarization\n",
    "# =====================================================\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# -------------------------\n",
    "# Load predictions\n",
    "# -------------------------\n",
    "CSV_PATH = \"/content/llmed_certification_FineTuneFlow/metrics/validation_predictions.csv\"\n",
    "OUTPUT_PATH = \"/content/llmed_certification_FineTuneFlow/metrics/validation_predictions_metrics.csv\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "print(f\"ðŸ“„ Loaded {len(df)} predictions from:\")\n",
    "print(f\"   {CSV_PATH}\")\n",
    "\n",
    "# -------------------------\n",
    "# ROUGE (1, 2, L)\n",
    "# -------------------------\n",
    "print(\"\\nðŸ” Computing ROUGE scores...\")\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(\n",
    "    ['rouge1', 'rouge2', 'rougeL'],\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "rouge1_list, rouge2_list, rougeL_list = [], [], []\n",
    "\n",
    "for pred, ref in zip(df['model_summary'], df['human_summary']):\n",
    "    scores = scorer.score(ref, pred)\n",
    "    rouge1_list.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_list.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_list.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "df['rouge1'] = rouge1_list\n",
    "df['rouge2'] = rouge2_list\n",
    "df['rougeL'] = rougeL_list\n",
    "\n",
    "print(\"âœ… ROUGE completed.\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# BERTScore (F1)\n",
    "# -------------------------\n",
    "print(\"\\nðŸ” Computing BERTScore (F1)...\")\n",
    "\n",
    "P, R, F1 = bert_score(\n",
    "    cands=df['model_summary'].tolist(),\n",
    "    refs=df['human_summary'].tolist(),\n",
    "    lang='en',\n",
    "    rescale_with_baseline=True\n",
    ")\n",
    "\n",
    "df['bert_f1'] = F1.numpy()\n",
    "\n",
    "print(\"âœ… BERTScore completed.\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# BLEU (corpus-level)\n",
    "# -------------------------\n",
    "print(\"\\nðŸ” Computing BLEU (corpus-level)...\")\n",
    "\n",
    "references = [[ref.split()] for ref in df['human_summary']]\n",
    "candidates = [pred.split() for pred in df['model_summary']]\n",
    "\n",
    "bleu = corpus_bleu(references, candidates)\n",
    "\n",
    "print(f\"âœ… BLEU Score: {bleu:.4f}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Save CSV with Metrics\n",
    "# -------------------------\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"\\nðŸ’¾ Metrics saved to:\\n   {OUTPUT_PATH}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Average scores:\")\n",
    "print(f\"ROUGE-1: {df['rouge1'].mean():.3f}\")\n",
    "print(f\"ROUGE-2: {df['rouge2'].mean():.3f}\")\n",
    "print(f\"ROUGE-L: {df['rougeL'].mean():.3f}\")\n",
    "print(f\"BERTScore-F1: {df['bert_f1'].mean():.3f}\")\n",
    "print(f\"BLEU: {bleu:.4f}\")\n",
    "\n",
    "print(\"\\nâœ” Evaluation metrics complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "kHrvLtv9b7xw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/61.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hðŸ“„ Loaded 200 predictions from:\n",
      "   /content/llmed_certification_FineTuneFlow/metrics/validation_predictions.csv\n",
      "\n",
      "ðŸ” Computing ROUGE scores...\n",
      "âœ… ROUGE completed.\n",
      "\n",
      "ðŸ” Computing BERTScore (F1)...\n",
      "tokenizer_config.json: 100% 25.0/25.0 [00:00<00:00, 162kB/s]\n",
      "config.json: 100% 482/482 [00:00<00:00, 2.86MB/s]\n",
      "vocab.json: 100% 899k/899k [00:00<00:00, 4.78MB/s]\n",
      "merges.txt: 100% 456k/456k [00:00<00:00, 3.65MB/s]\n",
      "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 5.46MB/s]\n",
      "2025-12-02 14:24:58.943540: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764685498.988093   43801 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764685499.001803   43801 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764685499.047969   43801 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764685499.048016   43801 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764685499.048023   43801 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764685499.048030   43801 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "model.safetensors: 100% 1.42G/1.42G [00:40<00:00, 34.7MB/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "âœ… BERTScore completed.\n",
      "\n",
      "ðŸ” Computing BLEU (corpus-level)...\n",
      "âœ… BLEU Score: 0.0204\n",
      "\n",
      "ðŸ’¾ Metrics saved to:\n",
      "   /content/llmed_certification_FineTuneFlow/metrics/validation_predictions_metrics.csv\n",
      "\n",
      "ðŸ“Š Average scores:\n",
      "ROUGE-1: 0.360\n",
      "ROUGE-2: 0.165\n",
      "ROUGE-L: 0.263\n",
      "BERTScore-F1: 0.312\n",
      "BLEU: 0.0204\n",
      "\n",
      "âœ” Evaluation metrics complete.\n"
     ]
    }
   ],
   "source": [
    "# Install bert-score and nltk\n",
    "!pip install -q bert-score nltk\n",
    "\n",
    "# Now run the evaluation script\n",
    "!python /content/llmed_certification_FineTuneFlow/eval_metrics_bart_lora.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B0B7d3ISb7lE",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5TgsN9JqTdUO",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge_bart_lora.py improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12kwHSCkTyJ8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /content/llmed_certification_FineTuneFlow/merge_bart_lora.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /content/llmed_certification_FineTuneFlow/merge_bart_lora.py\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# -------------------------\n",
    "# Paths\n",
    "# -------------------------\n",
    "BASE_MODEL = \"facebook/bart-large-cnn\"\n",
    "LORA_PATH = \"/content/llmed_certification_FineTuneFlow/ft_outputs/bart_lora_highlightsum\"\n",
    "OUTPUT_PATH = \"/content/llmed_certification_FineTuneFlow/ft_outputs/bart_merged_highlightsum\"\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "print(\"ðŸ”„ Loading base BART model...\")\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "\n",
    "print(\"ðŸ”Œ Attaching LoRA adapter...\")\n",
    "lora_model = PeftModel.from_pretrained(base_model, LORA_PATH)\n",
    "\n",
    "print(\"ðŸ§¬ Merging LoRA â†’ base model (this may take a moment)...\")\n",
    "merged_model = lora_model.merge_and_unload()  # applies LoRA deltas + removes adapters\n",
    "\n",
    "print(\"ðŸ’¾ Saving merged model...\")\n",
    "merged_model.save_pretrained(OUTPUT_PATH)\n",
    "\n",
    "print(\"ðŸ“ Saving tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(LORA_PATH)  # safer (matches training tokenizer)\n",
    "tokenizer.save_pretrained(OUTPUT_PATH)\n",
    "\n",
    "print(\"\\nâœ… Merge complete!\")\n",
    "print(f\"ðŸš€ Final merged model saved to:\\n{OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47IUg2vlTx-U",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-02 13:48:50.151836: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764683330.194250   34510 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764683330.207342   34510 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764683330.255015   34510 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764683330.255066   34510 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764683330.255071   34510 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764683330.255074   34510 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "ðŸ”„ Loading base BART model...\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "ðŸ”Œ Attaching LoRA adapter...\n",
      "ðŸ§¬ Merging LoRA â†’ base model (this may take a moment)...\n",
      "ðŸ’¾ Saving merged model...\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "ðŸ“ Saving tokenizer...\n",
      "\n",
      "âœ… Merge complete!\n",
      "ðŸš€ Final merged model saved to:\n",
      "/content/llmed_certification_FineTuneFlow/ft_outputs/bart_merged_highlightsum\n"
     ]
    }
   ],
   "source": [
    "!python /content/llmed_certification_FineTuneFlow/merge_bart_lora.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uFdnnq9NUcTP",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "V9gFYiQQWP33",
   "metadata": {
    "id": "V9gFYiQQWP33"
   },
   "source": [
    "A matching inference script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ZLqcNF-vXPCZ",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /content/llmed_certification_FineTuneFlow/inference_bart_lora.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /content/llmed_certification_FineTuneFlow/inference_bart_lora.py\n",
    "# =====================================================\n",
    "# Inference for BART-LoRA Highlight Summarizer\n",
    "# Clean, GPU-optimized, LoRA-safe loading\n",
    "# =====================================================\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "MODEL_DIR = \"/content/llmed_certification_FineTuneFlow/ft_outputs/bart_lora_highlightsum\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "\n",
    "MAX_INPUT_LENGTH = 768        # matches training\n",
    "MAX_NEW_TOKENS = 192          # matches evaluation script\n",
    "NUM_BEAMS = 4\n",
    "NO_REPEAT_NGRAM_SIZE = 3\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "OUTPUT_CSV = \"/content/llmed_certification_FineTuneFlow/metrics/inference_predictions.csv\"\n",
    "\n",
    "print(\"ðŸ”¥ Using device:\", DEVICE)\n",
    "\n",
    "# -------------------------\n",
    "# Load tokenizer\n",
    "# -------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# -------------------------\n",
    "# Load model (merged OR LoRA separate)\n",
    "# -------------------------\n",
    "print(\"\\nðŸ”§ Loading model...\")\n",
    "\n",
    "try:\n",
    "    # Try loading as a merged model\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        MODEL_DIR,\n",
    "        torch_dtype=DTYPE\n",
    "    )\n",
    "    print(\"âœ… Loaded merged model.\")\n",
    "\n",
    "except Exception as err:\n",
    "    # If merged model fails, load base + LoRA adapter\n",
    "    print(\"â„¹ï¸ Merged model not found â€” loading base BART + LoRA adapter.\")\n",
    "    print(\"   Reason:\", err)\n",
    "\n",
    "    base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        \"facebook/bart-large-cnn\",\n",
    "        torch_dtype=DTYPE\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(base_model, MODEL_DIR)\n",
    "    print(\"âœ… Loaded LoRA adapter on top of base model.\")\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Generation helpers\n",
    "# -------------------------\n",
    "def generate_summary(text):\n",
    "    \"\"\"Single-sample inference.\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            num_beams=NUM_BEAMS,\n",
    "            no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,\n",
    "            length_penalty=1.0,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def batch_generate(dialogues):\n",
    "    \"\"\"Fast batch inference.\"\"\"\n",
    "    preds = []\n",
    "\n",
    "    for i in range(0, len(dialogues), BATCH_SIZE):\n",
    "        batch = dialogues[i:i + BATCH_SIZE]\n",
    "\n",
    "        tok = tokenizer(\n",
    "            batch,\n",
    "            truncation=True,\n",
    "            max_length=MAX_INPUT_LENGTH,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outs = model.generate(\n",
    "                **tok,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                num_beams=NUM_BEAMS,\n",
    "                no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "        preds.extend([tokenizer.decode(o, skip_special_tokens=True) for o in outs])\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 1) Quick example\n",
    "# -------------------------\n",
    "sample_dialogue = (\n",
    "    \"A: Hi Tom, are you busy tomorrow afternoon?\\n\"\n",
    "    \"B: I think I am. Why?\\n\"\n",
    "    \"A: I want to go to the animal shelter.\\n\"\n",
    "    \"B: For what?\\n\"\n",
    "    \"A: I'm getting a puppy for my son.\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== Single Example ===\")\n",
    "print(\"âž¡ï¸\", generate_summary(sample_dialogue))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 2) Batch inference\n",
    "# -------------------------\n",
    "print(\"\\n=== Batch inference on HighlightSUM validation (N=200) ===\")\n",
    "\n",
    "try:\n",
    "    ds = load_dataset(\"knkarthick/highlightsum\")[\"validation\"]\n",
    "    N = min(200, len(ds))\n",
    "\n",
    "    dialogues = [ds[i][\"dialogue\"] for i in range(N)]\n",
    "    human_summaries = [ds[i][\"summary\"] for i in range(N)]\n",
    "    predictions = batch_generate(dialogues)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"id\": range(N),\n",
    "        \"dialogue\": dialogues,\n",
    "        \"human_summary\": human_summaries,\n",
    "        \"model_summary\": predictions\n",
    "    })\n",
    "\n",
    "    os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "    print(f\"ðŸ“ Saved predictions to: {OUTPUT_CSV}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"âš ï¸ Dataset could not load (offline).\")\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "print(\"\\nâœ” Inference complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4w7AUgLgU_9B",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-02 14:06:08.302739: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764684368.346255   38940 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764684368.362334   38940 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764684368.418536   38940 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764684368.418587   38940 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764684368.418595   38940 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764684368.418601   38940 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "ðŸ”¥ Using device: cuda\n",
      "\n",
      "ðŸ”§ Loading model...\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "âœ… Loaded merged model.\n",
      "\n",
      "=== Single Example ===\n",
      "âž¡ï¸ B and A are going to the animal shelter tomorrow afternoon. A wants to get a puppy for her son, and wants to go to Tom. Tom is busy tomorrow afternoon, so they won't be able to meet. A is getting the puppy for his son.\n",
      "\n",
      "=== Batch inference on HighlightSUM validation (N=200) ===\n",
      "ðŸ“ Saved predictions to: /content/llmed_certification_FineTuneFlow/metrics/inference_predictions.csv\n",
      "\n",
      "âœ” Inference complete.\n"
     ]
    }
   ],
   "source": [
    "! python /content/llmed_certification_FineTuneFlow/inference_bart_lora.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ovqokXZMU_yA",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "Fh5wbgQldm0Y",
   "metadata": {
    "id": "Fh5wbgQldm0Y"
   },
   "source": [
    "FROM HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7dd76c",
   "metadata": {
    "id": "7e7dd76c"
   },
   "source": [
    "- Qualitative Error Analysis & Failure Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de782dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# â”‘ Qualitative error analysis\n",
    "# =====================================================\n",
    "import numpy as np\n",
    "\n",
    "# Define simple failure buckets\n",
    "def classify_failure(row):\n",
    "    if row['rouge1'] < 0.3:\n",
    "        return 'Low ROUGE'\n",
    "    elif len(row['model_summary'].split()) > len(row['human_summary'].split()) * 1.5:\n",
    "        return 'Over-generated'\n",
    "    elif len(row['model_summary'].split()) < len(row['human_summary'].split()) * 0.5:\n",
    "        return 'Under-generated'\n",
    "    else:\n",
    "        return 'Acceptable'\n",
    "\n",
    "df['failure_bucket'] = df.apply(classify_failure, axis=1)\n",
    "\n",
    "# Inspect top failures\n",
    "low_rouge_samples = df[df['failure_bucket'] == 'Low ROUGE']\n",
    "print(f\"\\nTop Low ROUGE samples: {len(low_rouge_samples)}\")\n",
    "display(low_rouge_samples[['model_summary','human_summary','rouge1']].head(5))\n",
    "\n",
    "# Save error analysis\n",
    "#df.to_csv(\"/content/validation_predictions_error_analysis.csv\", index=False)\n",
    "df.to_csv(\"/content/llmed_certification_FineTuneFlow/metrics/validation_predictions_error_analysis.csv\", index=False)\n",
    "print(\"âœ… Error analysis CSV saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054506e7",
   "metadata": {
    "id": "054506e7"
   },
   "source": [
    "- CSV Visual Dashboard Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7543d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# â€ Simple interactive dashboard in Colab\n",
    "# =====================================================\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from IPython.display import display\n",
    "\n",
    "#df = pd.read_csv(\"/content/validation_predictions_error_analysis.csv\")\n",
    "df = pd.read_csv(\"/content/llmed_certification_FineTuneFlow/metrics/validation_predictions_error_analysis.csv\")\n",
    "\n",
    "\n",
    "# Histogram of ROUGE1 scores\n",
    "fig = px.histogram(df, x='rouge1', nbins=20, title='ROUGE1 Distribution')\n",
    "fig.show()\n",
    "\n",
    "# Scatter: ROUGE1 vs BERT F1\n",
    "fig2 = px.scatter(df, x='rouge1', y='bert_f1', color='failure_bucket',\n",
    "                  title='ROUGE1 vs BERTScore F1', hover_data=['model_summary','human_summary'])\n",
    "fig2.show()\n",
    "\n",
    "# Pie chart of failure buckets\n",
    "fig3 = px.pie(df, names='failure_bucket', title='Failure Buckets')\n",
    "fig3.show()\n",
    "\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a78a272",
   "metadata": {
    "id": "3a78a272"
   },
   "source": [
    "- Evaluation baseline BART vs fine-tuned BART-LoRA, computes ROUGE scores, and outputs a CSV + bar chart comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f4a107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Compare ROUGE: Baseline BART vs Fine-tuned BART-LoRA\n",
    "# =====================================================\n",
    "!pip install -q datasets transformers rouge_score matplotlib\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from rouge_score import rouge_scorer\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "BASELINE_MODEL = \"facebook/bart-large-cnn\"\n",
    "#FT_MODEL_DIR = \"./ft_outputs/bart_lora_highlightsum\"\n",
    "FT_MODEL_DIR = \"/content/llmed_certification_FineTuneFlow/ft_outputs/bart_lora_highlightsum\"\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "VALIDATION_SIZE = 200\n",
    "OUTPUT_CSV = \"rouge_comparison.csv\"  #to be changed to /content/llmed_certification_FineTuneFlow/metrics/rouge_comparison.csv\n",
    "OUTPUT_CSV = \"/content/llmed_certification_FineTuneFlow/metrics/rouge_comparison.csv\"\n",
    "\n",
    "\n",
    "MAX_INPUT_LENGTH = 768\n",
    "MAX_TARGET_LENGTH = 128\n",
    "\n",
    "# -------------------------\n",
    "# Load validation dataset\n",
    "# -------------------------\n",
    "dataset = load_dataset(\"knkarthick/highlightsum\")[\"validation\"].select(range(VALIDATION_SIZE))\n",
    "print(f\"Loaded {len(dataset)} validation samples.\")\n",
    "\n",
    "# -------------------------\n",
    "# Load models and tokenizer\n",
    "# -------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASELINE_MODEL)\n",
    "baseline_model = AutoModelForSeq2SeqLM.from_pretrained(BASELINE_MODEL).to(DEVICE)\n",
    "baseline_model.eval()\n",
    "\n",
    "ft_model = AutoModelForSeq2SeqLM.from_pretrained(FT_MODEL_DIR).to(DEVICE)\n",
    "ft_model.eval()\n",
    "\n",
    "# -------------------------\n",
    "# ROUGE scorer\n",
    "# -------------------------\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# -------------------------\n",
    "# Predict & score\n",
    "# -------------------------\n",
    "results = []\n",
    "\n",
    "for idx, sample in enumerate(dataset):\n",
    "    inputs = tokenizer(sample[\"dialogue\"], return_tensors=\"pt\", truncation=True, max_length=MAX_INPUT_LENGTH).to(DEVICE)\n",
    "\n",
    "    # Baseline prediction\n",
    "    with torch.no_grad():\n",
    "        baseline_output = baseline_model.generate(**inputs, max_new_tokens=MAX_TARGET_LENGTH, num_beams=4)\n",
    "    baseline_pred = tokenizer.decode(baseline_output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Fine-tuned prediction\n",
    "    with torch.no_grad():\n",
    "        ft_output = ft_model.generate(**inputs, max_new_tokens=MAX_TARGET_LENGTH, num_beams=4)\n",
    "    ft_pred = tokenizer.decode(ft_output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Human summary\n",
    "    human_summary = sample[\"summary\"]\n",
    "\n",
    "    # ROUGE scores\n",
    "    rouge_baseline = scorer.score(human_summary, baseline_pred)\n",
    "    rouge_ft = scorer.score(human_summary, ft_pred)\n",
    "\n",
    "    results.append({\n",
    "        \"sample_id\": idx,\n",
    "        \"dialogue\": sample[\"dialogue\"],\n",
    "        \"human_summary\": human_summary,\n",
    "        \"baseline_pred\": baseline_pred,\n",
    "        \"ft_pred\": ft_pred,\n",
    "        \"rouge1_baseline\": rouge_baseline['rouge1'].fmeasure,\n",
    "        \"rouge2_baseline\": rouge_baseline['rouge2'].fmeasure,\n",
    "        \"rougeL_baseline\": rouge_baseline['rougeL'].fmeasure,\n",
    "        \"rouge1_ft\": rouge_ft['rouge1'].fmeasure,\n",
    "        \"rouge2_ft\": rouge_ft['rouge2'].fmeasure,\n",
    "        \"rougeL_ft\": rouge_ft['rougeL'].fmeasure\n",
    "    })\n",
    "\n",
    "# -------------------------\n",
    "# Save CSV\n",
    "# -------------------------\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"âœ… Saved ROUGE comparison to {OUTPUT_CSV}\")\n",
    "\n",
    "# -------------------------\n",
    "# Plot average ROUGE\n",
    "# -------------------------\n",
    "avg_scores = {\n",
    "    \"Baseline\": [df['rouge1_baseline'].mean(), df['rouge2_baseline'].mean(), df['rougeL_baseline'].mean()],\n",
    "    \"Fine-tuned\": [df['rouge1_ft'].mean(), df['rouge2_ft'].mean(), df['rougeL_ft'].mean()]\n",
    "}\n",
    "\n",
    "labels = ['rouge1', 'rouge2', 'rougeL']\n",
    "x = range(len(labels))\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar([i-0.15 for i in x], avg_scores['Baseline'], width=0.3, label='Baseline')\n",
    "plt.bar([i+0.15 for i in x], avg_scores['Fine-tuned'], width=0.3, label='Fine-tuned')\n",
    "plt.xticks(x, labels)\n",
    "plt.ylabel(\"Average ROUGE F1\")\n",
    "plt.title(\"Baseline vs Fine-tuned BART LoRA\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090398a1",
   "metadata": {
    "id": "090398a1"
   },
   "source": [
    "- post_merge_cleanup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99633ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile post_merge_cleanup.py\n",
    "from transformers import AutoConfig, GenerationConfig\n",
    "\n",
    "#MODEL_DIR = \"./ft_outputs/bart_merged_highlightsum\"\n",
    "MODEL_DIR = \"/content/llmed_certification_FineTuneFlow/ft_outputs/bart_merged_highlightsum\"\n",
    "\n",
    "print(f\"ðŸ” Loading model from: {MODEL_DIR}\")\n",
    "\n",
    "# Load model config\n",
    "config = AutoConfig.from_pretrained(MODEL_DIR)\n",
    "\n",
    "changed = False\n",
    "\n",
    "# Ensure BOS + decoder_start_token\n",
    "if config.bos_token_id is None:\n",
    "    config.bos_token_id = 0\n",
    "    changed = True\n",
    "\n",
    "if config.decoder_start_token_id is None:\n",
    "    config.decoder_start_token_id = config.bos_token_id\n",
    "    changed = True\n",
    "\n",
    "# Ensure forced BOS consistency\n",
    "if getattr(config, \"forced_bos_token_id\", None) != config.bos_token_id:\n",
    "    config.forced_bos_token_id = config.bos_token_id\n",
    "    changed = True\n",
    "\n",
    "# Save config if changed\n",
    "if changed:\n",
    "    config.save_pretrained(MODEL_DIR)\n",
    "    print(\"ðŸ’¾ Config updated and saved.\")\n",
    "else:\n",
    "    print(\"âœ” Config already correct â€” no change needed.\")\n",
    "\n",
    "# ---- Now build GenerationConfig ----\n",
    "gen_cfg = GenerationConfig.from_model_config(config)\n",
    "\n",
    "# Required default decoding fields\n",
    "gen_cfg.max_new_tokens = 192\n",
    "gen_cfg.num_beams = 4\n",
    "gen_cfg.length_penalty = 1.0\n",
    "\n",
    "# ðŸ”¥ Fix for the crash: set valid value for early_stopping\n",
    "if getattr(gen_cfg, \"early_stopping\", None) is None:\n",
    "    gen_cfg.early_stopping = False\n",
    "\n",
    "# Save GenerationConfig safely\n",
    "gen_cfg.save_pretrained(MODEL_DIR)\n",
    "print(\"ðŸ’¾ GenerationConfig saved successfully.\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Cleanup finished â€” model ready for inference and upload.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820bc8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python post_merge_cleanup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b164fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "#MODEL = \"./ft_outputs/bart_merged_highlightsum\"\n",
    "MODEL = \"/content/llmed_certification_FineTuneFlow/ft_outputs/bart_merged_highlightsum\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL)\n",
    "\n",
    "text = \"The Amazon rainforest is the largest rainforest in the world and is home to an incredible diversity of plant and animal life. It plays a crucial role in regulating the Earth's climate.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "summary = model.generate(**inputs, max_new_tokens=120, num_beams=4, early_stopping=True, length_penalty=1.0)\n",
    "print(tokenizer.decode(summary[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
