# LLM Engineering & Deployment (LLMED) Certification ‚Äì Capstone Project : Fine-Tuning & Optimization of BART for Dialogue Summarization (HighlightSum)    

This repository contains the full implementation for to the publication **LLMED Certification - Capstone Project : LLM Fine-Tuning & Optimization of Bart for Dialogue Summarization (HighlightSum)**, completed as part of the **LLM Engineering and Deployment Certification program** by [Ready Tensor](https://www.readytensor.ai) 

The objective is to **select**, **fine-tune**, **evaluate**, and **deploy** an efficient open-source model for **dialogue summarization**, using a subset of the 
[HighlightSum dataset](https://huggingface.co/datasets/knkarthick/highlightsum). The project includes:  
- model benchmarking and selection  
- LoRA/QLoRA fine-tuning  
- evaluation (ROUGE, BERTScore, BLEU)  
- merging 
- inference and deployment  
- Hugging Face publishing
- reproducible scripts & W&B experiment tracking

---

## Project Overview   

This project builds a complete, reproducible pipeline for fine-tuning open LLMs for conversational summarization:

- **Model benchmarking & selection**  
Comparison of five candidate models ( [BART-large](https://huggingface.co/facebook/bart-large-cnn), [T5-large](https://huggingface.co/google/flan-t5-large), [Phi-3-Mini](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct), [LLaMA-1B](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct), and [LLaMA-3B](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)).  
Evaluated on speed + efficiency + (memory?) + ROUGE to determine the best fine-tuning candidate.  

- **LoRA / QLoRA Fine-Tuning**
Efficient parameter-efficient training (PEFT) with LoRA adapters.  
Training uses **2,000 training samples + 200 validation samples** from HighlightSum.

- **Full Evaluation Pipeline**  
ROUGE-1 / ROUGE-2 / ROUGE-L  
BERTScore (semantic similarity)  
BLEU  (?)  
Failure category analysis  
Prediction exports (CSV + metrics)  

- **Merging + Final Model**  
Merge LoRA adapters into base BART ‚Üí produce a **single deployable checkpoint**.

- **Deployment**    
Inference script  
GGUF export (llama.cpp / LM Studio)  ?
Optional FastAPI / Gradio endpoint  ?

- **Experiment Tracking**  
All training & evaluation runs logged to [Weights & Biases (W&B)](https://wandb.ai/site).  

---  

## Repository Structure  

```text
üìÅ C:\Users\Michela\llmed_certification_FineTuneFlow
‚îÇ
‚îú‚îÄ‚îÄ train_bart_lora.py                 # LoRA training script (2k samples)
‚îú‚îÄ‚îÄ baseline_eval.py                   # Baseline BART evaluation (pre-training)
‚îú‚îÄ‚îÄ eval_bart_lora.py                  # Validation evaluation (ROUGE only)
‚îú‚îÄ‚îÄ eval_metrics_bart_lora.py          # Extended metrics: ROUGE + BERTScore + BLEU
‚îú‚îÄ‚îÄ inference_bart_lora.py             # Inference (LoRA or merged)
‚îú‚îÄ‚îÄ merge_bart_lora.py                 # Merge LoRA -> base BART
‚îÇ
‚îú‚îÄ‚îÄ ft_outputs/
‚îÇ   ‚îú‚îÄ‚îÄ bart_lora_highlightsum/        # LoRA adapter checkpoint
‚îÇ       ‚îú‚îÄ‚îÄ adapter_model.safetensor
‚îÇ       ‚îú‚îÄ‚îÄ adapter_config.json
‚îÇ       ‚îî‚îÄ‚îÄ tokenizer files
‚îÇ       ‚îî‚îÄ‚îÄ etc...
‚îÇ   ‚îú‚îÄ‚îÄ bart_merged_highlightsum/      # Merged model checkpoint
‚îÇ       ‚îú‚îÄ‚îÄ config.json
‚îÇ       ‚îú‚îÄ‚îÄ pytorch_model.bin
‚îÇ       ‚îú‚îÄ‚îÄ tokenizer.json
‚îÇ       ‚îú‚îÄ‚îÄ special_tokens_map.json
‚îÇ       ‚îî‚îÄ‚îÄ etc...
‚îÇ
‚îú‚îÄ‚îÄ metrics/                           # All metrics + prediction CSVs  
‚îÇ   ‚îú‚îÄ‚îÄ baseline_predictions.csv
‚îÇ   ‚îú‚îÄ‚îÄ baseline_predictions_metrics.csv
‚îÇ   ‚îú‚îÄ‚îÄ validation_predictions.csv
‚îÇ   ‚îú‚îÄ‚îÄ validation_predictions_metrics.csv
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ Notebook_C.ipynb               # Model benchmarking & selection
‚îÇ   ‚îú‚îÄ‚îÄ Notebook_D.ipynb               # Auto fine-tuning plan generator
‚îÇ   ‚îú‚îÄ‚îÄ Notebook_E.ipynb               # Evaluation dashboard (plots, charts) / (evaluation, optional)
‚îÇ   ‚îú‚îÄ‚îÄ Notebook_F.ipynb               # GGUF export + deployment options / ()
‚îÇ  
‚îÇ
‚îú‚îÄ‚îÄ outputs/
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/                          # Evaluation results / Generated by Notebook E  MISSING CHECK WHETHER THESE ARE ENCLOSED IN THE METRICS 
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rouge1.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rouge2.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rougel.png
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ benchmarks/
‚îÇ       ‚îú‚îÄ‚îÄ notebook_C/                      # Ranking results
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ final_ranking.csv
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ final_ranking.json
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ final_ranking.html
‚îÇ       ‚îú‚îÄ‚îÄ notebook_D/                      # Fine-tuning plans, scripts
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ finetune_plan.md
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ qLoRa_train.sh  MISSING
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ train_lora_BART-large_20251202_123700.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ train_q_lora_LLaMA-1B_20251202_123700.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ recommendations.json
‚îÇ       ‚îÇ
‚îú‚îÄ‚îÄ requirements.txt                   # Project dependencies
‚îî‚îÄ‚îÄ .env_example.txt                   # Example environment file for API keys
‚îî‚îÄ‚îÄ README.md                          # This file (HF Model Card compatible)
‚îú‚îÄ‚îÄ Model_Card.md                      # Full model card for HF Hub
```
---

## Workflow Overview

**Stage 1 ‚Äî Dataset Preparation**    
- Load HighlightSum from HuggingFace  
- Select subset (2,000 train / 200 validation)  
- Inspect text length, sample quality  
- No additional preprocessing required

**Stage 2 ‚Äî Baseline Evaluation (Before Training)**  
Run:  
```
python baseline_eval.py
```

Outputs:
- baseline_predictions.csv
- ROUGE baseline
- BERTScore baseline
- BLEU baseline
Used to verify fine-tuning improvement.

**Stage 3 ‚Äî Fine-Tuning with LoRA**  
Run:  
```
python train_bart_lora.py
```
Training Features:  
- LoRA (r=8, Œ±=32, dropout=0.05)  
- Effective batch size = 8 (4 √ó 2 accumulation)  
- fp16 training on T4 GPU  
- W&B tracking enabled  

Output folder:
```
ft_outputs/bart_lora_highlightsum/
```

**Stage 4 ‚Äî Evaluation of LoRA Model**  
ROUGE-level scoring:  
```
python eval_bart_lora.py
```
Extended metrics (ROUGE + BERTScore + BLEU):  
```
python eval_metrics_bart_lora.py
```
Outputs:  
```
metrics/validation_predictions.csv
metrics/validation_predictions_metrics.csv
```

**Stage 5 ‚Äî Merge LoRA ‚Üí Base BART**  
```
python merge_bart_lora.py
```
Produces:  
```
ft_outputs/bart_merged_highlightsum/
```
This is the **deployment-ready model**.  

**Stage 6 ‚Äî Deployment & Inference**  
Inference script:  
```
python inference_bart_lora.py
```
Supports:  
- LoRA adapter mode   
- Fully merged model mode

---

## Metrics Provided    
Each evaluation includes:  
| Metric                          | Purpose                                         |
| ------------------------------- | ----------------------------------------------- |
| **ROUGE-1 / ROUGE-2 / ROUGE-L** | Lexical overlap with human summaries            |
| **BERTScore (F1)**              | Meaning-based semantic similarity               |
| **BLEU**                        | N-gram precision similarity                                |
| **Avg. summary length**         | Output consistency check                        |
| **Full prediction CSV**         | For manual inspection                           |
| **Failure case examples**       | Over-/under-summarization, hallucinations, etc. |

>_Note_: validation_predictions.csv include | id | source_text | reference_summary | generated_summary | rougeL_score |

---

## Key Comparisons  
| Comparison                                | Purpose                                    |    Question                          |
| ----------------------------------------- | -------------------------------------------|--------------------------------------|
| **Baseline BART vs LoRA-fine-tuned BART** | Measure fine-tuning impact                 | Is LoRA training effective?          |
| **LoRA vs Merged model**                  | Check whether merge preserved weights      | Is merging lossless?                 |
| **Merged model inference**                | Final deployment quality                   |  Is the final model production-ready?|

> This follows the certification requirement: Fine-tune ‚Üí evaluate ‚Üí merge ‚Üí cleanup ‚Üí evaluate ‚Üí deploy

---  

## Deployment Options    
- Hugging Face Hub : Includes full model card + checkpoint upload script.

---   

## Batch inference deployment  
for large-scale summarization workflows.

--- 

## Weights & Biases Integration
All scripts support:  
- training loss curves  
- evaluation metrics  
- full prediction tables  
- sample predictions  
- hyperparameter logging  

--- 

## Model Card (Hugging Face Summary)  > see also Model_Card.md   
**Model**: bart-large-cnn fine-tuned with LoRA on HighlightSum  
**Task**: Dialogue Summarization  
**Training Samples**: 2,000  
**Validation Samples**: 200  
**Method**: LoRA (r=8, Œ±=32)  
**Metrics**: ROUGE, BERTScore, BLEU  
**Intended Use**: Conversational summarization (multi-turn dialog)  
**Limitations**: May miss fine-grained details, not domain-specialized  
**License**: MIT (inherits from BART)  

---

## LLMED Fine-Tuning & Deployment Workflow  
The complete and up-to-date pipeline / workflow (end-to-end) including training ‚Üí evaluation ‚Üí merging ‚Üí deployment ‚Üí export (production)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. DATASET PREPARATION ‚Äî HighlightSum                                       ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   ‚Ä¢ Load raw dialogues + highlights                                         ‚îÇ
‚îÇ   ‚Ä¢ Tokenize (max_input=768, max_target=192)                                ‚îÇ
‚îÇ   ‚Ä¢ Create HuggingFace DatasetDict (train/val)                              ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Output: ./data/highlightsum_dataset (HF format)                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
                                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. MODEL BENCHMARKING (Notebook C)                                          ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   ‚Ä¢ Compare: BART-large, T5-large, Phi-3-Mini, LLaMA-1B, LLaMA-3B           ‚îÇ
‚îÇ   ‚Ä¢ Compute: ROUGE-1/2/L ¬∑ Throughput ¬∑ Efficiency Score                    ‚îÇ
‚îÇ   ‚Ä¢ Produces: final_ranking.csv                                             ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Output Dir: ./outputs/benchmarks/notebook_C                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
                                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. AUTO FINE-TUNING PLAN (Notebook D)                                       ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   ‚Ä¢ Reads: final_ranking.csv                                                ‚îÇ
‚îÇ   ‚Ä¢ Generates fine-tuning recommendations per model                          ‚îÇ
‚îÇ   ‚Ä¢ Exports:                                                                ‚îÇ
‚îÇ       ‚Äì finetune_plan.md                                                    ‚îÇ
‚îÇ       ‚Äì recommendations.json                                                ‚îÇ
‚îÇ       ‚Äì train_qLoRA.py (template)                                           ‚îÇ
‚îÇ       ‚Äì qLoRA_train.sh (Accelerate launcher)                                ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Output Dir: ./outputs/benchmarks/notebook_D                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
                                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. LoRA FINE-TUNING (Optimized for T4 GPU)                                  ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Command:                                                                  ‚îÇ
‚îÇ     python train_bart_lora.py                                               ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   ‚Ä¢ QLoRA (rank=8)                                                          ‚îÇ
‚îÇ   ‚Ä¢ fp16 training for speed                                                 ‚îÇ
‚îÇ   ‚Ä¢ Gradient accumulation ‚Üí effective batch=8                               ‚îÇ
‚îÇ   ‚Ä¢ W&B logging enabled                                                     ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Output: ./ft_outputs/bart_lora_highlightsum                               ‚îÇ
‚îÇ            (LoRA adapter weights, config, tokenizer, logs)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
                                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 5. EVALUATION (Pre-Merge)                                                   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Command:                                                                  ‚îÇ
‚îÇ     python eval_bart_lora.py                                                ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Metrics:                                                                  ‚îÇ
‚îÇ     ‚Äì ROUGE-1 / ROUGE-2 / ROUGE-L                                           ‚îÇ
‚îÇ     ‚Äì BERTScore                                                             ‚îÇ
‚îÇ     ‚Äì BLEU                                                                  ‚îÇ
‚îÇ   Files: validation_predictions.csv                                         ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Output Dir: ./metrics/lora_eval.json                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
                                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 6. MERGE LoRA ‚Üí BASE MODEL                                                  ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Command:                                                                  ‚îÇ
‚îÇ     python merge_bart_lora.py                                               ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   ‚Ä¢ Load base model in fp16                                                 ‚îÇ
‚îÇ   ‚Ä¢ Apply LoRA adapters                                                     ‚îÇ
‚îÇ   ‚Ä¢ Save full merged checkpoint                                             ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Output: ./ft_outputs/bart_merged_highlightsum                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
                                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 7. POST-MERGE CLEANUP   (TO BE EXCLUDED?)                                                    ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Command:                                                                  ‚îÇ
‚îÇ     python post_merge_cleanup.py                                            ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Fixes:                                                                    ‚îÇ
‚îÇ     ‚Äì forced_bos_token_id                                                   ‚îÇ
‚îÇ     ‚Äì decoder_start_token_id                                                ‚îÇ
‚îÇ     ‚Äì early_stopping flag                                                   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Output: ./ft_outputs/bart_merged_clean                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
                                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 8. FINAL EVALUATION (Merged Model)                                          ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Command:                                                                  ‚îÇ
‚îÇ     python eval_bart_lora.py --model=merged_clean                           ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Output: ./metrics/merged_eval.json                                        ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Comparison:                                                               ‚îÇ
‚îÇ     lora_eval.json   vs   merged_eval.json                                  ‚îÇ
‚îÇ     ‚Üí verifies merging preserves ROUGE, BERTScore, BLEU                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
                                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 9. DEPLOYMENT (Inference + Production)                                      ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Command: python inference_bart_lora.py                                    ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Deployment options:                                                       ‚îÇ
‚îÇ     ‚Äì FastAPI inference server                                              ‚îÇ
‚îÇ     ‚Äì Gradio Web UI                                                         ‚îÇ
‚îÇ     ‚Äì Hugging Face Space  (TO BE SELECTED THIS                              ‚îÇ
‚îÇ                                                                             ‚îÇ                                                                            ‚îÇ
‚îÇ   Output: production-ready model + API                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```
---

## Getting Started  
This section shows how to install dependencies, configure authentication, and run the full pipeline.  

### Prerequisites  
_Required_:
- Python 3.10+    
- [HuggingFace Account & API Key](https://huggingface.co/)
- GPU environment (Colab T4, L4, A100, or local RTX)
    
_Recommended_:   
- [Weights & Biases Account](https://wandb.ai/site) (for experiment tracking‚Äîoptional, but recommended)

_Set relevant API keys in your environment_:  
```bash
export HF_API_KEY=your_huggingface_key
export WANDB_API_KEY=your_wandb_key
```
### Authentication for Notebooks (Colab/Jupyter)
**Hugging Face Login**
```python
from huggingface_hub import notebook_login
notebook_login()
```
**Weights & Biases Login**  
```
pip install wandb
wandb login
```

### Installation

```bash
git clone https://github.com/micag2025/llmed_certification_FineTuneFlow.git
cd llmed_certification_FineTuneFlow
pip install -r requirements.txt
```
---

## Running the Pipeline

**1 Dataset Selection and Preparation**  
- Dataset: Highlightsum dataset 
- Sample used:
  2,000 for training  
  200 for validation  
- Preprocessing Includes:  
  - Tokenization (BART tokenizer)
  - Input truncation: 768 tokens
  - Target truncation: 192 tokens
  - Padding to longest sequence
  - Text formatting for summarization tasks
      
This project focuses on model evaluation + fine-tuning ‚Äì not data cleaning.  

**2 Model Benchmarking (Notebook C)**  
Notebook `notebook_C.ipynb` evaluates: 
- BART-large  
- T5-large  
- Phi-3-Mini  
- LLaMA-1B  
- LLaMA-3B  
**Metrics computed**  
- ROUGE-1 / ROUGE-2 / ROUGE-L  
- Throughput (tokens/s)  
- Latency per sample  
- Composite efficiency score  
**Tokenizer safety rules**    
- Set pad_token = eos_token when needed  
- Truncate to max_length = 768  
- Use padding="longest" for seq2seq  
- Use padding="max_length" for causal models  
- Batch inputs to reduce GPU RAM usage  

**Outputs**
```
model_benchmarks/notebook_C/final_ranking.csv
model_benchmarks/notebook_C/final_ranking.json
model_benchmarks/notebook_C/final_ranking.html
```

**3 Auto Fine-Tuning Recommendation (Notebook D)**    
Notebook `notebook_D.ipynb` generates:    
- _finetune_plan.md_ ‚Äî human-readable training plan
- _recommendations.json_ ‚Äî hyperparameter recommendations
- _train_qLoRA.py_ ‚Äî template script
- _qLoRA_train.sh_ ‚Äî Accelerate launcher

You are expected to:  
- Set model name  
- Set dataset  
- Tune hyperparameters  
- Choose LoRA or QLoRA  
- Set output path  
- Enable W&B logging  
This project selects **BART-large** for fine-tuning.

**4 Fine-Tuning (`train_bart_lora.py`)**

Run LoRA fine-tuning:
```
python train_bart_lora.py
```
**Features**:
- Optimized for T4 GPUs  
- Effective batch size = 8  
- LoRA r=8, Œ±=32  
- fp16 training  
- use_cache=False for stability  
- W&B logging built-in

**Output**:  
```
ft_outputs/bart_lora_highlightsum/
```

**5 Evaluation (Before Merge)**  
Basic evaluation:

```
python eval_bart_lora.py
```
Full Metrics:  
```
python eval_metrics_bart_lora.py
```
Outputs:
```
metrics/validation_predictions.csv
metrics/validation_predictions_metrics.csv
```
Metrics include:
ROUGE-1 / ROUGE-2 / ROUGE-L
BERTScore (semantic similarity)
BLEU score
Per-sample predictions

**6 Merge LoRA ‚Üí Base Model**

Merge adapters:
```
python merge_bart_lora.py
```

What the script does:  
- Loads base model in FP16  
- Loads LoRA adapters  
- Applies LoRA weights  
- Writes a standalone merged checkpoint  

Output:  
```
ft_outputs/bart_merged_highlightsum/
```

**7 Final Evaluation (Merged Model)**  
```
python eval_bart_lora.py --model=merged
```
Or full metrics:  
```
python eval_metrics_bart_lora.py
```
Used to confirm that merging preserves LoRA improvements.  


**8 Inference & Deployment**  
Run inference locally:  
```
python inference_bart_lora.py  
```
Deployment options supported:
- Hugging Face Hub
- Gradio Web UI
- Fast API server
- GGUF export (via Notebook F)
- LM Studio / Ollama

## Evaluation Overview

Each evaluation includes:  
| Metric          | Purpose                        |
| --------------- | ------------------------------ |
| ROUGE-1/2/L     | Overlap with reference summary |
| BLEU            | n-gram precision               |
| BERTScore F1    | Semantic similarity            |
| Avg Length      | Output stability               |
| Failure Screens | Identify weak spots            |

**Files produced:**  
```
validation_predictions.csv
validation_predictions_metrics.csv
```

**Model Comparison**
You evaluate:
- Base BART (optional baseline)  
- LoRA-adapted BART  
- Merged FP16 BART  

This verifies:
- Did LoRA training improve performance?
- Does merging preserve performance?
- Is the model ready for deployment?


## Usage Examples  

### Inspection Dataset

```
 üìä Dataset Overview:
  Train splits: 27,401 samples
  Val splits: 1,360 samples
  Test splits: 2,347 samples`

üîë Keys: ['id', 'dialogue', 'summary']

üìò First training example:

üî∏ DIALOGUE (32390 chars):
Speaker A: Cool. Do you wanna give me the little cable thing? Yeah. Cool. Ah, that's why it won't meet. Okay, cool. Yep, cool. Okay, functional requirements. Alright, yeah. It's working. Cool, okay. So what I have, wh where I've got my information from is a survey where the usability lab um observed...

üîπ SUMMARY (1299 chars):
The project manager opens the meeting by stating that they will address functional design and then going over the agenda. The industrial designer gives his presentation, explaining how remote controls function and giving personal preference to a clear, simple design that upgrades the technology as well as incorporates the latest features in chip design. The interface specialist gives her presentation next, addressing the main purpose of a remote control. She pinpoints the main functions of on/off, channel-switching, numbers for choosing particular channels, and volume; and also suggests adding a menu button to change settings such as brightness on the screen. She gives preference to a remote that is small, easy to use, and follows some conventions. The group briefly discusses the possibility of using an LCD screen if cost allows it, since it is fancy and fashionable. The marketing expert presents, giving statistical information from a survey of 100 subjects. She prefers a remote that is sleek, stylish, sophisticated, cool, beautiful, functional, solar-powered, has long battery life, and has a locator. They discuss the target group, deciding it should be 15-35 year olds. After they talk about features they might include, the project manager closes the meeting by allocating tasks.  
```


### Benchmark Results

| model       | model_id                                       |    rouge1 |    rouge2 |    rougeL |        time | throughput |  efficiency | composite_score |
|-------------|------------------------------------------------|----------:|----------:|----------:|------------:|------------:|------------:|-----------------:|
| BART-large  | facebook/bart-large-cnn                        |  28.106   |   9.183   |  21.063   |   101.632   |    1.968    |    0.207    |      1.231      |
| LLaMA-1B    | meta-llama/Llama-3.2-1B-Instruct               |  28.636   |   9.618   |  21.205   |   393.929   |    0.508    |    0.054    |      0.463      |
| LLaMA-3B    | meta-llama/Llama-3.2-3B-Instruct               |  23.772   |   8.223   |  17.306   |   748.223   |    0.267    |    0.023    |     -0.162      |
| Phi-3-Mini  | microsoft/Phi-3-mini-4k-instruct               |  20.550   |   7.028   |  14.307   |   987.636   |    0.203    |    0.014    |     -0.572      |
| T5-large    | t5-large                                       |  10.977   |   1.944   |   9.637   |   263.028   |    0.760    |    0.037    |     -0.960      |

> _Notes_:  
- Accuracy: ROUGE-L is used as the primary accuracy metric.
- Latency: Time refers to the average inference time per sample.
- Throughput:samples/sec = speed=total time
- Efficiency: Defined as ROUGE-L divided by inference time = ROUGE/time  
- Composite score: Normalized metric combining accuracy and efficiency to support model selection.

The Ranking Table provides a full benchmarking and model-selection pipeline. Thus, this identifies (recommends) automatically the best model to fine-tune based on balanced performance rather than size alone. To sum up, the highest composite_score wins.  When selecting models for dialogue summarization, balancing prediction quality with inference efficiency is crucial ‚Äî especially in practical or real-time settings.  
_Key takeaways_  
  - Composite score reflects both accuracy and speed, giving a more holistic evaluation than ROUGE alone.    
  - Models like BART-large outperform others because they achieve solid accuracy and fast inference.    
  - Larger causal models (e.g., LLaMA-3B, Phi-3-Mini) may achieve acceptable ROUGE scores, but their high latency significantly reduces their overall ranking.   

This shows the importance of balancing accuracy with inference speed when benchmarking large models for dialogue summarization.  

---

## Auto-fine-tuning Recommendation & Plan Results  

Notebook D generates `recommendations.json`, which contains per-model fine-tuning strategies and hyperparameters. Based on this analysis, BART-large and LLaMA-1B emerged as the top two candidates.  

Recommendation Output
```json
{
  "BART-large": {
    "size_hint": "0.4B",
    "method": "LoRA (PEFT) \u2014 encoder\u2013decoder friendly",
    "recommended_hyperparams": {
      "epochs": 3,
      "micro_batch_size": 8,
      "lr": 0.0002
    }
  },
  "LLaMA-1B": {
    "size_hint": "1B",
    "method": "LoRA or full fine-tune",
    "recommended_hyperparams": {
      "epochs": 3,
      "micro_batch_size": 8,
      "lr": 0.0002
    }
  }
```
Interpretation & Comparison
| Model          | Size | Recommended PEFT Method                    | Suggested Hypers                   | Meaning                                                   |
| -------------- | ---- | ------------------------------------------ | ---------------------------------- | --------------------------------------------------------- |
| **BART-large** | 0.4B | **LoRA (PEFT) ‚Äî encoder‚Äìdecoder friendly** | epochs: 3, batch size: 8, LR: 2e-4 | **Best match for abstractive summarisation + efficiency** |
| **LLaMA-1B**   | 1B   | LoRA **or full fine-tune**                 | epochs: 3, batch size: 8, LR: 2e-4 | Strong, but slower + worse summarisation on highlightSUM  |

**BART-large** was selected and it is the preferred choice because:  
- Highest composite score from Notebook D rankings  
- Optimized architecture for encoder-decoder summarization tasks (HighlightSUM)  
- Native LoRA support on attention projections (q_proj/v_proj) without special patching  
- Efficient training & inference on Colab T4 GPU  

While both models received identical hyperparameters, BART-large offers superior performance due to its architecture fit, ROUGE scores, and latency characteristics.  


**Customize `train_qLoRA.py`**  

Next steps:  
- customize the `train_qLoRA.py` to the chosen model (map tokenizers/prompt style precisely). 
- add validation loop + ROUGE evaluation inside training to checkpoint best model.
- produce a small sample dataset JSONL generator from SAMSum that matches the expected supervised format.
- estimate training time more accurately based on the GPU type (T4 / L4 / A100) and hours you can run.  

Example of `customization of train_qLoRA.py`  

 ```bash
  # -------------------------
# Config
# -------------------------
MODEL_NAME = "facebook/bart-large-cnn"
OUTPUT_DIR = "/content/llmed_certification_FineTuneFlow/ft_outputs/bart_lora_highlightsum"

TRAIN_SAMPLES = 2000
VAL_SAMPLES = 200

EPOCHS = 3
MICRO_BATCH_SIZE = 4
GRAD_ACC = 2   # effective batch size = 8
LEARNING_RATE = 2e-4
MAX_INPUT_LENGTH = 768
MAX_TARGET_LENGTH = 192
WANDB_PROJECT = "highlightsum_bart_lora"
os.makedirs(OUTPUT_DIR, exist_ok=True)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"üî• Using device: {device}")
```    


### Examples output train.py 

-**Model Loaded in 4-bit Successfully**
![Model Loaded in 4-bit Successfully](Screenshot_3-12-2025_163751_colab.research.google.com.jpeg)


- **QLoRA Training Started**
```bash
 üöÄ Starting QLoRA training‚Ä¶
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
```
> _Note_: This is normal. It actually reduces memory usage, which is good.

- **Completion**  
```bash  
üíæ Saving LoRA adapters‚Ä¶  
üéâ Training completed. Saved to bart_lora_highlightsum
```    
 
###  Experiment Tracking with Weights & Biases (W&B)  
Training runs are instrumented and tracked using [Weights & Biases](https://wandb.ai/). This integration enables:
- Visualization of training loss and evaluation curves
- Learning rate schedule and gradient monitoring
- GPU memory usage tracking during training
- Evaluation metrics logging after each epoch
- Model artifact saving and versioning

All W&B integration is handled in the training script [`run_llama_qlora.py`](run_llama_qlora.py) with minimal and safe changes:
- Added `wandb.init(...)` for project setup
- Configured `report_to="wandb"` and custom `run_name`
- Enabled configuration tracking for reproducibility

#### Example Visualizations

![Training Loss Curve](graphs_W&B/Training_Loss_Curves.jpeg)

![Evaluation Metrics](graphs_W&B/evaluation_metrics.jpeg)   

![GPU Utilization](graphs_W&B/GPU_utilisation.jpeg)   

> To access interactive dashboards and full experiment details, [visit our Weights & Biases project](https://wandb.ai/agostinimichelait-ready-tensor/llama-qlora-samsum).

**Usage Note**: All tracking features are enabled only during training within the notebook/script.

---

**Want to view your training run?**
Once W&B is installed and you are logged in, your training run will automatically appear at:
```
https://wandb.ai/<your-team-or-user>/llama-qlora-samsum
```
- Loss curves: automatically logged  
- Eval metrics: automatically logged  
- Model artifacts: saved & versioned  
- GPU utilization: tracked
  
## Results & Benchmarks  
Baseline (BART-large CNN)  TABLE IN PROGRESS -THE SCORES ARE IN THE OUTPUTS 

| Metric | Score |
|---|---:|
| ROUGE-1 | X.XXX |
| ROUGE-2 | X.XXX |
| ROUGE-L | X.XXX |
| BERTScore (F1) | X.XXX |
| BLEU | X.XXX |

Fine-tuned LoRA Model

| Metric | Score |
|---|---:|
| ROUGE-1 | X.XXX |
| ROUGE-2 | X.XXX |
| ROUGE-L | X.XXX |
| BERTScore (F1) | X.XXX |
| BLEU | X.XXX |


### Summarize new dialogues with your fine-tuned model  IN PROGREESS OPTIONAL
### Reproduce benchmarking with your own subset          IN PROGRESS OPTIONAL
### Swap in other dialogue datasets with minor tweaks     IN PROGRESS OPTIONAL 

---

## Technologies Used

- [Hugging Face Transformers](https://huggingface.co/)
- [PEFT (LoRA/QLoRA)](https://github.com/huggingface/peft)
- [Weights & Biases](https://wandb.ai/site)
- [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) (quantization)
- [GGUF/Llama.cpp compatible conversion](https://github.com/ggerganov/llama.cpp)
- Python, Colab/Jupyter

---

## Limitations & Recommendations

- _1. Dataset Coverage & Domain Shift_  
  _Limitation_: The HighlightSum dataset mainly contains structured, turn-based dialogues.  
  _Recommendation_: Apply domain-specific fine-tuning if your data differs significantly. Consider adding 5‚Äì10% in-domain samples for mixed fine-tuning.  
- _2. Limited Long-Context Support (BART limitation)_  
  _Limitation_: BART-large has a hard max input length of 1024 tokens. The training uses max_length=768 for speed on T4.  
  _Recommendation_: Pre-chunk long transcripts into semantic segments (speaker turns, topic segments). Summarize each chunk ‚Üí then summarize summaries (‚Äúrecursive summarization‚Äù). If long-context is critical, consider exporting the
    pipeline to a LLaMA 3.1/3.2 8B model with LoRA.  
- _3 PEFT/LoRA Adapters Cannot Fix All Model Weaknesses_  
  _Limitations_: LoRA fine-tuning only trains a small set of low-rank adapter weights.  
   _Recommendation_: If summaries must follow a strict structure, consider adding: instruction-prefix templates, or task-specific prompting or control tokens (e.g., length control, style control)  
- _4 T4 GPU Compute Constraints_  
  _Limitation_: T4 has limited VRAM (16 GB), relatively low tensor-core throughput, slow 4-bit dequantization, slow 4-bit dequantization  
  _Recommendation_: Keep max_length ‚â§ 768 for training,  Use gradient accumulation rather than larger batch sizes. Prefer LoRA r=8 for stability. For heavy workloads, consider: A100 or L4  
- _5 Summaries May Become Too ‚ÄúGeneric‚Äù_   
  _Limitations_: BART tends to: compress aggressively omit nuance produce safe but generic summaries. Especially if dialogue contains ambiguous or multi-topic content.  
  _Recommendation_: Add style-conditioned training samples ("bullet points", "2-line summary").    

---
 
## License

This project is licensed under the MIT License. See the [LICENSE](https://github.com/micag2025/llmed_Certification_Project1_FineTuneFlow/blob/97ed39ce6ae05e2b0546450448328841ef67816f/LICENSE) file for details.

---

## Contact

If you encounter bugs, have questions, or want to request a new feature, please [open an issue](https://github.com/micag2025/llmed_Certification_Project1_FineTuneFlow/issues) on this repository.   






