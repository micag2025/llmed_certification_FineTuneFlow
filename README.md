# LLM Engineering and Deployment (LLMED) Certification:Capstone Project: LLM Fine-Tuning & Optimization for Dialogue Summarization (HighlightSum)

This repository is part of capstone project for the **LLM Engineering and Deployment Certification program** by [Ready Tensor](https://www.readytensor.ai) and it is linked to the publication:**LLMED Certification:Capstone Project:LLM Fine-Tuning & Optimization for Dialogue Summarization (HighlightSum)** available on [Ready Tensor](https://www.readytensor.ai). This project builds a complete evaluation, selection, and fine-tuning pipeline for small-to-medium open-source language models. The objective is to identify the most efficient model for dialogue summarization, then fine-tune it using QLoRA and optimize it for real-world deployment. This was achieved using a subset of the HighlightSum dataset. This capstone project focuses on fine-tuning and benchmarking large language models for efficient, high-quality conversational summarization.

---

## Project Overview (Description)

This project develops a scalable, efficient workflow for selecting, fine-tuning, and evaluating open-source LLMs (e.g. BART, T5, LLaMA-1B, LLaMA-3B, Phi-3-Mini) for the task of dialogue summarization, using a subset of the benchmark [HighlightSum dataset](https://huggingface.co/datasets/knkarthick/highlightsum) as a test dataset. The codebase automates model selection via benchmarking, applies QLoRA for parameter-efficient fine-tuning, and outputs deployable artifacts.

---

## Workflow & Stages for BART-LoRA Fine-Tuning  

The complete and up-to-date pipeline / workflow (end-to-end) including training â†’ evaluation â†’ merging â†’ deployment â†’ export (production)

```text  

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Inspection and Prepare Dataset  HighlightSum                               â”‚
â”‚  â”€ Raw documents                                    â”‚
â”‚  â”€ Highlights / summaries                           â”‚
â”‚  â†’ Format into HuggingFace dataset (train/val)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Fine-Tune Base BART with LoRA (PEFT)             â”‚
â”‚  python train_bart_lora.py                          â”‚
â”‚  Output: ./ft_outputs/bart_lora_highlightsum                    â”‚
â”‚   (LoRA adapter weights + training logs)            â”‚(PEFT checkpoints + base model refs only)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Evaluate LoRA Model (Validation)                 â”‚
â”‚  python eval_bart_lora.py                           â”‚
â”‚  Output: ./metrics/lora_eval.json ?                   â”‚
â”‚    - ROUGE-1 / ROUGE-2 / ROUGE-L                    â”‚
â”‚    - BERTScore, BLEU                                â”‚
â”‚    - validation_predictions.csv                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Merge LoRA into Base BART                        â”‚
â”‚  python merge_bart_lora.py                               â”‚
â”‚  Output: ./ft_outputs/bart_merged_highlighsum                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Post-Merge Cleanup (Fix Config)    ???              â”‚
â”‚  python post_merge_cleanup.py                       â”‚
â”‚  Fixes:                                             â”‚
â”‚   - forced_bos_token_id                             â”‚
â”‚   - decoder_start_token_id                          â”‚
â”‚   - early_stopping flag                             â”‚
â”‚  Output: ./ft_outputs/bart_merged_clean             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Evaluate Final Merged Model      TO BE ENCLOSED                â”‚
â”‚  python eval_bart_lora.py --model=merged_clean      â”‚
â”‚  Output: ./metrics/merged_eval.json                 â”‚
â”‚                                                      â”‚
â”‚  ğŸ”½ Comparison (automatic in notebook)               â”‚
â”‚    lora_eval.json       vs       merged_eval.json    â”‚
â”‚    â†’ Does merging preserve or improve ROUGE?         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. Inference / Deployment                           â”‚
â”‚  python test_inference.py                           â”‚
â”‚  or deploy using:                                    â”‚
â”‚   - FastAPI Endpoint   (?)                            â”‚
â”‚   - Gradio Web UI      (?)                             â”‚
â”‚   - Hugging Face Space                              â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  6. POST-MERGE USAGE  (deployment stage)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                   ./ft_outputs/bart_merged_clean
                                â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                     â–¼                     â–¼
   inference.py          evaluate.py           Notebook-F (GGUF export)
(Real use / API)   (ROUGE + BERTScore + BLEU     for llama.cpp /
                       + charts dashboard)        LM Studio / Ollama
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  7. PRODUCTION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 Option A â€” Hugging Face pipeline
 Option B â€” FastAPI / Flask service
 Option C â€” GGUF quantized using llama.cpp/LM Studio
 Option D â€” Batch inference at scale
```
```
<svg width="1080" height="2100" viewBox="0 0 1080 2100" xmlns="http://www.w3.org/2000/svg">
  <style>
    .box { fill:#fdfdfd; stroke:#222; stroke-width:2; rx:12; ry:12; }
    .title { font: bold 26px sans-serif; }
    .text { font: 20px sans-serif; }
    .arrow { stroke:#222; stroke-width:2; marker-end:url(#arrowhead); }
  </style>

  <defs>
    <marker id="arrowhead" markerWidth="12" markerHeight="12" refX="6" refY="3" orient="auto">
      <polygon points="0 0, 6 3, 0 6" fill="#222" />
    </marker>
  </defs>

  <!-- Title -->
  <text x="540" y="60" text-anchor="middle" class="title">LLMED Fine-Tuning & Deployment Workflow</text>

  <!-- Step 1: Dataset -->
  <rect x="200" y="100" width="680" height="150" class="box" />
  <text x="540" y="160" text-anchor="middle" class="title">1. Dataset Preparation</text>
  <text x="540" y="195" text-anchor="middle" class="text">HighlightSum (2000 train / 200 val)</text>
  <text x="540" y="225" text-anchor="middle" class="text">Tokenization Â· Truncation 768 / 192</text>

  <!-- Arrow 1 -->
  <line x1="540" y1="250" x2="540" y2="310" class="arrow" />

  <!-- Step 2: Benchmarking -->
  <rect x="200" y="310" width="680" height="180" class="box" />
  <text x="540" y="365" text-anchor="middle" class="title">2. Model Benchmarking (Notebook C)</text>
  <text x="540" y="400" text-anchor="middle" class="text">BART Â· T5 Â· Phi-3 Â· LLaMA-1B Â· LLaMA-3B</text>
  <text x="540" y="435" text-anchor="middle" class="text">ROUGE Â· Throughput Â· Efficiency Score</text>
  <text x="540" y="465" text-anchor="middle" class="text">Output: final_ranking.csv</text>

  <!-- Arrow 2 -->
  <line x1="540" y1="490" x2="540" y2="560" class="arrow" />

  <!-- Step 3: Auto Plan -->
  <rect x="200" y="560" width="680" height="190" class="box" />
  <text x="540" y="615" text-anchor="middle" class="title">3. Auto Fine-Tuning Plan (Notebook D)</text>
  <text x="540" y="655" text-anchor="middle" class="text">Reads final_ranking.csv</text>
  <text x="540" y="690" text-anchor="middle" class="text">Produces: finetune_plan.md Â· recommendations.json</text>
  <text x="540" y="725" text-anchor="middle" class="text">Generates: train_qLoRA.py Â· qLoRA_train.sh</text>

  <!-- Arrow 3 -->
  <line x1="540" y1="750" x2="540" y2="820" class="arrow" />

  <!-- Step 4: Training -->
  <rect x="200" y="820" width="680" height="180" class="box" />
  <text x="540" y="875" text-anchor="middle" class="title">4. LoRA Fine-Tuning</text>
  <text x="540" y="910" text-anchor="middle" class="text">train_bart_lora.py</text>
  <text x="540" y="945" text-anchor="middle" class="text">Optimized for T4 Â· LoRA r=8 Â· fp16</text>
  <text x="540" y="975" text-anchor="middle" class="text">Output: bart_lora_highlightsum</text>

  <!-- Arrow 4 -->
  <line x1="540" y1="1000" x2="540" y2="1070" class="arrow" />

  <!-- Step 5: Evaluation -->
  <rect x="200" y="1070" width="680" height="180" class="box" />
  <text x="540" y="1125" text-anchor="middle" class="title">5. Evaluation (Pre-Merge)</text>
  <text x="540" y="1160" text-anchor="middle" class="text">eval_bart_lora.py</text>
  <text x="540" y="1195" text-anchor="middle" class="text">ROUGE Â· BERTScore Â· BLEU</text>
  <text x="540" y="1225" text-anchor="middle" class="text">Output: validation_predictions.csv</text>

  <!-- Arrow 5 -->
  <line x1="540" y1="1250" x2="540" y2="1320" class="arrow" />

  <!-- Step 6: Merge -->
  <rect x="200" y="1320" width="680" height="180" class="box" />
  <text x="540" y="1375" text-anchor="middle" class="title">6. Merge LoRA â†’ Base Model</text>
  <text x="540" y="1410" text-anchor="middle" class="text">merge_bart_lora.py</text>
  <text x="540" y="1445" text-anchor="middle" class="text">Creates FP16 standalone model</text>
  <text x="540" y="1475" text-anchor="middle" class="text">Output: bart_merged_highlightsum</text>

  <!-- Arrow 6 -->
  <line x1="540" y1="1500" x2="540" y2="1570" class="arrow" />

  <!-- Step 7: Post-Merge Evaluation -->
  <rect x="200" y="1570" width="680" height="170" class="box" />
  <text x="540" y="1620" text-anchor="middle" class="title">7. Final Evaluation</text>
  <text x="540" y="1655" text-anchor="middle" class="text">eval_bart_lora.py --model=merged</text>
  <text x="540" y="1690" text-anchor="middle" class="text">Compare LoRA vs Merged</text>

  <!-- Arrow 7 -->
  <line x1="540" y1="1720" x2="540" y2="1790" class="arrow" />

  <!-- Step 8: Deployment -->
  <rect x="200" y="1790" width="680" height="190" class="box" />
  <text x="540" y="1845" text-anchor="middle" class="title">8. Deployment</text>
  <text x="540" y="1880" text-anchor="middle" class="text">Inference scripts Â· Notebook F</text>
  <text x="540" y="1915" text-anchor="middle" class="text">FastAPI Â· Gradio Â· HF Hub Â· GGUF export</text>
  <text x="540" y="1950" text-anchor="middle" class="text">Model ready for production</text>
</svg>
```












To evaluate and improve a modelâ€™s step-by-step summarisation capability using a subset of the [HighlightSum dataset](https://huggingface.co/datasets/knkarthick/highlightsum), the following **workflow**, divided into several stages, is employed:  
  
1. **Benchmarking & Model Selection**
   - Multiple models are compared (BART, T5, LLaMA, etc.) on ROUGE, speed, and efficiency.
   - Visual ranking and automatic recommendation of the best model to fine-tune.

2. **Fine-Tuning (QLoRA)**
   - Selected model is fine-tuned using QLoRA for memory efficiency (4-bit quantization).
   - Produces LoRA adapter weights.

3. **Evaluation**
   - Fine-tuned model is evaluated using ROUGE and other metrics on HighlightSum validation set.
   - Outputs include plots and summary tables.

4. **Deployment Prep**
   - Merge LoRA adapters with base model.
   - Convert to GGUF for fast CPU inference (llama.cpp compatibility).
   - Tracks all experiments with Weights & Biases.

---

## Features / Whatâ€™s Included

- Automated benchmarking and composite ranking of open LLMs
- QLoRA-based fine-tuning pipeline 
- Inference & evaluation scripts
- Artifacts for deployment (merged weights, GGUF exports)
- Experiment tracking (Weights & Biases)
- Example Colab/Notebook integration

---

## Repository Structure  TO BE UPDATED

```text
ğŸ“ C:\Users\Michela\llmed_Certification_Project1_FineTuneFlow     project/
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ train_bart_lora.py            # QLoRA training (2k or full dataset)  / Training script
â”œâ”€â”€ merge_bart_lora.py                 # Merge LoRA â†’ full FP16 model  / Merge adapters with base model
â”œâ”€â”€ inference_bart_lora.py                  # Generation with LoRA or merged model  / Summarization with fine-tuned model
â”œâ”€â”€ eval_bart_lora.py                   # ROUGE metrics + charts (CLI)   / Compute ROUGE, generate charts
â”œâ”€â”€ eval_metrics_bart_lora.py    
â”‚ 
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ Notebook_C.ipynb              # Benchmarking + model selection / Benchmarking & Selection
â”‚   â”œâ”€â”€ Notebook_D.ipynb              # Auto finetune plan recommendation / Fine-Tuning Recommendation
â”‚   â”œâ”€â”€ Notebook_E.ipynb              # Inference + evaluation + ROUGE   / Inference/Evaluation Pipeline  MISSING
â”‚   â”œâ”€â”€ Notebook_F.ipynb              # Production (FastAPI, GGUF export) / Productionization Guide       MISSING
â”‚   â”œâ”€â”€ Notebook_G.ipynb              # Stretch-goal / safety alignment  / (API/Deployment)               MISSING
â”‚
â”œâ”€â”€ models (ft_outputs)/
â”‚   â”œâ”€â”€ bart_lora_highlightsum/      # Training output (2k subset)  / OUTPUT_DIR for 2k-subset training
â”‚       â”œâ”€â”€ adapter_model.bin
â”‚       â”œâ”€â”€ adapter_config.json
â”‚       â””â”€â”€ tokenizer files
â”‚   â”œâ”€â”€ bart_merged_highlightsum/        # Full merged HF model   / MERGED_DIR after merge_lora.py
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ pytorch_model.bin
â”‚       â”œâ”€â”€ tokenizer.json
â”‚       â”œâ”€â”€ special_tokens_map.json
â”‚       â””â”€â”€ etc...
â”‚   â”œâ”€â”€ gguf/                         # Quantized GGUF exports (Notebook F) TO BE ENCLOSED
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ evaluation/                          # Evaluation results / Generated by Notebook E  MISSING
â”‚   â”‚   â”œâ”€â”€ metrics.json
â”‚   â”‚   â”œâ”€â”€ metrics.csv
â”‚   â”‚   â”œâ”€â”€ rouge1.png
â”‚   â”‚   â”œâ”€â”€ rouge2.png
â”‚   â”‚   â”œâ”€â”€ rougel.png
â”‚   â”‚
â”‚   â”œâ”€â”€ benchmarks/
â”‚       â”œâ”€â”€ notebook_C/                      # Ranking results, charts THIS HAS BEEN ENCLOSED
â”‚       â”‚   â”œâ”€â”€ final_ranking.csv
â”‚       â”‚   â”œâ”€â”€ final_ranking.json
â”‚       â”‚   â”œâ”€â”€ final_ranking.html
â”‚       â”œâ”€â”€ notebook_D/                      # Fine-tuning plans, scripts
â”‚       â”‚   â”œâ”€â”€ finetune_plan.md
â”‚       â”‚   â”œâ”€â”€ qLoRa_train.sh  MISSING
â”‚       â”‚   â”œâ”€â”€ train_lora_BART-large_20251202_123700.py
â”‚       â”‚   â”œâ”€â”€ train_q_lora_LLaMA-1B_20251202_123700.py
â”‚       â”‚   â”œâ”€â”€ recommendations.json
â”‚
â”œâ”€â”€ requirements.txt                          # Project dependencies
â””â”€â”€ .env_example.txt                         # Example environment file for API keys
```

```
llmed_certification_FineTuneFlow/
â”‚
â”œâ”€â”€ train_bart_lora.py                  # LoRA fine-tuning
â”œâ”€â”€ baseline_eval.py                    # Baseline evaluation (pre-training)
â”œâ”€â”€ eval_bart_lora.py                   # Post-training evaluation
â”œâ”€â”€ inference_bart_lora.py              # Inference w/ LoRA or merged model
â”œâ”€â”€ merge_bart_lora.py                  # Merge LoRA â†’ base model
â”‚
â”œâ”€â”€ metrics/                            # All metrics + prediction CSVs
â”‚   â”œâ”€â”€ baseline_predictions.csv
â”‚   â”œâ”€â”€ baseline_predictions_metrics.csv
â”‚   â”œâ”€â”€ validation_predictions.csv
â”‚   â”œâ”€â”€ validation_predictions_metrics.csv
â”‚
â”œâ”€â”€ ft_outputs/                         # Model outputs
â”‚   â”œâ”€â”€ bart_lora_highlightsum/         # LoRA adapter model
â”‚   â”œâ”€â”€ bart_merged_highlightsum/       # (optional) merged checkpoint
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md                           # Full model card for HF Hub
â”‚
â””â”€â”€ utils/ (optional)
    â”œâ”€â”€ dataset_utils.py
    â”œâ”€â”€ generation_utils.py
```

### What the evaluation step provides

Each evaluation run (`eval_bart_lora.py`) computes:
| Metric                      | Purpose                                   |
| --------------------------- | ----------------------------------------- |
| ROUGE-1 / ROUGE-2 / ROUGE-L | Measures overlap with reference summaries |
| BERTScore                   | Semantic similarity                       |
| BLEU                        | Precision-based n-gram similarity         |
| Avg. Length                 | Output stability check                    |
| Failure Buckets             | Templates for common failure cases        |

Plus the CSV:
validation_predictions.csv
| id | source_text | reference_summary | generated_summary | rougeL_score |


###  Final comparison across model versions
| Model version                    | When to compute               |
| -------------------------------- | ----------------------------- |
|  Base BART (optional)           | Before fine-tuning (baseline) |
|  BART + LoRA (during training) | After fine-tuning             |
|  BART merged_clean             | Final deployment model        |

This allows to answer three key questions:
| Question                             | Which comparison?                   |
| ------------------------------------ | ----------------------------------- |
| Is LoRA training effective?          | Base vs. BART-LoRA                  |
| Is merging lossless?                 | BART-LoRA vs. merged_clean          |
| Is the final model production-ready? | merged_clean eval score + inference |

>_Note_: LoRA fine-tuning â†’ evaluate â†’ merge LoRA into base â†’ cleanup â†’ evaluate again â†’ deploy.

---

## Getting Started

### Prerequisites

- Python 3.10+    
- [HuggingFace Account & API Key](https://huggingface.co/)
- [Weights & Biases Account](https://wandb.ai/site) (for experiment trackingâ€”optional, but recommended)

Set relevant API keys in your environment:

```bash
export HF_API_KEY=your_hf_key
export WANDB_API_KEY=your_wandb_key
```
### Hugging Face & Weights & Biases Authentication in Notebooks

If running in a Colab or Jupyter notebook, authenticate your session with Hugging Face Hub and Weights & Biases as follows:

```python
from huggingface_hub import notebook_login
notebook_login()
```

Install Weights & Biases (if not already installed) and log in:

```bash
pip install wandb
wandb login
```
---

### Installation

```bash
git clone https://github.com/micag2025/llmed_certification_FineTuneFlow.git
cd llmed_certification_FineTuneFlow
pip install -r requirements.txt
```
---

## Running the Pipeline

**Dataset Selection and Preparation**  
- Dataset: Highlightsum dataset on Hugging Face  
- Sample Size: 2,000 training samples, 200 validation samples.  
- Preprocessing:  
  - Tokenization with BART tokenizer.    
  - Input truncation (max length 768), target truncation (max length 192).    
  - Splitting into training and validation sets.  

Here the focus is on _flow of data into fine-tuning pipeline_ rather than dataset collection or cleaning.

**Model Benchmarking**

Run the benchmarking notebook (`notebook_C`) to compare multiple candidate models using accuracy and efficiency metrics. The evaluation `notebook C`, compare five candidate models ( [BART-large](https://huggingface.co/facebook/bart-large-cnn), [T5-large](https://huggingface.co/google/flan-t5-large), [Phi-3-Mini](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct), [LLaMA-1B](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct), and [LLaMA-3B](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)) on 200 validation samples of Highlightsum dataset using:    
- ROUGE-1 / ROUGE-2 / ROUGE-L scores    
- Execution time per sample    
- Tokens-per-second throughput    
- An overall efficiency score (accuracy vs speed)

Basic preprocessing has been performed, including `tokenization of dialogues` with appropriate padding and truncation, `batch preparation` for seq2seq models, and `selection of a subset from the HighlightSUM train split` for benchmarking. In details, the basic preprocessing performed is based on:  
- **Tokenization**:    
  - All text inputs (`dialogue`) are tokenized using the model-specific tokenizer.  
  - For causal models, if `pad_token` was missing, it was set to `eos_token` to allow batching/padding.  
  - Seq2seq and causal models both use truncation and padding (`max_length=768`) to ensure consistent tensor shapes.  
- **Dataset splitting**:  
  - Selected a subset of samples (`N_SAMPLES`) from the HighlightSUM train split for benchmarking.  
- **Batching (seq2seq models)**:
 - Inputs are batched to reduce memory usage on GPU, which is part of preprocessing before model inference.

`Notebook C` allows:  
- Benchmarks large models safely on Colab.    
- Performs basic preprocessing (tokenization, padding, truncation, batching).  
- Handles tokenizer safety (pad_token set if missing).  
- Includes train/validation/test split handling.  
- Clearly highlights all places where tokenizer safety or padding/truncation is applied.

_Key Preprocessing & Tokenizer Safety Highlights_  
1 **Tokenizer safety**:
```bash
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
```
- Applied for **all models**, including causal LMs.
2 **Batching & padding**:  
- Seq2seq models use `padding="longest"` and `max_length=768`.  
- Causal models use `padding="max_length"` and `max_length=768`.    
3 **Truncation**:  
- Ensures sequences donâ€™t exceed modelâ€™s max input length.  
4 **Dataset split**:  
- Train, validation, and test subsets selected (N_SAMPLES for test subset).  

`Notebook_C` generates `final_ranking.csv`that reflects the real performance trade-offs (ROUGE + throughput + efficiency) on HighlightSUM dataset.  

 **Auto-fine-tuning Recommendation & Plan** 

`Notebook D` (Auto-fine-tuning Recommendation & Plan) reads the final leaderboard from `model_benchmarks/notebook_C/final_ranking.csv` (inputs) and generates a comprehensive fine-tuning strategy available in the following outputs (`model_benchmarks/notebook_D/`):     
- `finetune_plan.md` â€” Human-readable fine-tuning plan with rationale and hyperparameters
- `recommendations.json` -	Structured recommendations per model (method + hyperparameters)  
- `train_qLoRA.py` â€” Training template using PEFT + QLoRA  (TO BE RENAMED?)
- `qLoRA_train.sh` â€” Bash wrapper to execute QLoRA training with Hugging Face Accelerate  (TO BE RENAMED?)

For each model, `Notebook D` produces:  
- Ranked recommendation â€” which model(s) to fine-tune
- Fine-tuning method â€” QLoRA / LoRA / full fine-tuning (based on model size & available GPU)
- Hyperparameters â€” recommended training settings
- Compute estimate â€” rough time/resource heuristic (informational only, not billing-accurate)  


Next steps:  
- customize the `train_qLoRA.py` to the chosen model (map tokenizers/prompt style precisely). 
- add validation loop + ROUGE evaluation inside training to checkpoint best model.
- produce a small sample dataset JSONL generator from HighlightSumthat matches the expected supervised format.
- estimate training time more accurately based on the GPU type (T4 / L4 / A100) and hours you can run.  


 `Customize train_qLoRA.py`
`Notebook D` provides a generic training script, but it needs to be adapted so you can:
1. Set your model
2. Set your dataset / Notebook D may include a placeholder dataset.
3. Set your training hyperparameters:    
    - batch size  
  	- gradient accumulation  
    - QLoRA R value  
    - learning rate  
    - warmup  
    - max steps / epochs  
    - max sequence length  
4. Set output directory 

For this project:  
> Use: Optimized BART-LoRA Training for T4 GPU  
       Uses effective batch size 8 via gradient accumulation  
       Includes padding, length updates, and stable T4 config    

QLoRA training script has been also  modifed for speed while keeping most of the QLoRA benefits and model quality.  Key changes:
  - Shorter context: MAX_LENGTH = 768 â€” biggest speed win.  
  - Smaller LoRA rank: r = 8 (was 16) â€” less computation, still effective.  
  - Smarter batching: smaller per-device batch + larger gradient_accumulation_steps to keep effective batch size.
  - Cap total steps: use max_steps to avoid unnecessary epochs (you can tune).
  - Fewer saves/eval/logging: reduce IO overhead.
  -  Keep gradient checkpointing and fp16 to reduce memory & speed tradeoff.  ??
  -  Faster preprocessing: use padding="longest" then collate, avoid padding="max_length" in map to reduce token workload.
  -  Minor other tweaks (num_workers for tokenizers, cudnn benchmark, use_cache=False).

The fully updated `train_bart_lora.py` has been also equipped with Weights & Biases (W&B)  

**Fine-tuning**

After having customized `train_qLoRA.py` as needed, the main training script has been called `train_bart_lora.py`, and then this has been used to launch training (QLoRA Training):

```bash
!python train_bart_lora.py
```

Training output is saved to:   
`models (ft_outputs)/bart_lora_highlightsum/`

This script:  (TO BE VERIFIED)
- Loads base model in FP16 (not 4-bit)  
- Loads LoRA adapters  
- Applies the LoRA weights  
- Merges them into the model  
- Saves a standalone checkpoint  

>_Note_ See Correct Flow Summary > training > evaluation> merge>inference 

**Merge and Evaluate** TO BE CHANGED 
The fully corrected, safe, and LLaMA-3.2 compatible merge_lora.py script has been used for merging:  
- the 4-bit base model, and
- the LoRA adapters
- into a single FP16 full model that you can use normally without PEFT.

To sum up, the  `merge_lora.py` script:  
- Loads base model in FP16 (not 4-bit)  
- Loads LoRA adapters  
- Applies the LoRA weights  
- Merges them into the model  
- Saves a standalone checkpoints

The LLaMA-3.2 compatible evaluation script (ROUGE on SAMSum validation set), `evaluate.py`:
- Loads either LoRA model or merged model  
- Computes ROUGE-1, ROUGE-2, ROUGE-L on the SAMSum validation set  
- Uses your chat template for inference  
- Runs on Colab GPU  
- Is optimized for speed (batch inference, no sampling, greedy decoding)      


```bash
!python merge_lora.py
!python evaluate.py
```

This produces a new folder:  
```bash
llama1b-samsum-merged/
    config.json
    generation_config.json
    model.safetensors
    tokenizer.json
    tokenizer.model
...

**Inference**

```bash
!python inference.py
```

Example:  
Input dialogue:
John: Are you joining the call?
Sarah: Yes, give me 2 minutes.
John: Sure, I'll wait.

Generated summary:
Sarah will join the call shortly.

Evaluation (ROUGE)  
```bash
python src/evaluate.py
```
Outputs go to:
`outputs/evaluation_results/`  
Including:
`metrics.json`  
`rouge1.png`, `rouge2.png`, `rougeL.png`  

---


Full Fine-Tuning Workflow 
for the BART-LoRA: training â†’ inference â†’ evaluation â†’ metrics â†’ merge (optional)
**STEP 1 â€” Train LoRA Model**

Runs:
train_bart_lora.py

âœ” Produces:
/content/.../ft_outputs/bart_lora_highlightsum

**STEP 2 â€” Evaluate on Validation Split (ROUGE only)**

Runs:
eval_bart_lora.py

âœ” Produces basic evaluation CSV:
validation_predictions.csv
(with columns: dialogue, human_summary, model_summary, rouge scores)

âš ï¸ This CSV is required for the Metrics step.

**STEP 3 â€” Full Metrics (ROUGE + BERTScore + BLEU)**

Runs:
eval_metrics_bart_lora
/metrics/validation_predictions.csv

âœ” Produces enriched CSV:
validation_predictions_metrics.csv
(contains rouge1, rouge2, rougeL, bert_f1, BLEU)

**STEP 4 â€” Optional: Merge LoRA â†’ Full Model**

Runs:
merge_bart_lora.py

âœ” Produces merged full model (no adapters):
bart_merged_highlightsum/

Only needed if:  
- you want to deploy without LoRA  
- or run inference outside PEFT context  

**STEP 5 â€” Inference Script (for new unseen data)**

Runs:
inference_bart_lora.py (the updated version)

âœ” Can load either:
LoRA model
OR merged model

## Usage Examples  

### Inspection Dataset

```
 ğŸ“Š Dataset Overview:
  Train splits: 27,401 samples
  Val splits: 1,360 samples
  Test splits: 2,347 samples

ğŸ”‘ Keys: ['id', 'dialogue', 'summary']

ğŸ“˜ First training example:

ğŸ”¸ DIALOGUE (32390 chars):
Speaker A: Cool. Do you wanna give me the little cable thing? Yeah. Cool. Ah, that's why it won't meet. Okay, cool. Yep, cool. Okay, functional requirements. Alright, yeah. It's working. Cool, okay. So what I have, wh where I've got my information from is a survey where the usability lab um observed...

ğŸ”¹ SUMMARY (1299 chars):
The project manager opens the meeting by stating that they will address functional design and then going over the agenda. The industrial designer gives his presentation, explaining how remote controls function and giving personal preference to a clear, simple design that upgrades the technology as well as incorporates the latest features in chip design. The interface specialist gives her presentation next, addressing the main purpose of a remote control. She pinpoints the main functions of on/off, channel-switching, numbers for choosing particular channels, and volume; and also suggests adding a menu button to change settings such as brightness on the screen. She gives preference to a remote that is small, easy to use, and follows some conventions. The group briefly discusses the possibility of using an LCD screen if cost allows it, since it is fancy and fashionable. The marketing expert presents, giving statistical information from a survey of 100 subjects. She prefers a remote that is sleek, stylish, sophisticated, cool, beautiful, functional, solar-powered, has long battery life, and has a locator. They discuss the target group, deciding it should be 15-35 year olds. After they talk about features they might include, the project manager closes the meeting by allocating tasks.  
```


### Benchmark Results

| model       | model_id                                       |    rouge1 |    rouge2 |    rougeL |        time | throughput |  efficiency | composite_score |
|-------------|------------------------------------------------|----------:|----------:|----------:|------------:|------------:|------------:|-----------------:|
| BART-large  | facebook/bart-large-cnn                        |  28.106   |   9.183   |  21.063   |   101.632   |    1.968    |    0.207    |      1.231      |
| LLaMA-1B    | meta-llama/Llama-3.2-1B-Instruct               |  28.636   |   9.618   |  21.205   |   393.929   |    0.508    |    0.054    |      0.463      |
| LLaMA-3B    | meta-llama/Llama-3.2-3B-Instruct               |  23.772   |   8.223   |  17.306   |   748.223   |    0.267    |    0.023    |     -0.162      |
| Phi-3-Mini  | microsoft/Phi-3-mini-4k-instruct               |  20.550   |   7.028   |  14.307   |   987.636   |    0.203    |    0.014    |     -0.572      |
| T5-large    | t5-large                                       |  10.977   |   1.944   |   9.637   |   263.028   |    0.760    |    0.037    |     -0.960      |

> _Notes_:  
- Accuracy: ROUGE-L is used as the primary accuracy metric.
- Latency: Time refers to the average inference time per sample.
- Throughput:samples/sec = speed=total time
- Efficiency: Defined as ROUGE-L divided by inference time = ROUGE/time  
- Composite score: Normalized metric combining accuracy and efficiency to support model selection.

The Ranking Table provides a full benchmarking and model-selection pipeline. Thus, this identifies (recommends) automatically the best model to fine-tune based on balanced performance rather than size alone. To sum up, the highest composite_score wins.  When selecting models for dialogue summarization, balancing prediction quality with inference efficiency is crucial â€” especially in practical or real-time settings.  
_Key takeaways_  
  - Composite score reflects both accuracy and speed, giving a more holistic evaluation than ROUGE alone.    
  - Models like BART-large outperform others because they achieve solid accuracy and fast inference.    
  - Larger causal models (e.g., LLaMA-3B, Phi-3-Mini) may achieve acceptable ROUGE scores, but their high latency significantly reduces their overall ranking.   

This shows the importance of balancing accuracy with inference speed when benchmarking large models for dialogue summarization.  

---

## Auto-fine-tuning Recommendation & Plan Results  

Notebook D generates `recommendations.json`, which contains per-model fine-tuning strategies and hyperparameters. Based on this analysis, BART-large and LLaMA-1B emerged as the top two candidates.  

Recommendation Output
```json
{
  "BART-large": {
    "size_hint": "0.4B",
    "method": "LoRA (PEFT) \u2014 encoder\u2013decoder friendly",
    "recommended_hyperparams": {
      "epochs": 3,
      "micro_batch_size": 8,
      "lr": 0.0002
    }
  },
  "LLaMA-1B": {
    "size_hint": "1B",
    "method": "LoRA or full fine-tune",
    "recommended_hyperparams": {
      "epochs": 3,
      "micro_batch_size": 8,
      "lr": 0.0002
    }
  }
```
Interpretation & Comparison
| Model          | Size | Recommended PEFT Method                    | Suggested Hypers                   | Meaning                                                   |
| -------------- | ---- | ------------------------------------------ | ---------------------------------- | --------------------------------------------------------- |
| **BART-large** | 0.4B | **LoRA (PEFT) â€” encoderâ€“decoder friendly** | epochs: 3, batch size: 8, LR: 2e-4 | **Best match for abstractive summarisation + efficiency** |
| **LLaMA-1B**   | 1B   | LoRA **or full fine-tune**                 | epochs: 3, batch size: 8, LR: 2e-4 | Strong, but slower + worse summarisation on highlightSUM  |

**BART-large** was selected and it is the preferred choice because:  
- Highest composite score from Notebook D rankings  
- Optimized architecture for encoder-decoder summarization tasks (HighlightSUM)  
- Native LoRA support on attention projections (q_proj/v_proj) without special patching  
- Efficient training & inference on Colab T4 GPU  

While both models received identical hyperparameters, BART-large offers superior performance due to its architecture fit, ROUGE scores, and latency characteristics.  


**Customize `train_qLoRA.py`**  

Next steps:  
- customize the `train_qLoRA.py` to the chosen model (map tokenizers/prompt style precisely). 
- add validation loop + ROUGE evaluation inside training to checkpoint best model.
- produce a small sample dataset JSONL generator from SAMSum that matches the expected supervised format.
- estimate training time more accurately based on the GPU type (T4 / L4 / A100) and hours you can run.  

Example of `customization of train_qLoRA.py`  

```bash
# -------------------------
# Config
# -------------------------
MODEL_NAME = "facebook/bart-large-cnn"
OUTPUT_DIR = "./ft_outputs/bart_lora_highlightsum"
N_SAMPLES = 2000
EPOCHS = 3                # recommended based on benchmark
MICRO_BATCH_SIZE = 4      # per-device batch size
GRAD_ACC = 2              # â†’ effective batch size = 8
LEARNING_RATE = 2e-4
MAX_INPUT_LENGTH = 768    # reduction speeds up training significantly
MAX_TARGET_LENGTH = 192
WANDB_PROJECT = "highlightsum_bart_lora"
```

```bash
# -------------------------
# Load dataset
# -------------------------
dataset = load_dataset("knkarthick/highlightsum")["train"].select(range(N_SAMPLES))
print(f"Loaded {len(dataset)} samples for training.")  
```

> It has choosen to train HighlightSum on a subset Recommandations sizes 
 
| Subset size      | GPU time     | Quality   |
| ---------------- | ------------ | --------- |
| **1k** samples   | ~ min   |        |
| **2k** samples   | ~ min      |      |
| **5k** samples   | ~ hours |  |
| **Full dataset** | ~s   |       |

HighlightSum is small, so even 2k samples already gives strong summarization  


### Examples output train.py 

- **Model Loaded in 4-bit Successfully** 
```bash
ğŸ”¥ Loading LLaMA-3.2-1B in 4-bitâ€¦
`torch_dtype` is deprecated! Use `dtype` instead!
```
> _Note_: The deprecation warning is harmless. 4-bit quantization is functioning.

- **LoRA adapters correctly attached**
```bash
trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039
```
> _Note_: This means only ~0.9% of the model is being fine-tuned, perfect for QLoRA. The LoRA config is working.  

- **Dataset fully loaded and tokenized**
```bash
Map: 100% 14731/14731 [00:17]
Map: 100% 818/818 [00:00]
```
> _Note_: Fast and correct â€” tokenizer & formatting are working.  

- **QLoRA Training Started**
```bash
 ğŸš€ Starting QLoRA trainingâ€¦
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
```
> _Note_: This is normal. It actually reduces memory usage, which is good.

- **Training is progressing normally**
```bash
  {'loss': 3.4777, 'grad_norm': 2.6477, 'learning_rate': 8.6e-06, 'epoch': 0.01}
  0% 26/5526 [03:38<12:49:09,  8.39s/it]
```

> _Note_: This tells - Training loop is working  - Gradients are valid  - Your GPU is training      
> _Note_: On a Colab T4 GPU, the real expected training time is:    
```bash
  ~25â€“35 minutes per epoch
  ~1h 20m for 3 epochs  
```  

- **Completion**  
```bash  
ğŸ’¾ Saving LoRA adaptersâ€¦  
ğŸ‰ Training completed. Saved to llama1b-samsum-qlora  
```    
 
###  Experiment Tracking with Weights & Biases (W&B)  
Training runs are instrumented and tracked using [Weights & Biases](https://wandb.ai/). This integration enables:
- Visualization of training loss and evaluation curves
- Learning rate schedule and gradient monitoring
- GPU memory usage tracking during training
- Evaluation metrics logging after each epoch
- Model artifact saving and versioning

All W&B integration is handled in the training script [`run_llama_qlora.py`](run_llama_qlora.py) with minimal and safe changes:
- Added `wandb.init(...)` for project setup
- Configured `report_to="wandb"` and custom `run_name`
- Enabled configuration tracking for reproducibility

#### Example Visualizations

![Training Loss Curve](loss_curve.jpeg)

![Evaluation Metrics](eval_metrics.jpeg)   TO BE UPLOADED

![GPU Utilization](gpu_utilization.jpeg)   

> To access interactive dashboards and full experiment details, [visit our Weights & Biases project](https://wandb.ai/agostinimichelait-ready-tensor/llama-qlora-samsum).

**Usage Note**: All tracking features are enabled only during training within the notebook/script.

---

**Want to view your training run?**
Once W&B is installed and you are logged in, your training run will automatically appear at:
```
https://wandb.ai/<your-team-or-user>/llama-qlora-samsum
```
- Loss curves: automatically logged  
- Eval metrics: automatically logged  
- Model artifacts: saved & versioned  
- GPU utilization: tracked
  

Notebook E, and the model produced:
```yaml
ğŸ“Š Final ROUGE scores:
ROUGE-1: 0.2142
ROUGE-2: 0.0915
ROUGE-L: 0.1633
```
Are these ROUGE scores good?

For only 1,000 training samples and 1B parameters, these numbers are reasonable.

Typical SAMSum results:  
| Model                                  | ROUGE-1   | ROUGE-2   | ROUGE-L   |
| -------------------------------------- | --------- | --------- | --------- |
| **LLaMA-1B baseline**                  | ~0.15     | ~0.04     | ~0.11     |
| **Your QLoRA fine-tuned (1k samples)** | **0.214** | **0.091** | **0.163** |
| BART-Large (full-size seq2seq)         | 0.49      | 0.26      | 0.45      |
| LLaMA-3-8B fine-tuned                  | 0.40+     | 0.20+     | 0.33+     |

ğŸ‘‰ Your model improved significantly over the baseline
ğŸ‘‰ But it's still far from seq2seq baselines or larger LLaMA models
ğŸ‘‰ Expected: 1B LLaMA is small and not optimized for summarization out of the box

The model:  
- Is working correctly  
- Respects the SAMSum structure  
- Has significant improvement over baseline  
- But is `not yet production-level`, I meant something very specific about quality, robustness, and reliabilityâ€”not that the model is bad or unusable. The model works, and the ROUGE numbers prove it is learning.
But for deployment in a real product, there are higher expectations.

The model is working and it can be considered as a successful prototype since it is:  
- generating grammatical summaries
- improving over the baseline
- responding correctly to dialogue inputs
- trained with a clean QLoRA pipeline

itâ€™s not yet production-quality. A production summarization model should meet certain criteria that are beyond what a 1B model fine-tuned on 1k samples can achieve.  
- Accuracy is not high enough
- Repetition & hallucination still occur
- A 1B LLaMA is small for summarization  Most production summarizers use: 3B, 7B, or 8B LLMs or seq2seq models optimized for summarization. 1B is good for prototyping, not for commercial-level results.  
- Trained on only 1k samples This is fine for testing, but: SAMSum has 14k training samples, Summarization requires large data, With 1000 samples, the model generalizes poorly. Production summarizers typically need:
  full dataset, 3â€“5 epochs, larger context window, decoding tuning


what is your model good for right now?

It is great for:
âœ”ï¸ research
âœ”ï¸ experimentation
âœ”ï¸ proof-of-concept demos
âœ”ï¸ learning QLoRA + LLaMA finetuning
âœ”ï¸ early prototypes of conversation summarization

ğŸ”¥ How to make it production-ready
If you want, I can generate:
1. A stronger training run
full SAMSum dataset
3 epochs
stronger LoRA (r=32)

2. A more advanced decoding setup
repetition penalty
penalties on n-grams
deterministic sampling

3. A distilled model evaluation
ROUGE
BERTScore
length normalization
human evaluation templates

4. A Notebook F: â€œFrom Prototype to Productionâ€
With:
merging
quantization (GGUF)
benchmarking
packaging for API deployment
  

### Summarize new dialogues with your fine-tuned model  
### Reproduce benchmarking with your own subset  
### Swap in other dialogue datasets with minor tweaks  

---

## Technologies Used

- [Hugging Face Transformers](https://huggingface.co/)
- [PEFT (LoRA/QLoRA)](https://github.com/huggingface/peft)
- [Weights & Biases](https://wandb.ai/site)
- [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) (quantization)
- [GGUF/Llama.cpp compatible conversion](https://github.com/ggerganov/llama.cpp)
- Python, Colab/Jupyter

---

## Results & Benchmarks

Example ROUGE scores for LLaMA-1B fine-tuned on 1k SAMSum samples:

| Model         | ROUGE-1 | ROUGE-2 | ROUGE-L |
|---------------|---------|---------|---------|
| LLaMA-1B base | ~0.15   | ~0.04   | ~0.11   |
| LLaMA-1B QLoRA (1k) | 0.214 | 0.091 | 0.163 |
| BART-Large    | 0.49    | 0.26    | 0.45    |
| LLaMA-3-8B QLoRA | 0.40+ | 0.20+  | 0.33+   |

_For 1B parameter models and limited data, this project significantly improves summarization performance while keeping memory and compute requirements low._

---

## Limitations & Recommendations

- 1B parameter models are ideal for prototyping, not production
- For higher accuracy, train on more data (full 14k SAMSum samples) and/or use larger models (3B/8B)
- Production deployments may require further tuning to reduce hallucinations or errors
- See Notebook F for tips on packaging and deploying your final model

---
 
## License

This project is licensed under the MIT License. See the [LICENSE](https://github.com/micag2025/llmed_Certification_Project1_FineTuneFlow/blob/97ed39ce6ae05e2b0546450448328841ef67816f/LICENSE) file for details.

---

## Contact

If you encounter bugs, have questions, or want to request a new feature, please [open an issue](https://github.com/micag2025/llmed_Certification_Project1_FineTuneFlow/issues) on this repository.   






