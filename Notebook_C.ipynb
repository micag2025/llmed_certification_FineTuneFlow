{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "zeRW-gFYc3kC",
   "metadata": {
    "id": "zeRW-gFYc3kC"
   },
   "source": [
    "# ðŸ“š Comprehensive Fine-Tuning Pipeline: BART-LoRA for HighlightSum\n",
    "\n",
    "## Overview\n",
    "This notebook implements a complete **end-to-end fine-tuning pipeline** for abstractive summarization using **BART-Large** with **LoRA adapters**. It includes:\n",
    "- âœ… Dataset loading & inspection\n",
    "- âœ… Multi-model benchmarking\n",
    "- âœ… Fine-tuning with PEFT\n",
    "- âœ… Comprehensive evaluation (ROUGE, BERTScore, BLEU)\n",
    "- âœ… Error analysis & visualization\n",
    "- âœ… Model merging & inference\n",
    "\n",
    "**Target Task**: Highlight-based abstractive summarization on HighlightSum dataset\n",
    "**Hardware**: GPU (T4, L4, or better recommended)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fca538",
   "metadata": {
    "id": "02fca538"
   },
   "source": [
    "# Dataset Inspection: HighlightSum\n",
    "\n",
    "This section loads and explores the **HighlightSum** dataset to understand its structure and contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ub3rxVBOdA3x",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13383f34163f42c186829e9d74c2ca32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94934e56cc6d42b5b62a7e0361bb11b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv:   0%|          | 0.00/27.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79201b0d3738418e8420f640788d49af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203a9e6b35a34e798646aff61733e534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d08f1fd5d2e44cc8e05759b702fbac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/27401 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2861240673487589247e17c451349f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1360 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35388f2aca744109ab2e0a23b3368f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2347 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Dataset Overview:\n",
      "  Train splits: 27,401 samples\n",
      "  Val splits: 1,360 samples\n",
      "  Test splits: 2,347 samples\n",
      "\n",
      "ðŸ”‘ Keys: ['id', 'dialogue', 'summary']\n",
      "\n",
      "ðŸ“˜ First training example:\n",
      "\n",
      "ðŸ”¸ DIALOGUE (32390 chars):\n",
      "Speaker A: Cool. Do you wanna give me the little cable thing? Yeah. Cool. Ah, that's why it won't meet. Okay, cool. Yep, cool. Okay, functional requirements. Alright, yeah. It's working. Cool, okay. So what I have, wh where I've got my information from is a survey where the usability lab um observed...\n",
      "\n",
      "ðŸ”¹ SUMMARY (1299 chars):\n",
      "The project manager opens the meeting by stating that they will address functional design and then going over the agenda. The industrial designer gives his presentation, explaining how remote controls function and giving personal preference to a clear, simple design that upgrades the technology as well as incorporates the latest features in chip design. The interface specialist gives her presentation next, addressing the main purpose of a remote control. She pinpoints the main functions of on/off, channel-switching, numbers for choosing particular channels, and volume; and also suggests adding a menu button to change settings such as brightness on the screen. She gives preference to a remote that is small, easy to use, and follows some conventions. The group briefly discusses the possibility of using an LCD screen if cost allows it, since it is fancy and fashionable. The marketing expert presents, giving statistical information from a survey of 100 subjects. She prefers a remote that is sleek, stylish, sophisticated, cool, beautiful, functional, solar-powered, has long battery life, and has a locator. They discuss the target group, deciding it should be 15-35 year olds. After they talk about features they might include, the project manager closes the meeting by allocating tasks.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"knkarthick/highlightsum\")\n",
    "\n",
    "# View first sample structure\n",
    "sample = dataset[\"train\"][0]\n",
    "print(\"ðŸ“Š Dataset Overview:\")\n",
    "print(f\"  Train splits: {len(dataset['train']):,} samples\")\n",
    "print(f\"  Val splits: {len(dataset['validation']):,} samples\")\n",
    "print(f\"  Test splits: {len(dataset['test']):,} samples\")\n",
    "print(f\"\\nðŸ”‘ Keys: {list(sample.keys())}\")\n",
    "\n",
    "# Inspect sample details\n",
    "print(\"\\nðŸ“˜ First training example:\")\n",
    "print(f\"\\nðŸ”¸ DIALOGUE ({len(sample['dialogue'])} chars):\\n{sample['dialogue'][:300]}...\")\n",
    "print(f\"\\nðŸ”¹ SUMMARY ({len(sample['summary'])} chars):\\n{sample['summary']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9840e9d2",
   "metadata": {
    "id": "9840e9d2"
   },
   "source": [
    "# Large Model Safety Benchmarking (with Preprocessing)\n",
    "\n",
    "This section outlines the preprocessing steps required before running benchmarking or fineâ€‘tuning large models. Since only **2,000 samples** will be used for fineâ€‘tuning, efficient batching and GPU-aware preprocessing are essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4f29c8",
   "metadata": {
    "id": "9b4f29c8"
   },
   "source": [
    "### Why Preprocessing Matters\n",
    "\n",
    "Preprocessing ensures that:\n",
    "- Inputs are normalized and consistently formatted.\n",
    "- Tokenization is performed efficiently before training.\n",
    "- Dataset splits are clearly defined.\n",
    "\n",
    "This improves both reproducibility and training performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb26e3be",
   "metadata": {
    "id": "fb26e3be"
   },
   "source": [
    "_Basic preprocessing_ has been performed, including tokenization of dialogues with appropriate padding and truncation, batch preparation for seq2seq models, and selection of a subset from the HighlightSUM train split for benchmarking. In details, the performed basic preprocessing is charachterised by:  \n",
    "> _Tokenization_:\n",
    "  - All text inputs (dialogue) are tokenized using the model-specific tokenizer.  \n",
    "  - For causal models, if pad_token was missing, it was set to eos_token to allow batching/padding.  \n",
    "  - Seq2seq and causal models both use truncation and padding (max_length=1024) to ensure consistent tensor shapes.  \n",
    "> Dataset splitting:  \n",
    "  - Selected a subset of samples (N_SAMPLES) from the HighlightSUM train split for benchmarking.\n",
    "  - If desired, you could extend this to full train/validation/test splits for proper evaluation.\n",
    "> Batching (seq2seq models):  \n",
    "  - Inputs are batched to reduce memory usage on GPU, which is part of preprocessing before model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eB8UU1ZMfURS",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "áŠ‚ Using device: cuda\n",
      "Train samples: 27401\n",
      "Validation samples: 1360\n",
      "Test samples (used): 200\n",
      "Columns: ['id', 'dialogue', 'summary']\n",
      "\n",
      "áŠ‚ Running Model Benchmarks...\n",
      "\n",
      "á½¨0 Benchmarking BART-large (facebook/bart-large-cnn) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2060890673804807aacfceb5d1ff50d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4abbcc789954fcf978084234184de62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ecca9ed262f46c490362620ad978055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0261f457cbad435499e842d738b935f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3a76761ea94921b95cb15284bb89af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497aa3311ea14aa880cdd8d78769c07e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "BART-large (seq2seq): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:42<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "á½¨0 Benchmarking T5-large (t5-large) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9eaa6e2f54849de91d09e7304f0d81b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247bd28af4924f029f147883b02983f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7bab1b471064d54ba99b2fcfcf9cdb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b217aedddbc94115aee0658e1296a82b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d712ae8b675f42688917b573bf46594c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T5-large (seq2seq): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [04:24<00:00,  5.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "á½¨0 Benchmarking Phi-3-Mini (microsoft/Phi-3-mini-4k-instruct) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0250632d424969af64891d260662eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2908deb6a9b4ece999471677c7a031f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2e94d91fee4df08f9452f2ec0ca32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea954a9248124bedbcdcda8156a2e3d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be030e5b7a34b8c8310670dbcd07b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f552eceb6cb349e8804dce09b01931be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6f0a61870b49f7a1e1856e5ffa420d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e41b58810bb04ab08bd16383d891032b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5039c96ff5f4815af4c1fd1cbed8107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5998f8577447c69b5af9b57631ab8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8045aacced66486882fed6cd716b771b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508e35cf2bc3496facafc5b02258e125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phi-3-Mini (causal): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [16:28<00:00,  9.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "á½¨0 Benchmarking LLaMA-1B (meta-llama/Llama-3.2-1B-Instruct) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f5c4d52c6d4852930bd8048d5ced0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef09b97dfd26497d80995ec2776a47b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c532b327db74771bb28fb58a401d647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf259f2a9364fdb91471301c25e8dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd4241736b24cc1903e266d50894307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846eaca69a5f47a391d28079f6efee3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLaMA-1B (causal):  14%|â–ˆâ–        | 14/100 [01:19<08:10,  5.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  15%|â–ˆâ–Œ        | 15/100 [01:21<06:37,  4.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  16%|â–ˆâ–Œ        | 16/100 [01:23<05:13,  3.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  17%|â–ˆâ–‹        | 17/100 [01:26<04:52,  3.52s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  18%|â–ˆâ–Š        | 18/100 [01:27<04:04,  2.98s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  19%|â–ˆâ–‰        | 19/100 [01:33<05:15,  3.89s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  20%|â–ˆâ–ˆ        | 20/100 [01:36<04:39,  3.49s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  21%|â–ˆâ–ˆ        | 21/100 [01:42<05:26,  4.14s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  22%|â–ˆâ–ˆâ–       | 22/100 [01:44<04:49,  3.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  23%|â–ˆâ–ˆâ–Ž       | 23/100 [01:47<04:28,  3.49s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  24%|â–ˆâ–ˆâ–       | 24/100 [01:51<04:22,  3.46s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  25%|â–ˆâ–ˆâ–Œ       | 25/100 [01:54<04:18,  3.45s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  26%|â–ˆâ–ˆâ–Œ       | 26/100 [02:04<06:32,  5.30s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  27%|â–ˆâ–ˆâ–‹       | 27/100 [02:09<06:27,  5.31s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  28%|â–ˆâ–ˆâ–Š       | 28/100 [02:18<07:40,  6.39s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  29%|â–ˆâ–ˆâ–‰       | 29/100 [02:21<06:14,  5.27s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [02:26<06:20,  5.43s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [02:29<05:16,  4.59s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [02:31<04:12,  3.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [02:33<03:44,  3.34s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [02:36<03:30,  3.19s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [02:41<04:12,  3.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [02:44<03:41,  3.46s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [02:50<04:26,  4.23s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [02:53<04:06,  3.98s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [02:55<03:13,  3.17s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [02:57<02:57,  2.95s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [02:59<02:35,  2.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [03:05<03:22,  3.49s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [03:17<05:59,  6.30s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [03:19<04:38,  4.98s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [03:28<05:40,  6.20s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [03:30<04:26,  4.93s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [03:33<03:40,  4.17s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [03:34<02:55,  3.37s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [03:36<02:28,  2.92s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [03:42<03:06,  3.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [03:44<02:47,  3.42s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [03:51<03:24,  4.26s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [03:56<03:40,  4.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [03:58<02:55,  3.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [04:02<02:55,  3.90s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [04:08<03:14,  4.42s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [04:14<03:32,  4.94s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [04:16<02:47,  3.99s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [04:18<02:26,  3.57s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [04:21<02:09,  3.23s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [04:27<02:39,  4.08s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [04:29<02:12,  3.49s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [04:31<01:55,  3.12s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [04:33<01:33,  2.59s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [04:39<02:06,  3.61s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [04:44<02:20,  4.13s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [04:45<01:45,  3.20s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [04:51<02:07,  4.00s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [04:54<01:57,  3.80s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [04:56<01:35,  3.18s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [04:59<01:33,  3.22s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [05:03<01:37,  3.49s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [05:09<01:50,  4.08s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [05:11<01:29,  3.44s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [05:14<01:24,  3.37s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [05:18<01:23,  3.50s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [05:21<01:21,  3.52s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [05:24<01:10,  3.22s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [05:30<01:25,  4.05s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [05:33<01:18,  3.90s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [05:36<01:05,  3.43s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [05:42<01:16,  4.24s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [05:43<00:58,  3.46s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [05:49<01:05,  4.09s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [05:51<00:54,  3.60s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [05:54<00:45,  3.25s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [05:55<00:34,  2.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [05:57<00:30,  2.52s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [06:01<00:31,  2.89s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [06:03<00:26,  2.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [06:05<00:22,  2.50s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [06:07<00:18,  2.32s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [06:13<00:23,  3.31s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [06:19<00:24,  4.07s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [06:24<00:22,  4.46s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [06:25<00:13,  3.50s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [06:28<00:09,  3.29s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [06:30<00:05,  2.83s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [06:32<00:02,  2.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-1B (causal): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [06:35<00:00,  3.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "á½¨0 Benchmarking LLaMA-3B (meta-llama/Llama-3.2-3B-Instruct) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6793663faa484db1c3f514e55f05a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "791b96bed34d4ad6905995cc0b524a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20eb73a30ad94149861b2d7b963ad404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0cc31bdb8149cb8f981a0266880bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702fe6212c7a40ad87d7b34308803cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6402964cd32a4ae1a38abf4a4a1e1aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16dc83d8c1af41f3bd65431749c4dac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4857e75b049e452daeaedd0c1d199f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa0ba88f0604fd7b84da422bb62ab72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b014d28d803d41919c31abc19012ac77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLaMA-3B (causal):  14%|â–ˆâ–        | 14/100 [02:42<16:47, 11.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  15%|â–ˆâ–Œ        | 15/100 [02:52<15:37, 11.03s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  16%|â–ˆâ–Œ        | 16/100 [02:55<12:13,  8.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  17%|â–ˆâ–‹        | 17/100 [03:03<11:26,  8.28s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  18%|â–ˆâ–Š        | 18/100 [03:07<09:33,  6.99s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  19%|â–ˆâ–‰        | 19/100 [03:11<08:30,  6.31s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  20%|â–ˆâ–ˆ        | 20/100 [03:14<07:06,  5.33s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  21%|â–ˆâ–ˆ        | 21/100 [03:24<08:49,  6.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  22%|â–ˆâ–ˆâ–       | 22/100 [03:32<09:01,  6.94s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  23%|â–ˆâ–ˆâ–Ž       | 23/100 [03:35<07:32,  5.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  24%|â–ˆâ–ˆâ–       | 24/100 [03:40<07:05,  5.59s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  25%|â–ˆâ–ˆâ–Œ       | 25/100 [03:50<08:40,  6.94s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  26%|â–ˆâ–ˆâ–Œ       | 26/100 [03:59<09:20,  7.58s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  27%|â–ˆâ–ˆâ–‹       | 27/100 [04:08<09:51,  8.10s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  28%|â–ˆâ–ˆâ–Š       | 28/100 [04:18<10:07,  8.44s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  29%|â–ˆâ–ˆâ–‰       | 29/100 [04:21<08:19,  7.03s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [04:31<08:59,  7.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [04:40<09:20,  8.13s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [04:49<09:30,  8.39s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [04:53<07:48,  7.00s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [04:56<06:31,  5.93s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [05:05<07:32,  6.96s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [05:15<08:10,  7.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [05:17<06:24,  6.10s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [05:22<05:49,  5.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [05:26<05:17,  5.21s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [05:30<04:48,  4.80s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [05:39<06:03,  6.16s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [05:48<06:41,  6.91s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [05:57<07:12,  7.58s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [05:59<05:30,  5.91s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [06:08<06:18,  6.89s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [06:13<05:32,  6.16s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [06:15<04:32,  5.13s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [06:24<05:20,  6.16s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [06:32<05:39,  6.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [06:34<04:24,  5.29s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [06:43<05:12,  6.37s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [06:52<05:48,  7.27s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [07:01<06:04,  7.76s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [07:05<04:59,  6.51s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [07:14<05:27,  7.29s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [07:18<04:40,  6.38s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [07:21<03:49,  5.33s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [07:30<04:35,  6.55s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [07:40<05:03,  7.41s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [07:48<05:12,  7.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [07:58<05:24,  8.31s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [08:00<04:05,  6.46s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [08:09<04:32,  7.35s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [08:14<03:52,  6.47s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [08:23<04:17,  7.35s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [08:32<04:28,  7.91s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [08:42<04:34,  8.32s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [08:51<04:31,  8.48s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [09:00<04:32,  8.78s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [09:10<04:29,  8.98s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [09:14<03:42,  7.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [09:19<03:12,  6.86s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [09:23<02:38,  5.86s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [09:27<02:22,  5.47s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [09:30<01:57,  4.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [09:39<02:22,  5.92s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [09:48<02:38,  6.90s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [09:57<02:48,  7.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [10:07<02:50,  8.12s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [10:15<02:46,  8.32s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [10:19<02:13,  7.02s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [10:26<02:06,  7.01s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [10:31<01:46,  6.27s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [10:40<01:53,  7.12s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [10:49<01:56,  7.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [10:59<01:55,  8.23s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [11:08<01:50,  8.50s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [11:11<01:22,  6.90s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [11:13<00:58,  5.36s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [11:22<01:05,  6.60s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [11:32<01:07,  7.45s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [11:35<00:50,  6.37s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [11:45<00:51,  7.35s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [11:54<00:46,  7.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [11:57<00:32,  6.52s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [12:06<00:28,  7.25s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [12:11<00:19,  6.57s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [12:15<00:11,  5.52s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [12:24<00:06,  6.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLaMA-3B (causal): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [12:29<00:00,  7.50s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df_sorted\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"model_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"meta-llama/Llama-3.2-1B-Instruct\",\n          \"t5-large\",\n          \"meta-llama/Llama-3.2-3B-Instruct\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rouge1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.197540545985217,\n        \"min\": 10.977281761408955,\n        \"max\": 28.635874255728172,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          28.635874255728172,\n          10.977281761408955,\n          23.771792578237726\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rouge2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.101704601687299,\n        \"min\": 1.9440093037647932,\n        \"max\": 9.618124519432426,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          9.618124519432426,\n          1.9440093037647932,\n          8.222793347601673\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rougeL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.881426094408254,\n        \"min\": 9.636944068641258,\n        \"max\": 21.205387493718362,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          21.205387493718362,\n          9.636944068641258,\n          17.30620340596481\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 362.45749717977753,\n        \"min\": 101.63183641433716,\n        \"max\": 987.636198759079,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          393.9294948577881,\n          263.02784180641174,\n          748.2234883308411\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"throughput\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7201431081377649,\n        \"min\": 0.2025037156913559,\n        \"max\": 1.9678872984704436,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.5077050655275298,\n          0.7603757785732805,\n          0.26729981498731864\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"efficiency\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07975915798730271,\n        \"min\": 0.014485775990486366,\n        \"max\": 0.20724446852009892,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.05383041323517471,\n          0.03663849424630127,\n          0.023129724842736486\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.866576615173311,\n        \"min\": -0.9599622636932412,\n        \"max\": 1.230693974550218,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.4632295061715081,\n          -0.9599622636932412,\n          -0.16234214229540284\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df_sorted"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-75aa3efe-a08e-4ea6-98a1-e3c0420e5643\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>time</th>\n",
       "      <th>throughput</th>\n",
       "      <th>efficiency</th>\n",
       "      <th>composite_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BART-large</th>\n",
       "      <td>facebook/bart-large-cnn</td>\n",
       "      <td>28.105383</td>\n",
       "      <td>9.183429</td>\n",
       "      <td>21.062636</td>\n",
       "      <td>101.631836</td>\n",
       "      <td>1.967887</td>\n",
       "      <td>0.207244</td>\n",
       "      <td>1.230694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLaMA-1B</th>\n",
       "      <td>meta-llama/Llama-3.2-1B-Instruct</td>\n",
       "      <td>28.635874</td>\n",
       "      <td>9.618125</td>\n",
       "      <td>21.205387</td>\n",
       "      <td>393.929495</td>\n",
       "      <td>0.507705</td>\n",
       "      <td>0.053830</td>\n",
       "      <td>0.463230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLaMA-3B</th>\n",
       "      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n",
       "      <td>23.771793</td>\n",
       "      <td>8.222793</td>\n",
       "      <td>17.306203</td>\n",
       "      <td>748.223488</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.023130</td>\n",
       "      <td>-0.162342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phi-3-Mini</th>\n",
       "      <td>microsoft/Phi-3-mini-4k-instruct</td>\n",
       "      <td>20.550442</td>\n",
       "      <td>7.028457</td>\n",
       "      <td>14.306677</td>\n",
       "      <td>987.636199</td>\n",
       "      <td>0.202504</td>\n",
       "      <td>0.014486</td>\n",
       "      <td>-0.571619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T5-large</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>10.977282</td>\n",
       "      <td>1.944009</td>\n",
       "      <td>9.636944</td>\n",
       "      <td>263.027842</td>\n",
       "      <td>0.760376</td>\n",
       "      <td>0.036638</td>\n",
       "      <td>-0.959962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-75aa3efe-a08e-4ea6-98a1-e3c0420e5643')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-75aa3efe-a08e-4ea6-98a1-e3c0420e5643 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-75aa3efe-a08e-4ea6-98a1-e3c0420e5643');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-d61dda4a-15f3-43fc-bb00-d3b46f58a058\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d61dda4a-15f3-43fc-bb00-d3b46f58a058')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-d61dda4a-15f3-43fc-bb00-d3b46f58a058 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_cd1e7008-0645-4447-a6fe-d16d7c384779\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_sorted')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_cd1e7008-0645-4447-a6fe-d16d7c384779 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df_sorted');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                    model_id     rouge1    rouge2     rougeL  \\\n",
       "BART-large           facebook/bart-large-cnn  28.105383  9.183429  21.062636   \n",
       "LLaMA-1B    meta-llama/Llama-3.2-1B-Instruct  28.635874  9.618125  21.205387   \n",
       "LLaMA-3B    meta-llama/Llama-3.2-3B-Instruct  23.771793  8.222793  17.306203   \n",
       "Phi-3-Mini  microsoft/Phi-3-mini-4k-instruct  20.550442  7.028457  14.306677   \n",
       "T5-large                            t5-large  10.977282  1.944009   9.636944   \n",
       "\n",
       "                  time  throughput  efficiency  composite_score  \n",
       "BART-large  101.631836    1.967887    0.207244         1.230694  \n",
       "LLaMA-1B    393.929495    0.507705    0.053830         0.463230  \n",
       "LLaMA-3B    748.223488    0.267300    0.023130        -0.162342  \n",
       "Phi-3-Mini  987.636199    0.202504    0.014486        -0.571619  \n",
       "T5-large    263.027842    0.760376    0.036638        -0.959962  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved outputs to /content/llmed_certification_FineTuneFlow/outputs/benchmarks/notebook_C\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# á¡ Notebook C â€” Large Model Safe Benchmarking (Refactored)\n",
    "# =====================================================\n",
    "# Usage: paste into Google Colab and run.\n",
    "# Notes:\n",
    "#  - Designed to be Colab-friendly (offload support, small sample defaults).\n",
    "#  - Batches causal models for better throughput.\n",
    "#  - Seeds for reproducibility.\n",
    "#  - Standardized metric normalization (z-score) for composite scoring.\n",
    "# =====================================================\n",
    "\n",
    "# -------------------------\n",
    "# Install dependencies (run once)\n",
    "# -------------------------\n",
    "!pip install -q datasets transformers accelerate sentencepiece rouge-score pandas tqdm matplotlib seaborn\n",
    "\n",
    "# -------------------------\n",
    "# Imports\n",
    "# -------------------------\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# -------------------------\n",
    "# Configuration (tweak here)\n",
    "# -------------------------\n",
    "DEBUG = False                 # set True to enable CUDA_LAUNCH_BLOCKING for debugging\n",
    "USE_OFFLOAD = True            # set True to allow model offloading (helps on Colab)\n",
    "OFFLOAD_FOLDER = \"offload\"    # folder for offloaded tensors\n",
    "N_SAMPLES = 200               # number of test samples to evaluate (use small number for quick runs)\n",
    "SEQ2SEQ_BATCH_SIZE = 4        # batch size for seq2seq (e.g., BART, T5)\n",
    "CAUSAL_BATCH_SIZE = 2         # batch size for causal (e.g., LLaMA-like) -- keep small for memory\n",
    "#MAX_INPUT_TOKENS = 1024       # tokenizer max length for inputs\n",
    "MAX_INPUT_TOKENS = 768\n",
    "#MAX_NEW_TOKENS = 120          # tokens to generate\n",
    "MAX_NEW_TOKENS = 192\n",
    "#SAVE_DIR = \"outputs/benchmarks/notebook_C\"\n",
    "SAVE_DIR = \"/content/llmed_certification_FineTuneFlow/outputs/benchmarks/notebook_C\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Optional debug env\n",
    "# -------------------------\n",
    "if DEBUG:\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# -------------------------\n",
    "# Device\n",
    "# -------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"áŠ‚ Using device: {device}\")\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# -------------------------\n",
    "# Load dataset (small subset by default)\n",
    "# -------------------------\n",
    "dataset = load_dataset(\"knkarthick/highlightsum\")\n",
    "train_data = dataset[\"train\"]\n",
    "val_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"].select(range(min(N_SAMPLES, len(dataset[\"test\"]))))\n",
    "print(\"Train samples:\", len(train_data))\n",
    "print(\"Validation samples:\", len(val_data))\n",
    "print(\"Test samples (used):\", len(test_data))\n",
    "print(\"Columns:\", test_data.column_names)\n",
    "\n",
    "# -------------------------\n",
    "# ROUGE scorer\n",
    "# -------------------------\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"], use_stemmer=True)\n",
    "\n",
    "def compute_rouge(preds, refs):\n",
    "    \"\"\"\n",
    "    Compute average ROUGE f-measures (percent).\n",
    "    Uses rouge_scorer where signature is: score(target/reference, prediction)\n",
    "    \"\"\"\n",
    "    assert len(preds) == len(refs), \"Predictions and references must match length\"\n",
    "    agg = {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        # target = reference, prediction = model output\n",
    "        scores = scorer.score(ref, pred)\n",
    "        for k in agg:\n",
    "            agg[k] += scores[k].fmeasure\n",
    "    n = len(preds) if len(preds) > 0 else 1\n",
    "    return {k: (v / n) * 100.0 for k, v in agg.items()}\n",
    "\n",
    "# -------------------------\n",
    "# Utilities\n",
    "# -------------------------\n",
    "def batchify(lst, batch_size):\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i:i+batch_size]\n",
    "\n",
    "def safe_tokenizer_pad(tokenizer):\n",
    "    if tokenizer.pad_token is None:\n",
    "        # ensure a pad token exists for batching\n",
    "        if tokenizer.eos_token is not None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        else:\n",
    "            tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "    return tokenizer\n",
    "\n",
    "# -------------------------\n",
    "# Summarization inference functions\n",
    "# -------------------------\n",
    "def summarize_seq2seq_batch(model, tokenizer, texts, max_input_tokens=MAX_INPUT_TOKENS, max_new_tokens=MAX_NEW_TOKENS):\n",
    "    \"\"\"\n",
    "    Perform batched seq2seq inference. Returns (pred_texts, elapsed_time_seconds).\n",
    "    Measures only the generation time (tokenization done outside timer).\n",
    "    \"\"\"\n",
    "    # Tokenize (on CPU/GPU depending on tokenizer implementation)\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        max_length=max_input_tokens,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    start = time.time()\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    latency = time.time() - start\n",
    "    preds = [tokenizer.decode(o, skip_special_tokens=True).strip() for o in output_ids]\n",
    "    return preds, latency\n",
    "\n",
    "def summarize_causal_batch(model, tokenizer, texts, system_prompt=\"Summarize this text concisely.\", max_input_tokens=MAX_INPUT_TOKENS, max_new_tokens=MAX_NEW_TOKENS):\n",
    "    \"\"\"\n",
    "    Perform batched inference for causal/chat-style models. Accepts a list of raw texts.\n",
    "    We create a simple prompt; for more advanced chat tokenizers, a tokenizer-specific template may be needed.\n",
    "    Returns (pred_texts, elapsed_time_seconds).\n",
    "    \"\"\"\n",
    "    # Build prompts: simple system + user text; if tokenizer has chat helpers you can extend this.\n",
    "    prompts = []\n",
    "    for t in texts:\n",
    "        # Use a modest prompt that works with most causal models\n",
    "        p = f\"{system_prompt}\\n\\nText:\\n{t}\\n\\nSummary:\"\n",
    "        prompts.append(p)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        max_length=max_input_tokens,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    start = time.time()\n",
    "    out_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.pad_token_id)\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    latency = time.time() - start\n",
    "    preds = [tokenizer.decode(o, skip_special_tokens=True).strip() for o in out_ids]\n",
    "    # Post-process: remove the prompt prefix if tokenizer doesn't handle it (best-effort)\n",
    "    # We'll do a simple split by \"Summary:\" and take the tail if present\n",
    "    cleaned = []\n",
    "    for prompt, pred in zip(prompts, preds):\n",
    "        if \"Summary:\" in pred:\n",
    "            cleaned.append(pred.split(\"Summary:\",1)[-1].strip())\n",
    "        else:\n",
    "            # fallback: try to remove repeating prompt portion\n",
    "            cleaned.append(pred.replace(prompt, \"\").strip()[:1000])\n",
    "    return cleaned, latency\n",
    "\n",
    "# -------------------------\n",
    "# Model benchmark core\n",
    "# -------------------------\n",
    "def load_model_safe(model_id, use_offload=USE_OFFLOAD, torch_dtype=torch.float16):\n",
    "    \"\"\"\n",
    "    Load a model with device_map='auto'. If offload enabled, set offload_folder.\n",
    "    Returns the loaded model and a flag is_seq2seq.\n",
    "    \"\"\"\n",
    "    # attempt to detect seq2seq by config when possible\n",
    "    # We try Seq2Seq loader first only if model config indicates encoder-decoder\n",
    "    kwargs = {\"device_map\": \"auto\", \"torch_dtype\": torch_dtype}\n",
    "    if use_offload:\n",
    "        os.makedirs(OFFLOAD_FOLDER, exist_ok=True)\n",
    "        kwargs[\"offload_folder\"] = OFFLOAD_FOLDER\n",
    "\n",
    "    # We'll try to load the model with generic loaders. Errors are caught by caller.\n",
    "    # Load as seq2seq if model config indicates encoder-decoder (preferred detection).\n",
    "    model_try = None\n",
    "    # We need to inspect remote config -> using AutoModelForSeq2SeqLM may fail on causal models,\n",
    "    # so we try seq2seq first and fallback to causal.\n",
    "    try:\n",
    "        model_try = AutoModelForSeq2SeqLM.from_pretrained(model_id, **kwargs)\n",
    "        is_seq2seq = getattr(model_try.config, \"is_encoder_decoder\", True)\n",
    "        return model_try, is_seq2seq\n",
    "    except Exception:\n",
    "        # fallback to causal\n",
    "        model_try = AutoModelForCausalLM.from_pretrained(model_id, **kwargs)\n",
    "        is_seq2seq = getattr(model_try.config, \"is_encoder_decoder\", False)\n",
    "        return model_try, is_seq2seq\n",
    "\n",
    "def benchmark_models_safe(models_dict, n_samples=N_SAMPLES, seq2seq_bs=SEQ2SEQ_BATCH_SIZE, causal_bs=CAUSAL_BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Benchmark multiple models safely.\n",
    "    Returns a dict of results per model.\n",
    "    Each model result contains: rouge1, rouge2, rougeL, time, throughput, efficiency\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    refs = test_data[\"summary\"][:n_samples]\n",
    "\n",
    "    for name, model_id in models_dict.items():\n",
    "        print(f\"\\ná½¨0 Benchmarking {name} ({model_id}) ...\")\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "            tokenizer = safe_tokenizer_pad(tokenizer)\n",
    "\n",
    "            # Load model\n",
    "            model, is_seq2seq = load_model_safe(model_id)\n",
    "            model.eval()\n",
    "            # ensure model uses tokenizer length if we added tokens\n",
    "            try:\n",
    "                model.resize_token_embeddings(len(tokenizer))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # Move model to correct device is handled by device_map=\"auto\" loader.\n",
    "            # But in CPU-only environment we move to CPU device explicitly.\n",
    "            if not torch.cuda.is_available():\n",
    "                model.to(device)\n",
    "\n",
    "            preds = []\n",
    "            total_time = 0.0\n",
    "\n",
    "            if is_seq2seq:\n",
    "                # Batch processing for seq2seq models\n",
    "                indices = list(range(0, n_samples))\n",
    "                for i in tqdm(range(0, n_samples, seq2seq_bs), desc=f\"{name} (seq2seq)\"):\n",
    "                    batch_texts = test_data[\"dialogue\"][i:i+seq2seq_bs]\n",
    "                    batch_preds, latency = summarize_seq2seq_batch(model, tokenizer, batch_texts)\n",
    "                    preds.extend(batch_preds)\n",
    "                    total_time += latency\n",
    "            else:\n",
    "                # Batch processing for causal/chat models\n",
    "                for i in tqdm(range(0, n_samples, causal_bs), desc=f\"{name} (causal)\"):\n",
    "                    batch_texts = test_data[\"dialogue\"][i:i+causal_bs]\n",
    "                    batch_preds, latency = summarize_causal_batch(model, tokenizer, batch_texts)\n",
    "                    preds.extend(batch_preds)\n",
    "                    total_time += latency\n",
    "\n",
    "            # ensure correct length\n",
    "            preds = preds[:n_samples]\n",
    "            refs_used = refs[:len(preds)]\n",
    "\n",
    "            # Compute metrics\n",
    "            scores = compute_rouge(preds, refs_used)\n",
    "            total_time = max(total_time, 1e-8)  # avoid div by zero\n",
    "            throughput = len(preds) / total_time\n",
    "            efficiency = scores[\"rougeL\"] / total_time\n",
    "\n",
    "            results[name] = {\n",
    "                \"model_id\": model_id,\n",
    "                \"rouge1\": scores[\"rouge1\"],\n",
    "                \"rouge2\": scores[\"rouge2\"],\n",
    "                \"rougeL\": scores[\"rougeL\"],\n",
    "                \"time\": total_time,\n",
    "                \"throughput\": throughput,\n",
    "                \"efficiency\": efficiency\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error with {name}: {e}\")\n",
    "            results[name] = {\n",
    "                \"model_id\": model_id,\n",
    "                \"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0,\n",
    "                \"time\": float(\"inf\"), \"throughput\": 0.0, \"efficiency\": 0.0\n",
    "            }\n",
    "\n",
    "        # Free GPU memory after each model if available\n",
    "        try:\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return results\n",
    "\n",
    "# -------------------------\n",
    "# Models to benchmark (example sets)\n",
    "# -------------------------\n",
    "MODELS_A = {\n",
    "    \"BART-large\": \"facebook/bart-large-cnn\",\n",
    "    \"T5-large\": \"t5-large\",\n",
    "}\n",
    "\n",
    "MODELS_B = {\n",
    "    # Replace with models accessible in your environment; some may not be downloadable in limited Colab.\n",
    "    \"Phi-3-Mini\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    \"LLaMA-1B\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"LLaMA-3B\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "}\n",
    "\n",
    "# =====================================================\n",
    "# RUN BENCHMARK (A then B)\n",
    "# =====================================================\n",
    "print(\"\\náŠ‚ Running Model Benchmarks...\")\n",
    "results_A = benchmark_models_safe(MODELS_A, n_samples=min(N_SAMPLES, len(test_data)))\n",
    "results_B = benchmark_models_safe(MODELS_B, n_samples=min(N_SAMPLES, len(test_data)))\n",
    "results_all = {**results_A, **results_B}\n",
    "\n",
    "# =====================================================\n",
    "# Build DataFrame and compute composite score\n",
    "# =====================================================\n",
    "df = pd.DataFrame.from_dict(results_all, orient=\"index\")\n",
    "# Ensure numeric types\n",
    "for col in [\"rouge1\",\"rouge2\",\"rougeL\",\"time\",\"throughput\",\"efficiency\"]:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "# Standardize (z-score) selected metrics to make composite score robust to scale\n",
    "metrics_for_norm = [\"rougeL\", \"efficiency\", \"throughput\"]\n",
    "df_norm = df.copy()\n",
    "for col in metrics_for_norm:\n",
    "    if df[col].std() > 0:\n",
    "        df_norm[col] = (df[col] - df[col].mean()) / df[col].std()\n",
    "    else:\n",
    "        df_norm[col] = 0.0\n",
    "\n",
    "# Composite: weights (tunable)\n",
    "df[\"composite_score\"] = (\n",
    "    0.60 * df_norm[\"rougeL\"]\n",
    "    + 0.25 * df_norm[\"efficiency\"]\n",
    "    + 0.15 * df_norm[\"throughput\"]\n",
    ")\n",
    "\n",
    "df_sorted = df.sort_values(\"composite_score\", ascending=False)\n",
    "display(df_sorted)\n",
    "\n",
    "# =====================================================\n",
    "# Save results\n",
    "# =====================================================\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "csv_path = os.path.join(SAVE_DIR, \"final_ranking.csv\")\n",
    "html_path = os.path.join(SAVE_DIR, \"final_ranking.html\")\n",
    "json_path = os.path.join(SAVE_DIR, \"final_ranking.json\")\n",
    "\n",
    "df_sorted.to_csv(csv_path)\n",
    "df_sorted.to_html(html_path)\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(df_sorted.to_dict(orient=\"index\"), f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved outputs to {SAVE_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
